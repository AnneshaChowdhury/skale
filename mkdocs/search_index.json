{
    "docs": [
        {
            "location": "/", 
            "text": "skale-engine\n\n\nSkale-engine is a fast and general purpose distributed data processing\nsystem. It provides a high-level API in Javascript and an optimized\nparallel execution engine on top of NodeJS.\n\n\nWord count using skale:\n\n\nvar sc = require('skale-engine').context();\n\nsc.textFile('/path/...')\n  .flatMap(line =\n line.split(' '))\n  .map(word =\n [word, 1])\n  .reduceByKey((a, b) =\n a + b, 0)\n  .count().on('data', console.log);\n\n\n\n\nFeatures\n\n\n\n\nIn-memory computing\n\n\nControlled memory usage, spill to disk when necessary\n\n\nFast multiple distributed streams\n\n\nrealtime lazy compiling and running of execution graphs\n\n\nworkers can connect through TCP or websockets\n\n\n\n\nDocs \n community\n\n\n\n\nskale-engine API\n\n\nGitter\n for support and\n  discussion\n\n\nskale\n\n  mailing list for discussion about use and development\n\n\n\n\nQuickstart\n\n\nThe best and quickest way to get started with skale-engine is to use\n\nskale\n to create, run\nand deploy skale applications.\n\n\n$ npm install -g skale      # Install skale command once and for all\n$ skale create my_app       # Create a new app, install skale-engine\n$ cd my_app\n$ skale run                 # Starts a local cluster if necessary and run\n\n\n\nExamples\n\n\nIn the following, we bypass \nskale\n\ntoolbelt, and use directly and only skale-engine. It's for you if you are\nrather more interested by the skale-engine architecture, details and internals.\n\n\nTo run the internal examples, clone the skale-engine repository and\ninstall the dependencies:\n\n\n$ git clone git://github.com/skale-me/skale-engine.git --depth 1\n$ cd skale-engine\n$ npm install\n\n\n\nThen start a skale-engine server and workers on local host:\n\n\n$ npm start\n\n\n\nThen run whichever example you want\n\n\n$ ./examples/core/wordcount.js /etc/hosts\n\n\n\nTests\n\n\nTo run the test suite, first install the dependencies, then run \nnpm test\n:\n\n\n$ npm install\n$ npm test\n\n\n\nPeople\n\n\nThe original authors of skale-engine are \nCedric Artigue\n and \nMarc Vertes\n.\n\n\nList of all\ncontributors\n\n\nLicense\n\n\nApache-2.0", 
            "title": "Home"
        }, 
        {
            "location": "/#skale-engine", 
            "text": "Skale-engine is a fast and general purpose distributed data processing\nsystem. It provides a high-level API in Javascript and an optimized\nparallel execution engine on top of NodeJS.  Word count using skale:  var sc = require('skale-engine').context();\n\nsc.textFile('/path/...')\n  .flatMap(line =  line.split(' '))\n  .map(word =  [word, 1])\n  .reduceByKey((a, b) =  a + b, 0)\n  .count().on('data', console.log);", 
            "title": "skale-engine"
        }, 
        {
            "location": "/#features", 
            "text": "In-memory computing  Controlled memory usage, spill to disk when necessary  Fast multiple distributed streams  realtime lazy compiling and running of execution graphs  workers can connect through TCP or websockets", 
            "title": "Features"
        }, 
        {
            "location": "/#docs-community", 
            "text": "skale-engine API  Gitter  for support and\n  discussion  skale \n  mailing list for discussion about use and development", 
            "title": "Docs &amp; community"
        }, 
        {
            "location": "/#quickstart", 
            "text": "The best and quickest way to get started with skale-engine is to use skale  to create, run\nand deploy skale applications.  $ npm install -g skale      # Install skale command once and for all\n$ skale create my_app       # Create a new app, install skale-engine\n$ cd my_app\n$ skale run                 # Starts a local cluster if necessary and run", 
            "title": "Quickstart"
        }, 
        {
            "location": "/#examples", 
            "text": "In the following, we bypass  skale \ntoolbelt, and use directly and only skale-engine. It's for you if you are\nrather more interested by the skale-engine architecture, details and internals.  To run the internal examples, clone the skale-engine repository and\ninstall the dependencies:  $ git clone git://github.com/skale-me/skale-engine.git --depth 1\n$ cd skale-engine\n$ npm install  Then start a skale-engine server and workers on local host:  $ npm start  Then run whichever example you want  $ ./examples/core/wordcount.js /etc/hosts", 
            "title": "Examples"
        }, 
        {
            "location": "/#tests", 
            "text": "To run the test suite, first install the dependencies, then run  npm test :  $ npm install\n$ npm test", 
            "title": "Tests"
        }, 
        {
            "location": "/#people", 
            "text": "The original authors of skale-engine are  Cedric Artigue  and  Marc Vertes .  List of all\ncontributors", 
            "title": "People"
        }, 
        {
            "location": "/#license", 
            "text": "Apache-2.0", 
            "title": "License"
        }, 
        {
            "location": "/skale-API/", 
            "text": "Contents\n\n\n\n\n\n\n\n\n\n\nOverview\n\n\nWorking with datasets\n\n\nSkale module\n\n\nskale.context([config])\n\n\nsc.end()\n\n\nsc.parallelize(array)\n\n\nsc.range(start[, end[, step]])\n\n\nsc.textFile(path)\n\n\nsc.lineStream(input_stream)\n\n\nsc.objectStream(input_stream)\n\n\n\n\n\n\nDataset methods\n\n\nds.aggregate(reducer, combiner, init[,obj])\n\n\nds.aggregateByKey(reducer, combiner, init,[ obj])\n\n\nds.cartesian(other)\n\n\nds.coGroup(other)\n\n\nds.collect([opt])\n\n\nds.count()\n\n\nds.countByKey()\n\n\nds.countByValue()\n\n\nds.distinct()\n\n\nds.filter(filter[,obj])\n\n\nds.first()\n\n\nds.flatMap(flatMapper[,obj])\n\n\nds.flatMapValues(flatMapper[,obj])\n\n\nds.foreach(callback[, obj])\n\n\nds.groupByKey()\n\n\nds.intersection(other)\n\n\nds.join(other)\n\n\nds.keys()\n\n\nds.leftOuterJoin(other)\n\n\nds.lookup(k)\n\n\nds.map(mapper[,obj])\n\n\nds.mapValues(mapper[,obj])\n\n\nds.partitionBy(partitioner)\n\n\nds.persist()\n\n\nds.reduce(reducer, init[,obj])\n\n\nds.reduceByKey(reducer, init[, obj])\n\n\nds.rightOuterJoin(other)\n\n\nds.sample(withReplacement, frac, seed)\n\n\nds.sortBy(keyfunc[, ascending])\n\n\nds.sortByKey(ascending)\n\n\nds.subtract(other)\n\n\nds.take(num)\n\n\nds.top(num)\n\n\nds.union(other)\n\n\nds.values()\n\n\n\n\n\n\nPartitioners\n\n\nHashPartitioner(numPartitions)\n\n\nRangePartitioner(numPartitions, keyfunc, dataset)\n\n\n\n\n\n\n\n\n\n\n\nOverview\n\n\nSkale is a fast and general purpose distributed data processing\nsystem. It provides a high-level API in Javascript and an optimized\nparallel execution engine.\n\n\nA Skale application consists of a \nmaster\n program that runs the\nuser code and executes various \nparallel operations\n on a cluster\nof \nworkers\n.\n\n\nThe main abstraction Skale provides is a \ndataset\n which is similar\nto a Javascript \narray\n, but partitioned accross the workers that\ncan be operated in parallel.\n\n\nThere are several ways to create a dataset: \nparallelizing\n an existing\narray in the master program, or referencing a dataset in a distributed\nstorage system (such as HDFS), or \nstreaming\n the content of any\nsource that can be processed through Node.js \nStreams\n. We call\n\nsource\n a function which initializes a dataset.\n\n\nDatasets support two kinds of operations: \ntransformations\n, which create\na new dataset from an existing one, and \nactions\n, which\nreturn a value to the \nmaster\n program after running a computation\non the dataset.\n\n\nFor example, \nmap\n is a transformation that applies a function to\neach element of a dataset, returning a new dataset. On the other\nhand, \nreduce\n is an action that aggregates all elements of a dataset\nusing some function, and returns the final result to the master.\n\n\nSources\n and \ntransformations\n in Skale are \nlazy\n. They do not\nstart right away, but are triggered by \nactions\n, thus allowing\nefficient pipelined execution and optimized data transfers.\n\n\nA first example:\n\n\nvar sc = require('skale-engine').context();     // create a new context\nsc.parallelize([1, 2, 3, 4]).               // source\n   map(function (x) {return x+1}).          // transform\n   reduce(function (a, b) {return a+b}, 0). // action\n   on('data', console.log);                 // process result: 14\n\n\n\n\nWorking with datasets\n\n\n\n\nAfter having initialized a cluster context using\n\nskale.context()\n, one can create a dataset\nusing the following sources:\n\n\n\n\n\n\n\n\nSource Name\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nlineStream(stream)\n\n\nCreate a dataset from a text stream\n\n\n\n\n\n\nobjectStream(stream)\n\n\nCreate a dataset from an object stream\n\n\n\n\n\n\nparallelize(array)\n\n\nCreate a dataset from an array\n\n\n\n\n\n\nrange(start,end,step)\n\n\nCreate a dataset containing integers from start to end\n\n\n\n\n\n\ntextFile(path)\n\n\nCreate a dataset from a regular text file\n\n\n\n\n\n\n\n\nTransformations operate on a dataset and return a new dataset. Note that some\ntransformation operate only on datasets where each element is in the form\nof 2 elements array of key and value (\n[k,v]\n dataset):\n\n\n[[Ki,Vi], ..., [Kj, Vj]]\n\n\n\nA special transformation \npersist()\n enables one to \npersist\n a dataset\nin memory, allowing efficient reuse accross parallel operations.\n\n\n\n\n\n\n\n\nTransformation Name\n\n\nDescription\n\n\nin\n\n\nout\n\n\n\n\n\n\n\n\n\n\naggregateByKey(func, func, init)\n\n\nreduce and combine by key using functions\n\n\n[k,v]\n\n\n[k,v]\n\n\n\n\n\n\ncartesian(other)\n\n\nPerform a cartesian product with the other dataset\n\n\nv w\n\n\n[v,w]\n\n\n\n\n\n\ncoGroup(other)\n\n\nGroup data from both datasets sharing the same key\n\n\n[k,v] [k,w]\n\n\n[k,[[v],[w]]]\n\n\n\n\n\n\ndistinct()\n\n\nReturn a dataset where duplicates are removed\n\n\nv\n\n\nw\n\n\n\n\n\n\nfilter(func)\n\n\nReturn a dataset of elements on which function returns true\n\n\nv\n\n\nw\n\n\n\n\n\n\nflatMap(func)\n\n\nPass the dataset elements to a function which returns a sequence\n\n\nv\n\n\nw\n\n\n\n\n\n\ngroupByKey()\n\n\nGroup values with the same key\n\n\n[k,v]\n\n\n[k,[v]]\n\n\n\n\n\n\nintersection(other)\n\n\nReturn a dataset containing only elements found in both datasets\n\n\nv w\n\n\nv\n\n\n\n\n\n\njoin(other)\n\n\nPerform an inner join between 2 datasets\n\n\n[k,v]\n\n\n[k,[v,w]]\n\n\n\n\n\n\nleftOuterJoin(other)\n\n\nJoin 2 datasets where the key must be present in the other\n\n\n[k,v]\n\n\n[k,[v,w]]\n\n\n\n\n\n\nrightOuterJoin(other)\n\n\nJoin 2 datasets where the key must be present in the first\n\n\n[k,v]\n\n\n[k,[v,w]]\n\n\n\n\n\n\nkeys()\n\n\nReturn a dataset of just the keys\n\n\n[k,v]\n\n\nk\n\n\n\n\n\n\nmap(func)\n\n\nReturn a dataset where elements are passed through a function\n\n\nv\n\n\nw\n\n\n\n\n\n\nmapValues(func)\n\n\nMap a function to the value field of key-value dataset\n\n\n[k,v]\n\n\n[k,w]\n\n\n\n\n\n\nreduceByKey(func, init)\n\n\nCombine values with the same key\n\n\n[k,v]\n\n\n[k,w]\n\n\n\n\n\n\npartitionBy(partitioner)\n\n\nPartition using the partitioner\n\n\nv\n\n\nv\n\n\n\n\n\n\npersist()\n\n\nIdempotent. Keep content of dataset in cache for further reuse.\n\n\nv\n\n\nv\n\n\n\n\n\n\nsample(rep, frac, seed)\n\n\nSample a dataset, with or without replacement\n\n\nv\n\n\nw\n\n\n\n\n\n\nsortBy(func)\n\n\nSort a dataset\n\n\nv\n\n\nv\n\n\n\n\n\n\nsortByKey()\n\n\nSort a [k,v] dataset\n\n\n[k,v]\n\n\n[k,v]\n\n\n\n\n\n\nsubtract(other)\n\n\nRemove the content of one dataset\n\n\nv w\n\n\nv\n\n\n\n\n\n\nunion(other)\n\n\nReturn a dataset containing elements from both datasets\n\n\nv\n\n\nv w\n\n\n\n\n\n\nvalues()\n\n\nReturn a dataset of just the values\n\n\n[k,v]\n\n\nv\n\n\n\n\n\n\n\n\nActions operate on a dataset and send back results to the \nmaster\n. Results\nare always produced asynchronously. All actions return a \nreadable stream\n\non which results are emitted.\n\n\n\n\n\n\n\n\nAction Name\n\n\nDescription\n\n\nout\n\n\n\n\n\n\n\n\n\n\naggregate(func, func, init)\n\n\nSimilar to reduce() but may return a different type\n\n\nstream of value\n\n\n\n\n\n\ncollect()\n\n\nReturn the content of dataset\n\n\nstream of elements\n\n\n\n\n\n\ncount()\n\n\nReturn the number of elements from dataset\n\n\nstream of number\n\n\n\n\n\n\ncountByKey()\n\n\nReturn the number of occurrences for each key in a \n[k,v]\n dataset\n\n\nstream of [k,number]\n\n\n\n\n\n\ncountByValue()\n\n\nReturn the number of occurrences of elements from dataset\n\n\nstream of [v,number]\n\n\n\n\n\n\nfirst()\n\n\nReturn the first element in dataset\n\n\nstream of value\n\n\n\n\n\n\nforeach(func)\n\n\nApply the provided function to each element of the dataset\n\n\nempty stream\n\n\n\n\n\n\nlookup(k)\n\n\nReturn the list of values \nv\n for key \nk\n in a \n[k,v]\n dataset\n\n\nstream of v\n\n\n\n\n\n\nreduce(func, init)\n\n\nAggregates dataset elements using a function into one value\n\n\nstream of value\n\n\n\n\n\n\ntake(num)\n\n\nReturn the first \nnum\n elements of dataset\n\n\nstream of value\n\n\n\n\n\n\ntop(num)\n\n\nReturn the top \nnum\n elements of dataset\n\n\nstream of value\n\n\n\n\n\n\n\n\nSkale module\n\n\nThe Skale module is the main entry point for Skale functionality.\nTo use it, one must \nrequire('skale-engine')\n.\n\n\nskale.context([config])\n\n\nCreates and returns a new context which represents the connection\nto the Skale cluster, and which can be used to create datasets on that\ncluster. Config is an \nObject\n which defines the cluster server,\nwith the following defaults:\n\n\n{\n  host: 'localhost',    // Cluster server host, settable also by SKALE_HOST env\n  port: '12346'         // Cluster server port, settable also by SKALE_PORT env\n}\n\n\n\n\nExample:\n\n\nvar skale = require('skale-engine');\nvar sc = skale.context();\n\n\n\n\nsc.end()\n\n\nCloses the connection to the cluster.\n\n\nsc.parallelize(array)\n\n\nReturns a new dataset containing elements from the \nArray\n array.\n\n\nExample:\n\n\nvar a = sc.parallelize(['Hello', 'World']);\n\n\n\n\nsc.range(start[, end[, step]])\n\n\nReturns a new dataset of integers from \nstart\n to \nend\n (exclusive)\nincreased by \nstep\n (default 1) every element. If called with a\nsingle argument, the argument is interpreted as \nend\n, and \nstart\n\nis set to 0.\n\n\nsc.range(5).collect.toArray()\n// [ 0, 1, 2, 3, 4 ]\nsc.range(2, 4).collect.toArray()\n// [ 2, 3 ]\nsc.range(10, -5, -3).collect().toArray()\n// [ 10, 7, 4, 1, -2 ]\n\n\n\n\nsc.textFile(path)\n\n\nReturns a new dataset of lines composing the file specified by path\n\nString\n.\n\n\nNote: If using a path on the local filesystem, the file must also\nbe accessible at the same path on worker nodes. Either copy the\nfile to all workers or use a network-mounted shared file system.\n\n\nExample, the following program prints the length of a text file:\n\n\nvar lines = sc.textFile('data.txt');\nlines.map(s =\n s.length).reduce((a, b) =\n a + b, 0).on('data', console.log);\n\n\n\n\nsc.lineStream(input_stream)\n\n\nReturns a new dataset of lines of text read from input_stream\n\nObject\n, which is a \nreadable stream\n where dataset content is\nread from.\n\n\nThe following example computes the size of a file using streams:\n\n\nvar stream = fs.createReadStream('data.txt', 'utf8');\nsc.lineStream(stream).\n   map(s =\n s.length).\n   reduce((a, b) =\n a + b, 0).\n   on('data', console.log);\n\n\n\n\nsc.objectStream(input_stream)\n\n\nReturns a new dataset of Javascript \nObjects\n read from input_stream\n\nObject\n, which is a \nreadable stream\n where dataset content is\nread from.\n\n\nThe following example counts the number of objects returned in an\nobject stream using the mongodb native Javascript driver:\n\n\nvar cursor = db.collection('clients').find();\nsc.objectStream(cursor).count().on('data', console.log);\n\n\n\n\nDataset methods\n\n\nDataset objects, as created initially by above skale context source\nfunctions, have the following methods, allowing either to instantiate\na new dataset through a transformation, or to return results to the\nmaster program.\n\n\nds.aggregate(reducer, combiner, init[,obj])\n\n\nReturns a \nreadable stream\n of the aggregated value of the elements\nof the dataset using two functions \nreducer()\n and \ncombiner()\n,\nallowing to use an arbitrary accumulator type, different from element\ntype (as opposed to \nreduce()\n which imposes the same type for\naccumulator and element).\n\n\n\n\nreducer\n: a function of the form \nfunction(acc,val[,obj[,wc]])\n,\n  which returns the next value of the accumulator (which must be\n  of the same type as \nacc\n) and with:\n\n\nacc\n: the value of the accumulator, initially set to \ninit\n\n\nval\n: the value of the next element of the dataset on which\n   \naggregate()\n operates\n\n\nobj\n: the same parameter \nobj\n passed to \naggregate()\n\n\nwc\n: the worker context, a persistent object local to each\n   worker, where user can store and access worker local dependencies.\n\n\n\n\n\n\ncombiner\n: a function of the form \nfunction(acc1,acc2[,obj])\n,\n  which returns the merged value of accumulators and with:\n\n\nacc1\n: the value of an accumulator, computed locally on a worker\n\n\nacc2\n: the value of an other accumulator, issued by another worker\n\n\nobj\n: the same parameter \nobj\n passed to \naggregate()\n\n\n\n\n\n\ninit\n: the initial value of the accumulators that are used by\n  \nreducer()\n and \ncombiner()\n. It should be the identity element\n  of the operation (a neutral zero value, i.e. applying it through the\n  function should not change result).\n\n\nobj\n: user provided data. Data will be passed to carrying\n  serializable data from master to workers, obj is shared amongst\n  mapper executions over each element of the dataset.\n\n\n\n\nThe following example computes the average of a dataset, avoiding a \nmap()\n:\n\n\nsc.parallelize([3, 5, 2, 7, 4, 8]).\n   aggregate((a, v) =\n [a[0] + v, a[1] + 1],\n    (a1, a2) =\n [a1[0] + a2[0], a1[1] + a2[1]], [0, 0]).\n   on('data', function(data) {\n    console.log(data[0] / data[1]);\n  })\n// 4.8333\n\n\n\n\nds.aggregateByKey(reducer, combiner, init,[ obj])\n\n\nWhen called on a dataset of type \n[k,v]\n, returns a dataset of type\n\n[k,v]\n where \nv\n is the aggregated value of all elements of same\nkey \nk\n. The aggregation is performed using two functions \nreducer()\n\nand \ncombiner()\n allowing to use an arbitrary accumulator type,\ndifferent from element type.\n\n\n\n\nreducer\n: a function of the form \nfunction(acc,val[,obj[,wc]])\n,\n  which returns the next value of the accumulator (which must be\n  of the same type as \nacc\n) and with:\n\n\nacc\n: the value of the accumulator, initially set to \ninit\n\n\nval\n: the value \nv\n of the next \n[k,v]\n element of the dataset\n   on which \naggregateByKey()\n operates\n\n\nobj\n: the same parameter \nobj\n passed to \naggregateByKey()\n\n\nwc\n: the worker context, a persistent object local to each\n   worker, where user can store and access worker local dependencies.\n\n\n\n\n\n\ncombiner\n: a function of the form \nfunction(acc1,acc2[,obj])\n,\n  which returns the merged value of accumulators and with:\n\n\nacc1\n: the value of an accumulator, computed locally on a worker\n\n\nacc2\n: the value of an other accumulator, issued by another worker\n\n\nobj\n: the same parameter \nobj\n passed to \naggregate()\n\n\n\n\n\n\ninit\n: the initial value of the accumulators that are used by\n  \nreducer()\n and \ncombiner()\n. It should be the identity element\n  of the operation (a neutral zero value, i.e. applying it through the\n  function should not change result).\n\n\nobj\n: user provided data. Data will be passed to carrying\n  serializable data from master to workers, obj is shared amongst\n  mapper executions over each element of the dataset.\n\n\n\n\nExample:\n\n\nsc.parallelize([['hello', 1], ['hello', 1], ['world', 1]]).\n   aggregateByKey((a, b) =\n a + b, (a, b) =\n a + b, 0).\n   collect().toArray().then(console.log);\n// [ [ 'hello', 2 ], [ 'world', 1 ] ]\n\n\n\n\nds.cartesian(other)\n\n\nReturns a dataset wich contains all possible pairs \n[a, b]\n where \na\n\nis in the source dataset and \nb\n is in the \nother\n dataset.\n\n\nExample:\n\n\nvar ds1 = sc.parallelize([1, 2]);\nvar ds2 = sc.parallelize(['a', 'b', 'c']);\nds1.cartesian(ds2).collect().toArray().then(console.log);\n// [ [ 1, 'a' ], [ 1, 'b' ], [ 1, 'c' ],\n//   [ 2, 'a' ], [ 2, 'b' ], [ 2, 'c' ] ]\n\n\n\n\nds.coGroup(other)\n\n\nWhen called on dataset of type \n[k,v]\n and \n[k,w]\n, returns a dataset of type\n\n[k, [[v], [w]]]\n, where data of both datasets share the same key.\n\n\nExample:\n\n\nvar ds1 = sc.parallelize([[10, 1], [20, 2]]);\nvar ds2 = sc.parallelize([[10, 'world'], [30, 3]]);\nds1.coGroup(ds2).collect().on('data', console.log);\n// [ 10, [ [ 1 ], [ 'world' ] ] ]\n// [ 20, [ [ 2 ], [] ] ]\n// [ 30, [ [], [ 3 ] ] ]\n\n\n\n\nds.collect([opt])\n\n\nReturns a \nreadable stream\n of all elements of the dataset. Optional\n\nopt\n parameter is an object with the default content \n{text:\nfalse}\n. if \ntext\n option is \ntrue\n, each element is passed through\n\nJSON.stringify()\n and a 'newline' is appended, making it possible to\npipe to standard output or any text stream.\n\n\nExample:\n\n\nsc.parallelize([1, 2, 3, 4]).\n   collect({text: true}).pipe(process.stdout);\n// 1\n// 2\n// 3\n// 4\n\n\n\n\nds.count()\n\n\nReturns a \nreadable stream\n of the number of elements in the dataset.\n\n\nExample:\n\n\nsc.parallelize([10, 20, 30, 40]).count().on('data', console.log);\n// 4\n\n\n\n\nds.countByKey()\n\n\nWhen called on a dataset of type \n[k,v]\n, computes the number of occurrences\nof elements for each key in a dataset of type \n[k,v]\n. Returns a \nreadable stream\n\nof elements of type \n[k,w]\n where \nw\n is the result count.\n\n\nExample:\n\n\nsc.parallelize([[10, 1], [20, 2], [10, 4]]).\n   countByKey().on('data', console.log);\n// [ 10, 2 ]\n// [ 20, 1 ]\n\n\n\n\nds.countByValue()\n\n\nComputes the number of occurences of each element in dataset and returns\na \nreadable stream\n of elements of type \n[v,n]\n where \nv\n is the\nelement and \nn\n its number of occurrences.\n\n\nExample:\n\n\nsc.parallelize([ 1, 2, 3, 1, 3, 2, 5 ]).\n   countByValue().\n   toArray().then(console.log);\n// [ [ 1, 2 ], [ 2, 2 ], [ 3, 2 ], [ 5, 1 ] ]\n\n\n\n\nds.distinct()\n\n\nReturns a dataset where duplicates are removed.\n\n\nExample:\n\n\nsc.parallelize([ 1, 2, 3, 1, 4, 3, 5 ]).\n   distinct().\n   collect().toArray().then(console.log);\n// [ 1, 2, 3, 4, 5 ]\n\n\n\n\nds.filter(filter[,obj])\n\n\n\n\nfilter\n: a function of the form \ncallback(element[,obj[,wc]])\n,\n  returning a \nBoolean\n and where:\n\n\nelement\n: the next element of the dataset on which \nfilter()\n operates\n\n\nobj\n: the same parameter \nobj\n passed to \nfilter()\n\n\nwc\n: the worker context, a persistent object local to each\n  worker, where user can store and access worker local dependencies.\n\n\n\n\n\n\nobj\n: user provided data. Data will be passed to carrying\n  serializable data from master to workers, obj is shared amongst\n  mapper executions over each element of the dataset\n\n\n\n\nApplies the provided filter function to each element of the source\ndataset and returns a new dataset containing the elements that passed the\ntest.\n\n\nExample:\n\n\nfunction filter(data, obj) { return data % obj.modulo; }\n\nsc.parallelize([1, 2, 3, 4]).\n   filter(filter, {modulo: 2}).\n   collect().on('data', console.log);\n// 1 3\n\n\n\n\nds.first()\n\n\nReturns a \nreadable stream\n of the first element in this dataset.\n\n\nsc.parallelize([1, 2, 3]).first().on('data', console.log)\n// 1\n\n\n\n\nds.flatMap(flatMapper[,obj])\n\n\nApplies the provided mapper function to each element of the source\ndataset and returns a new dataset.\n\n\n\n\nflatMapper\n: a function of the form \ncallback(element[,obj[,wc]])\n,\n  returning an \nArray\n and where:\n\n\nelement\n: the next element of the dataset on which \nflatMap()\n operates\n\n\nobj\n: the same parameter \nobj\n passed to \nflatMap()\n\n\nwc\n: the worker context, a persistent object local to each\n  worker, where user can store and access worker local dependencies.\n\n\n\n\n\n\nobj\n: user provided data. Data will be passed to carrying\n  serializable data from master to workers, obj is shared amongst\n  mapper executions over each element of the dataset\n\n\n\n\nExample:\n\n\nfunction flatMapper(data, obj) {\n    var tmp = [];\n    for (var i = 0; i \n obj.N; i++) tmp.push(data);\n    return tmp;\n}\n\nsc.parallelize([1, 2, 3, 4]).\n   flatMap(flatMapper, {N: 2}).\n   collect().on('data', console.log);\n// [ 'hello', 2 ]\n// [ 'hello', 2 ]\n// [ 'world', 4 ]\n// [ 'world', 4 ]\n\n\n\n\nds.flatMapValues(flatMapper[,obj])\n\n\nApplies the provided flatMapper function to the value of each [key,\nvalue] element of the source dataset and return a new dataset containing\nelements defined as [key, mapper(value)], keeping the key unchanged\nfor each source element.\n\n\n\n\nflatMapper\n: a function of the form \ncallback(element[,obj[,wc]])\n,\n  returning an \nArray\n and where:\n\n\nelement\n: the value v of the next [k,v] element of the dataset on\n  which \nflatMapValues()\n operates\n\n\nobj\n: the same parameter \nobj\n passed to \nflatMapValues()\n\n\nwc\n: the worker context, a persistent object local to each\n  worker, where user can store and access worker local dependencies.\n\n\n\n\n\n\nobj\n: user provided data. Data will be passed to carrying\n  serializable data from master to workers, obj is shared amongst\n  mapper executions over each element of the dataset\n\n\n\n\nExample:\n\n\nfunction valueFlatMapper(data, obj) {\n    var tmp = [];\n    for (var i = 0; i \n obj.N; i++) tmp.push(data * obj.fact);\n    return tmp;\n}\n\nsc.parallelize([['hello', 1], ['world', 2]]).\n   flatMapValues(valueFlatMapper, {N: 2, fact: 2}).\n   collect().on('data', console.log);\n\n\n\n\nds.foreach(callback[, obj])\n\n\nThis action applies a \ncallback\n function on each element of the dataset.\nA stream is returned, and closed when all callbacks have returned.\nNo data is written on the stream.\n\n\n\n\ncallback\n: a function of the form \nfunction(val[,obj[,wc]])\n,\n  which returns \nnull\n and with:\n\n\nval\n: the value of the next element of the dataset on which\n   \nforeach()\n operates\n\n\nobj\n: the same parameter \nobj\n passed to \nforeach()\n\n\nwc\n: the worker context, a persistent object local to each\n   worker, where user can store and access worker local dependencies.\n\n\n\n\n\n\nobj\n: user provided data. Data will be passed to carrying\n  serializable data from master to workers, obj is shared amongst\n  mapper executions over each element of the dataset\n\n\n\n\nIn the following example, the \nconsole.log()\n callback provided\nto \nforeach()\n is executed on workers and may be not visible:\n\n\nsc.parallelize([1, 2, 3, 4]).\n   foreach(console.log).on('end', console.log('finished'));\n\n\n\n\nds.groupByKey()\n\n\nWhen called on a dataset of type \n[k,v]\n, returns a dataset of type \n[k, [v]]\n\nwhere values with the same key are grouped.\n\n\nExample:\n\n\nsc.parallelize([[10, 1], [20, 2], [10, 4]]).\n   groupByKey().collect().on('data', console.log);\n// [ 10, [ 1, 4 ] ]\n// [ 20, [ 2 ] ]\n\n\n\n\nds.intersection(other)\n\n\nReturns a dataset containing only elements found in source dataset and \nother\n\ndataset.\n\n\nExample:\n\n\nvar ds1 = sc.parallelize([1, 2, 3, 4, 5]);\nvar ds2 = sc.parallelize([3, 4, 5, 6, 7]);\nds1.intersection(ds2).collect().toArray().then(console.log);\n// [ 3, 4, 5 ]\n\n\n\n\nds.join(other)\n\n\nWhen called on source dataset of type \n[k,v]\n and \nother\n dataset of type\n\n[k,w]\n, returns a dataset of type \n[k, [v, w]]\n pairs with all pairs\nof elements for each key.\n\n\nExample:\n\n\nvar ds1 = sc.parallelize([[10, 1], [20, 2]]);\nvar ds2 = sc.parallelize([[10, 'world'], [30, 3]]);\nds1.join(ds2).collect().on('data', console.log);\n// [ 10, [ 1, 'world' ] ]\n\n\n\n\nds.keys()\n\n\nWhen called on source dataset of type \n[k,v]\n, returns a dataset with just\nthe elements \nk\n.\n\n\nExample:\n\n\nsc.parallelize([[10, 'world'], [30, 3]]).\n   keys.collect().on('data', console.log);\n// 10\n// 30\n\n\n\n\nds.leftOuterJoin(other)\n\n\nWhen called on source dataset of type \n[k,v]\n and \nother\n dataset of type\n\n[k,w]\n, returns a dataset of type \n[k, [v, w]]\n pairs where the key\nmust be present in the \nother\n dataset.\n\n\nExample:\n\n\nvar ds1 = sc.parallelize([[10, 1], [20, 2]]);\nvar ds2 = sc.parallelize([[10, 'world'], [30, 3]]);\nds1.leftOuterJoin(ds2).collect().on('data', console.log);\n// [ 10, [ 1, 'world' ] ]\n// [ 20, [ 2, null ] ]\n\n\n\n\nds.lookup(k)\n\n\nWhen called on source dataset of type \n[k,v]\n, returns a \nreadable stream\n\nof values \nv\n for key \nk\n.\n\n\nExample:\n\n\nsc.parallelize([[10, 'world'], [20, 2], [10, 1], [30, 3]]).\n   lookup(10).on('data', console.log);\n// world\n// 1\n\n\n\n\nds.map(mapper[,obj])\n\n\nApplies the provided mapper function to each element of the source\ndataset and returns a new dataset.\n\n\n\n\nmapper\n: a function of the form \ncallback(element[,obj[,wc]])\n,\n  returning an element and where:\n\n\nelement\n: the next element of the dataset on which \nmap()\n operates\n\n\nobj\n: the same parameter \nobj\n passed to \nmap()\n\n\nwc\n: the worker context, a persistent object local to each\n  worker, where user can store and access worker local dependencies.\n\n\n\n\n\n\nobj\n: user provided data. Data will be passed to carrying\n  serializable data from master to workers, obj is shared amongst\n  mapper executions over each element of the dataset\n\n\n\n\nThe following example program\n\n\nsc.parallelize([1, 2, 3, 4]).\n   map((data, obj) =\n data * obj.scaling, {scaling: 1.2}).\n   collect().toArray().then(console.log);\n// [ 1.2, 2.4, 3.6, 4.8 ]\n\n\n\n\nds.mapValues(mapper[,obj])\n\n\n\n\nmapper\n: a function of the form \ncallback(element[,obj[,wc]])\n,\n  returning an element and where:\n\n\nelement\n: the value v of the next [k,v] element of the dataset on\n  which \nmapValues()\n operates\n\n\nobj\n: the same parameter \nobj\n passed to \nmapValues()\n\n\nwc\n: the worker context, a persistent object local to each\n  worker, where user can store and access worker local dependencies\n\n\n\n\n\n\nobj\n: user provided data. Data will be passed to carrying\n  serializable data from master to workers, obj is shared amongst\n  mapper executions over each element of the dataset\n\n\n\n\nApplies the provided mapper function to the value of each \n[k,v]\n\nelement of the source dataset and return a new dataset containing elements\ndefined as \n[k, mapper(v)]\n, keeping the key unchanged for each\nsource element.\n\n\nExample:\n\n\nsc.parallelize([['hello', 1], ['world', 2]]).\n   mapValues((a, obj) =\n a*obj.fact, {fact: 2}).\n   collect().on('data', console.log);\n// ['hello', 2]\n// ['world', 4]\n\n\n\n\nds.partitionBy(partitioner)\n\n\nReturns a dataset partitioned using the specified partitioner. The\npurpose of this transformation is not to change the dataset content,\nbut to increase processing speed by ensuring that the elements\naccessed by further transfomations reside in the same partition.\n\n\nExample:\n\n\nvar skale = require('skale-engine');\nvar sc = skale.context();\n\nsc.parallelize([['hello', 1], ['world', 1], ['hello', 2], ['world', 2], ['cedric', 3]])\n  .partitionBy(new skale.HashPartitioner(3))\n  .collect.on('data', console.log)\n// ['world', 1], ['world', 2], ['hello', 1], ['hello', 2], ['cedric', 3]\n\n\n\n\nds.persist()\n\n\nReturns the dataset, and persists the dataset content on disk (and\nin memory if available) in order to directly reuse content in further\ntasks.\n\n\nExample:\n\n\nvar dataset = sc.range(100).map(a =\n a * a);\n\n// First action: compute dataset\ndataset.collect().on('data', console.log)\n\n// Second action: reuse dataset, avoid map transform\ndataset.collect().on('data', console.log)\n\n\n\n\nds.reduce(reducer, init[,obj])\n\n\nReturns a \nreadable stream\n of the aggregated value of the elements\nof the dataset using a \nreducer()\n function.\n\n\n\n\nreducer\n: a function of the form \nfunction(acc,val[,obj[,wc]])\n,\n  which returns the next value of the accumulator (which must be\n  of the same type as \nacc\n and \nval\n) and with:\n\n\nacc\n: the value of the accumulator, initially set to \ninit\n\n\nval\n: the value of the next element of the dataset on which\n   \nreduce()\n operates\n\n\nobj\n: the same parameter \nobj\n passed to \nreduce()\n\n\nwc\n: the worker context, a persistent object local to each\n   worker, where user can store and access worker local dependencies.\n\n\n\n\n\n\ninit\n: the initial value of the accumulators that are used by\n  \nreducer()\n. It should be the identity element of the operation\n  (i.e. applying it through the function should not change result).\n\n\nobj\n: user provided data. Data will be passed to carrying\n  serializable data from master to workers, obj is shared amongst\n  mapper executions over each element of the dataset\n\n\n\n\nExample:\n\n\nsc.parallelize([1, 2, 4, 8]).\n   reduce((a, b) =\n a + b, 0).\n   on('data', console.log);\n// 15\n\n\n\n\nds.reduceByKey(reducer, init[, obj])\n\n\n\n\nreducer\n: a function of the form \ncallback(acc,val[,obj[,wc]])\n,\n  returning the next value of the accumulator (which must be of the\n  same type as \nacc\n and \nval\n) and where:\n\n\nacc\n: the value of the accumulator, initially set to \ninit\n\n\nval\n: the value \nv\n of the next \n[k,v]\n element of the dataset on\n  which \nreduceByKey()\n operates\n\n\nobj\n: the same parameter \nobj\n passed to \nreduceByKey()\n\n\nwc\n: the worker context, a persistent object local to each\n  worker, where user can store and access worker local dependencies.\n\n\n\n\n\n\ninit\n: the initial value of accumulator for each key. Will be\n  passed to \nreducer\n.\n\n\nobj\n: user provided data. Data will be passed to carrying\n  serializable data from master to workers, obj is shared amongst\n  mapper executions over each element of the dataset\n\n\n\n\nWhen called on a dataset of type \n[k,v]\n, returns a dataset of type \n[k,v]\n\nwhere the values of each key are aggregated using the \nreducer\n\nfunction and the \ninit\n initial value.\n\n\nExample:\n\n\nsc.parallelize([[10, 1], [10, 2], [10, 4]]).\n   reduceByKey((a,b) =\n a+b, 0).\n   collect().on('data', console.log);\n// [10, 7]\n\n\n\n\nds.rightOuterJoin(other)\n\n\nWhen called on source dataset of type \n[k,v]\n and \nother\n dataset of type\n\n[k,w]\n, returns a dataset of type \n[k, [v, w]]\n pairs where the key\nmust be present in the \nsource\n dataset.\n\n\nExample:\n\n\nvar ds1 = sc.parallelize([[10, 1], [20, 2]]);\nvar ds2 = sc.parallelize([[10, 'world'], [30, 3]]);\nds1.rightOuterJoin(ds2).collect().on('data', console.log);\n// [ 10, [ 1, 'world' ] ]\n// [ 30, [ null, 2 ] ]\n\n\n\n\nds.sample(withReplacement, frac, seed)\n\n\n\n\nwithReplacement\n: \nBoolean\n value, \ntrue\n if data must be sampled\n  with replacement\n\n\nfrac\n: \nNumber\n value of the fraction of source dataset to return\n\n\nseed\n: \nNumber\n value of pseudo-random seed\n\n\n\n\nReturns a dataset by sampling a fraction \nfrac\n of source dataset, with or\nwithout replacement, using a given random generator \nseed\n.\n\n\nExample:\n\n\nsc.parallelize([1, 2, 3, 4, 5, 6, 7, 8]).\n   sample(true, 0.5, 0).\n   collect().toArray().then(console.log);\n// [ 1, 1, 3, 4, 4, 5, 7 ]\n\n\n\n\nds.sortBy(keyfunc[, ascending])\n\n\nReturns a dataset sorted by the given \nkeyfunc\n.\n\n\n\n\nkeyfunc\n: a function of the form \nfunction(element)\n which returns\n  a value used for comparison in the sort function and where \nelement\n\n  is the next element of the dataset on which \nsortBy()\n operates\n\n\nascending\n: a boolean to set the sort direction. Default: true\n\n\n\n\nExample:\n\n\nsc.parallelize([4, 6, 10, 5, 1, 2, 9, 7, 3, 0])\n  .sortBy(a =\n a)\n  .collect().toArray().then(console.log)\n// [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n\n\n\nds.sortByKey(ascending)\n\n\nWhen called on a dataset of type \n[k,v]\n, returns a dataset of type \n[k,v]\n\nsorted on \nk\n. The optional parameter \nascending\n is a boolean which sets\nthe sort direction, true by default.\n\n\nExample:\n\n\nsc.parallelize([['world', 2], ['cedric', 3], ['hello', 1]])\n  .sortByKey()\n  .collect().toArray().then(console.log)\n// [['cedric', 3], ['hello', 1], ['world', 2]]\n\n\n\n\nds.subtract(other)\n\n\nReturns a dataset containing only elements of source dataset which are not\nin \nother\n dataset.\n\n\nExample:\n\n\nvar ds1 = sc.parallelize([1, 2, 3, 4, 5]);\nvar ds2 = sc.parallelize([3, 4, 5, 6, 7]);\nds1.subtract(ds2).collect().on('data', console.log);\n// 1 2\n\n\n\n\nds.take(num)\n\n\nReturns a \nreadable stream\n of the \nnum\n first elements of the source\ndataset.\n\n\nExample:\n\n\nsc.parallelize([1, 2, 3, 4]).\n   take(2).\n   toArray().then(console.log)\n// [1, 2]\n\n\n\n\nds.top(num)\n\n\nReturns a \nreadable stream\n of the \nnum\n top elements of the source\ndataset.\n\n\nExample:\n\n\nsc.parallelize([1, 2, 3, 4]).\n   top(2).\n   toArray().then(console.log)\n// [3, 4]\n\n\n\n\nds.union(other)\n\n\nReturns a dataset that contains the union of the elements in the source\ndataset and the \nother\n dataset.\n\n\nExample:\n\n\nvar ds1 = sc.parallelize([1, 2, 3, 4, 5]);\nvar ds2 = sc.parallelize([3, 4, 5, 6, 7]);\nds1.union(ds2).collect().toArray().then(console.log);\n// [ 1, 2, 3, 4, 5, 3, 4, 5, 6, 7 ]\n\n\n\n\nds.values()\n\n\nWhen called on source dataset of type \n[k,v]\n, returns a dataset with just\nthe elements \nv\n.\n\n\nExample:\n\n\nsc.parallelize([[10, 'world'], [30, 3]]).\n   keys.collect().on('data', console.log);\n// 'world'\n// 3\n\n\n\n\nPartitioners\n\n\nA partitioner is an object passed to\n\nds.partitionBy(partitioner)\n which\nplaces data in partitions according to a strategy, for example hash\npartitioning, where data having the same key are placed in the same\npartition, or range partitioning, where data in the same range are\nin the same partition. This is useful to accelerate processing, as\nit limits data transfers between workers during jobs.\n\n\nA partition object must provide the following properties:\n\n\n\n\nnumPartitions\n: a \nNumber\n of partitions for the dataset\n\n\ngetPartitionIndex\n: a \nFunction\n of type \nfunction(element)\n\n  which returns the partition index (comprised between 0 and\n  \nnumPartitions\n) for the \nelement\n of the dataset on which\n  \npartitionBy()\n operates.\n\n\n\n\nHashPartitioner(numPartitions)\n\n\nReturns a partitioner object which implements hash based partitioning\nusing a hash checksum of each element as a string.\n\n\n\n\nnumPartitions\n: \nNumber\n of partitions for this dataset\n\n\n\n\nExample:\n\n\nvar hp = new skale.HashPartitioner(3)\nvar dataset = sc.range(10).partitionBy(hp)\n\n\n\n\nRangePartitioner(numPartitions, keyfunc, dataset)\n\n\nReturns a partitioner object which first defines ranges by sampling\nthe dataset and then places elements by comparing them with ranges.\n\n\n\n\nnumPartitions\n: \nNumber\n of partitions for this dataset\n\n\nkeyfunc\n: a function of the form \nfunction(element)\n which returns\n  a value used for comparison in the sort function and where \nelement\n\n  is the next element of the dataset on which \npartitionBy()\n operates\n\n\ndataset\n: the dataset object on which \npartitionBy()\n operates\n\n\n\n\nExample:\n\n\nvar dataset = sc.range(100)\nvar rp = new skale.RangePartitioner(3, a =\n a, dataset)\nvar dataset = sc.range(10).partitionBy(rp)", 
            "title": "skale API"
        }, 
        {
            "location": "/skale-API/#contents", 
            "text": "Overview  Working with datasets  Skale module  skale.context([config])  sc.end()  sc.parallelize(array)  sc.range(start[, end[, step]])  sc.textFile(path)  sc.lineStream(input_stream)  sc.objectStream(input_stream)    Dataset methods  ds.aggregate(reducer, combiner, init[,obj])  ds.aggregateByKey(reducer, combiner, init,[ obj])  ds.cartesian(other)  ds.coGroup(other)  ds.collect([opt])  ds.count()  ds.countByKey()  ds.countByValue()  ds.distinct()  ds.filter(filter[,obj])  ds.first()  ds.flatMap(flatMapper[,obj])  ds.flatMapValues(flatMapper[,obj])  ds.foreach(callback[, obj])  ds.groupByKey()  ds.intersection(other)  ds.join(other)  ds.keys()  ds.leftOuterJoin(other)  ds.lookup(k)  ds.map(mapper[,obj])  ds.mapValues(mapper[,obj])  ds.partitionBy(partitioner)  ds.persist()  ds.reduce(reducer, init[,obj])  ds.reduceByKey(reducer, init[, obj])  ds.rightOuterJoin(other)  ds.sample(withReplacement, frac, seed)  ds.sortBy(keyfunc[, ascending])  ds.sortByKey(ascending)  ds.subtract(other)  ds.take(num)  ds.top(num)  ds.union(other)  ds.values()    Partitioners  HashPartitioner(numPartitions)  RangePartitioner(numPartitions, keyfunc, dataset)", 
            "title": "Contents"
        }, 
        {
            "location": "/skale-API/#overview", 
            "text": "Skale is a fast and general purpose distributed data processing\nsystem. It provides a high-level API in Javascript and an optimized\nparallel execution engine.  A Skale application consists of a  master  program that runs the\nuser code and executes various  parallel operations  on a cluster\nof  workers .  The main abstraction Skale provides is a  dataset  which is similar\nto a Javascript  array , but partitioned accross the workers that\ncan be operated in parallel.  There are several ways to create a dataset:  parallelizing  an existing\narray in the master program, or referencing a dataset in a distributed\nstorage system (such as HDFS), or  streaming  the content of any\nsource that can be processed through Node.js  Streams . We call source  a function which initializes a dataset.  Datasets support two kinds of operations:  transformations , which create\na new dataset from an existing one, and  actions , which\nreturn a value to the  master  program after running a computation\non the dataset.  For example,  map  is a transformation that applies a function to\neach element of a dataset, returning a new dataset. On the other\nhand,  reduce  is an action that aggregates all elements of a dataset\nusing some function, and returns the final result to the master.  Sources  and  transformations  in Skale are  lazy . They do not\nstart right away, but are triggered by  actions , thus allowing\nefficient pipelined execution and optimized data transfers.  A first example:  var sc = require('skale-engine').context();     // create a new context\nsc.parallelize([1, 2, 3, 4]).               // source\n   map(function (x) {return x+1}).          // transform\n   reduce(function (a, b) {return a+b}, 0). // action\n   on('data', console.log);                 // process result: 14", 
            "title": "Overview"
        }, 
        {
            "location": "/skale-API/#working-with-datasets", 
            "text": "After having initialized a cluster context using skale.context() , one can create a dataset\nusing the following sources:     Source Name  Description      lineStream(stream)  Create a dataset from a text stream    objectStream(stream)  Create a dataset from an object stream    parallelize(array)  Create a dataset from an array    range(start,end,step)  Create a dataset containing integers from start to end    textFile(path)  Create a dataset from a regular text file     Transformations operate on a dataset and return a new dataset. Note that some\ntransformation operate only on datasets where each element is in the form\nof 2 elements array of key and value ( [k,v]  dataset):  [[Ki,Vi], ..., [Kj, Vj]]  A special transformation  persist()  enables one to  persist  a dataset\nin memory, allowing efficient reuse accross parallel operations.     Transformation Name  Description  in  out      aggregateByKey(func, func, init)  reduce and combine by key using functions  [k,v]  [k,v]    cartesian(other)  Perform a cartesian product with the other dataset  v w  [v,w]    coGroup(other)  Group data from both datasets sharing the same key  [k,v] [k,w]  [k,[[v],[w]]]    distinct()  Return a dataset where duplicates are removed  v  w    filter(func)  Return a dataset of elements on which function returns true  v  w    flatMap(func)  Pass the dataset elements to a function which returns a sequence  v  w    groupByKey()  Group values with the same key  [k,v]  [k,[v]]    intersection(other)  Return a dataset containing only elements found in both datasets  v w  v    join(other)  Perform an inner join between 2 datasets  [k,v]  [k,[v,w]]    leftOuterJoin(other)  Join 2 datasets where the key must be present in the other  [k,v]  [k,[v,w]]    rightOuterJoin(other)  Join 2 datasets where the key must be present in the first  [k,v]  [k,[v,w]]    keys()  Return a dataset of just the keys  [k,v]  k    map(func)  Return a dataset where elements are passed through a function  v  w    mapValues(func)  Map a function to the value field of key-value dataset  [k,v]  [k,w]    reduceByKey(func, init)  Combine values with the same key  [k,v]  [k,w]    partitionBy(partitioner)  Partition using the partitioner  v  v    persist()  Idempotent. Keep content of dataset in cache for further reuse.  v  v    sample(rep, frac, seed)  Sample a dataset, with or without replacement  v  w    sortBy(func)  Sort a dataset  v  v    sortByKey()  Sort a [k,v] dataset  [k,v]  [k,v]    subtract(other)  Remove the content of one dataset  v w  v    union(other)  Return a dataset containing elements from both datasets  v  v w    values()  Return a dataset of just the values  [k,v]  v     Actions operate on a dataset and send back results to the  master . Results\nare always produced asynchronously. All actions return a  readable stream \non which results are emitted.     Action Name  Description  out      aggregate(func, func, init)  Similar to reduce() but may return a different type  stream of value    collect()  Return the content of dataset  stream of elements    count()  Return the number of elements from dataset  stream of number    countByKey()  Return the number of occurrences for each key in a  [k,v]  dataset  stream of [k,number]    countByValue()  Return the number of occurrences of elements from dataset  stream of [v,number]    first()  Return the first element in dataset  stream of value    foreach(func)  Apply the provided function to each element of the dataset  empty stream    lookup(k)  Return the list of values  v  for key  k  in a  [k,v]  dataset  stream of v    reduce(func, init)  Aggregates dataset elements using a function into one value  stream of value    take(num)  Return the first  num  elements of dataset  stream of value    top(num)  Return the top  num  elements of dataset  stream of value", 
            "title": "Working with datasets"
        }, 
        {
            "location": "/skale-API/#skale-module", 
            "text": "The Skale module is the main entry point for Skale functionality.\nTo use it, one must  require('skale-engine') .", 
            "title": "Skale module"
        }, 
        {
            "location": "/skale-API/#skalecontextconfig", 
            "text": "Creates and returns a new context which represents the connection\nto the Skale cluster, and which can be used to create datasets on that\ncluster. Config is an  Object  which defines the cluster server,\nwith the following defaults:  {\n  host: 'localhost',    // Cluster server host, settable also by SKALE_HOST env\n  port: '12346'         // Cluster server port, settable also by SKALE_PORT env\n}  Example:  var skale = require('skale-engine');\nvar sc = skale.context();", 
            "title": "skale.context([config])"
        }, 
        {
            "location": "/skale-API/#scend", 
            "text": "Closes the connection to the cluster.", 
            "title": "sc.end()"
        }, 
        {
            "location": "/skale-API/#scparallelizearray", 
            "text": "Returns a new dataset containing elements from the  Array  array.  Example:  var a = sc.parallelize(['Hello', 'World']);", 
            "title": "sc.parallelize(array)"
        }, 
        {
            "location": "/skale-API/#scrangestart-end-step", 
            "text": "Returns a new dataset of integers from  start  to  end  (exclusive)\nincreased by  step  (default 1) every element. If called with a\nsingle argument, the argument is interpreted as  end , and  start \nis set to 0.  sc.range(5).collect.toArray()\n// [ 0, 1, 2, 3, 4 ]\nsc.range(2, 4).collect.toArray()\n// [ 2, 3 ]\nsc.range(10, -5, -3).collect().toArray()\n// [ 10, 7, 4, 1, -2 ]", 
            "title": "sc.range(start[, end[, step]])"
        }, 
        {
            "location": "/skale-API/#sctextfilepath", 
            "text": "Returns a new dataset of lines composing the file specified by path String .  Note: If using a path on the local filesystem, the file must also\nbe accessible at the same path on worker nodes. Either copy the\nfile to all workers or use a network-mounted shared file system.  Example, the following program prints the length of a text file:  var lines = sc.textFile('data.txt');\nlines.map(s =  s.length).reduce((a, b) =  a + b, 0).on('data', console.log);", 
            "title": "sc.textFile(path)"
        }, 
        {
            "location": "/skale-API/#sclinestreaminput_stream", 
            "text": "Returns a new dataset of lines of text read from input_stream Object , which is a  readable stream  where dataset content is\nread from.  The following example computes the size of a file using streams:  var stream = fs.createReadStream('data.txt', 'utf8');\nsc.lineStream(stream).\n   map(s =  s.length).\n   reduce((a, b) =  a + b, 0).\n   on('data', console.log);", 
            "title": "sc.lineStream(input_stream)"
        }, 
        {
            "location": "/skale-API/#scobjectstreaminput_stream", 
            "text": "Returns a new dataset of Javascript  Objects  read from input_stream Object , which is a  readable stream  where dataset content is\nread from.  The following example counts the number of objects returned in an\nobject stream using the mongodb native Javascript driver:  var cursor = db.collection('clients').find();\nsc.objectStream(cursor).count().on('data', console.log);", 
            "title": "sc.objectStream(input_stream)"
        }, 
        {
            "location": "/skale-API/#dataset-methods", 
            "text": "Dataset objects, as created initially by above skale context source\nfunctions, have the following methods, allowing either to instantiate\na new dataset through a transformation, or to return results to the\nmaster program.", 
            "title": "Dataset methods"
        }, 
        {
            "location": "/skale-API/#dsaggregatereducer-combiner-initobj", 
            "text": "Returns a  readable stream  of the aggregated value of the elements\nof the dataset using two functions  reducer()  and  combiner() ,\nallowing to use an arbitrary accumulator type, different from element\ntype (as opposed to  reduce()  which imposes the same type for\naccumulator and element).   reducer : a function of the form  function(acc,val[,obj[,wc]]) ,\n  which returns the next value of the accumulator (which must be\n  of the same type as  acc ) and with:  acc : the value of the accumulator, initially set to  init  val : the value of the next element of the dataset on which\n    aggregate()  operates  obj : the same parameter  obj  passed to  aggregate()  wc : the worker context, a persistent object local to each\n   worker, where user can store and access worker local dependencies.    combiner : a function of the form  function(acc1,acc2[,obj]) ,\n  which returns the merged value of accumulators and with:  acc1 : the value of an accumulator, computed locally on a worker  acc2 : the value of an other accumulator, issued by another worker  obj : the same parameter  obj  passed to  aggregate()    init : the initial value of the accumulators that are used by\n   reducer()  and  combiner() . It should be the identity element\n  of the operation (a neutral zero value, i.e. applying it through the\n  function should not change result).  obj : user provided data. Data will be passed to carrying\n  serializable data from master to workers, obj is shared amongst\n  mapper executions over each element of the dataset.   The following example computes the average of a dataset, avoiding a  map() :  sc.parallelize([3, 5, 2, 7, 4, 8]).\n   aggregate((a, v) =  [a[0] + v, a[1] + 1],\n    (a1, a2) =  [a1[0] + a2[0], a1[1] + a2[1]], [0, 0]).\n   on('data', function(data) {\n    console.log(data[0] / data[1]);\n  })\n// 4.8333", 
            "title": "ds.aggregate(reducer, combiner, init[,obj])"
        }, 
        {
            "location": "/skale-API/#dsaggregatebykeyreducer-combiner-init-obj", 
            "text": "When called on a dataset of type  [k,v] , returns a dataset of type [k,v]  where  v  is the aggregated value of all elements of same\nkey  k . The aggregation is performed using two functions  reducer() \nand  combiner()  allowing to use an arbitrary accumulator type,\ndifferent from element type.   reducer : a function of the form  function(acc,val[,obj[,wc]]) ,\n  which returns the next value of the accumulator (which must be\n  of the same type as  acc ) and with:  acc : the value of the accumulator, initially set to  init  val : the value  v  of the next  [k,v]  element of the dataset\n   on which  aggregateByKey()  operates  obj : the same parameter  obj  passed to  aggregateByKey()  wc : the worker context, a persistent object local to each\n   worker, where user can store and access worker local dependencies.    combiner : a function of the form  function(acc1,acc2[,obj]) ,\n  which returns the merged value of accumulators and with:  acc1 : the value of an accumulator, computed locally on a worker  acc2 : the value of an other accumulator, issued by another worker  obj : the same parameter  obj  passed to  aggregate()    init : the initial value of the accumulators that are used by\n   reducer()  and  combiner() . It should be the identity element\n  of the operation (a neutral zero value, i.e. applying it through the\n  function should not change result).  obj : user provided data. Data will be passed to carrying\n  serializable data from master to workers, obj is shared amongst\n  mapper executions over each element of the dataset.   Example:  sc.parallelize([['hello', 1], ['hello', 1], ['world', 1]]).\n   aggregateByKey((a, b) =  a + b, (a, b) =  a + b, 0).\n   collect().toArray().then(console.log);\n// [ [ 'hello', 2 ], [ 'world', 1 ] ]", 
            "title": "ds.aggregateByKey(reducer, combiner, init,[ obj])"
        }, 
        {
            "location": "/skale-API/#dscartesianother", 
            "text": "Returns a dataset wich contains all possible pairs  [a, b]  where  a \nis in the source dataset and  b  is in the  other  dataset.  Example:  var ds1 = sc.parallelize([1, 2]);\nvar ds2 = sc.parallelize(['a', 'b', 'c']);\nds1.cartesian(ds2).collect().toArray().then(console.log);\n// [ [ 1, 'a' ], [ 1, 'b' ], [ 1, 'c' ],\n//   [ 2, 'a' ], [ 2, 'b' ], [ 2, 'c' ] ]", 
            "title": "ds.cartesian(other)"
        }, 
        {
            "location": "/skale-API/#dscogroupother", 
            "text": "When called on dataset of type  [k,v]  and  [k,w] , returns a dataset of type [k, [[v], [w]]] , where data of both datasets share the same key.  Example:  var ds1 = sc.parallelize([[10, 1], [20, 2]]);\nvar ds2 = sc.parallelize([[10, 'world'], [30, 3]]);\nds1.coGroup(ds2).collect().on('data', console.log);\n// [ 10, [ [ 1 ], [ 'world' ] ] ]\n// [ 20, [ [ 2 ], [] ] ]\n// [ 30, [ [], [ 3 ] ] ]", 
            "title": "ds.coGroup(other)"
        }, 
        {
            "location": "/skale-API/#dscollectopt", 
            "text": "Returns a  readable stream  of all elements of the dataset. Optional opt  parameter is an object with the default content  {text:\nfalse} . if  text  option is  true , each element is passed through JSON.stringify()  and a 'newline' is appended, making it possible to\npipe to standard output or any text stream.  Example:  sc.parallelize([1, 2, 3, 4]).\n   collect({text: true}).pipe(process.stdout);\n// 1\n// 2\n// 3\n// 4", 
            "title": "ds.collect([opt])"
        }, 
        {
            "location": "/skale-API/#dscount", 
            "text": "Returns a  readable stream  of the number of elements in the dataset.  Example:  sc.parallelize([10, 20, 30, 40]).count().on('data', console.log);\n// 4", 
            "title": "ds.count()"
        }, 
        {
            "location": "/skale-API/#dscountbykey", 
            "text": "When called on a dataset of type  [k,v] , computes the number of occurrences\nof elements for each key in a dataset of type  [k,v] . Returns a  readable stream \nof elements of type  [k,w]  where  w  is the result count.  Example:  sc.parallelize([[10, 1], [20, 2], [10, 4]]).\n   countByKey().on('data', console.log);\n// [ 10, 2 ]\n// [ 20, 1 ]", 
            "title": "ds.countByKey()"
        }, 
        {
            "location": "/skale-API/#dscountbyvalue", 
            "text": "Computes the number of occurences of each element in dataset and returns\na  readable stream  of elements of type  [v,n]  where  v  is the\nelement and  n  its number of occurrences.  Example:  sc.parallelize([ 1, 2, 3, 1, 3, 2, 5 ]).\n   countByValue().\n   toArray().then(console.log);\n// [ [ 1, 2 ], [ 2, 2 ], [ 3, 2 ], [ 5, 1 ] ]", 
            "title": "ds.countByValue()"
        }, 
        {
            "location": "/skale-API/#dsdistinct", 
            "text": "Returns a dataset where duplicates are removed.  Example:  sc.parallelize([ 1, 2, 3, 1, 4, 3, 5 ]).\n   distinct().\n   collect().toArray().then(console.log);\n// [ 1, 2, 3, 4, 5 ]", 
            "title": "ds.distinct()"
        }, 
        {
            "location": "/skale-API/#dsfilterfilterobj", 
            "text": "filter : a function of the form  callback(element[,obj[,wc]]) ,\n  returning a  Boolean  and where:  element : the next element of the dataset on which  filter()  operates  obj : the same parameter  obj  passed to  filter()  wc : the worker context, a persistent object local to each\n  worker, where user can store and access worker local dependencies.    obj : user provided data. Data will be passed to carrying\n  serializable data from master to workers, obj is shared amongst\n  mapper executions over each element of the dataset   Applies the provided filter function to each element of the source\ndataset and returns a new dataset containing the elements that passed the\ntest.  Example:  function filter(data, obj) { return data % obj.modulo; }\n\nsc.parallelize([1, 2, 3, 4]).\n   filter(filter, {modulo: 2}).\n   collect().on('data', console.log);\n// 1 3", 
            "title": "ds.filter(filter[,obj])"
        }, 
        {
            "location": "/skale-API/#dsfirst", 
            "text": "Returns a  readable stream  of the first element in this dataset.  sc.parallelize([1, 2, 3]).first().on('data', console.log)\n// 1", 
            "title": "ds.first()"
        }, 
        {
            "location": "/skale-API/#dsflatmapflatmapperobj", 
            "text": "Applies the provided mapper function to each element of the source\ndataset and returns a new dataset.   flatMapper : a function of the form  callback(element[,obj[,wc]]) ,\n  returning an  Array  and where:  element : the next element of the dataset on which  flatMap()  operates  obj : the same parameter  obj  passed to  flatMap()  wc : the worker context, a persistent object local to each\n  worker, where user can store and access worker local dependencies.    obj : user provided data. Data will be passed to carrying\n  serializable data from master to workers, obj is shared amongst\n  mapper executions over each element of the dataset   Example:  function flatMapper(data, obj) {\n    var tmp = [];\n    for (var i = 0; i   obj.N; i++) tmp.push(data);\n    return tmp;\n}\n\nsc.parallelize([1, 2, 3, 4]).\n   flatMap(flatMapper, {N: 2}).\n   collect().on('data', console.log);\n// [ 'hello', 2 ]\n// [ 'hello', 2 ]\n// [ 'world', 4 ]\n// [ 'world', 4 ]", 
            "title": "ds.flatMap(flatMapper[,obj])"
        }, 
        {
            "location": "/skale-API/#dsflatmapvaluesflatmapperobj", 
            "text": "Applies the provided flatMapper function to the value of each [key,\nvalue] element of the source dataset and return a new dataset containing\nelements defined as [key, mapper(value)], keeping the key unchanged\nfor each source element.   flatMapper : a function of the form  callback(element[,obj[,wc]]) ,\n  returning an  Array  and where:  element : the value v of the next [k,v] element of the dataset on\n  which  flatMapValues()  operates  obj : the same parameter  obj  passed to  flatMapValues()  wc : the worker context, a persistent object local to each\n  worker, where user can store and access worker local dependencies.    obj : user provided data. Data will be passed to carrying\n  serializable data from master to workers, obj is shared amongst\n  mapper executions over each element of the dataset   Example:  function valueFlatMapper(data, obj) {\n    var tmp = [];\n    for (var i = 0; i   obj.N; i++) tmp.push(data * obj.fact);\n    return tmp;\n}\n\nsc.parallelize([['hello', 1], ['world', 2]]).\n   flatMapValues(valueFlatMapper, {N: 2, fact: 2}).\n   collect().on('data', console.log);", 
            "title": "ds.flatMapValues(flatMapper[,obj])"
        }, 
        {
            "location": "/skale-API/#dsforeachcallback-obj", 
            "text": "This action applies a  callback  function on each element of the dataset.\nA stream is returned, and closed when all callbacks have returned.\nNo data is written on the stream.   callback : a function of the form  function(val[,obj[,wc]]) ,\n  which returns  null  and with:  val : the value of the next element of the dataset on which\n    foreach()  operates  obj : the same parameter  obj  passed to  foreach()  wc : the worker context, a persistent object local to each\n   worker, where user can store and access worker local dependencies.    obj : user provided data. Data will be passed to carrying\n  serializable data from master to workers, obj is shared amongst\n  mapper executions over each element of the dataset   In the following example, the  console.log()  callback provided\nto  foreach()  is executed on workers and may be not visible:  sc.parallelize([1, 2, 3, 4]).\n   foreach(console.log).on('end', console.log('finished'));", 
            "title": "ds.foreach(callback[, obj])"
        }, 
        {
            "location": "/skale-API/#dsgroupbykey", 
            "text": "When called on a dataset of type  [k,v] , returns a dataset of type  [k, [v]] \nwhere values with the same key are grouped.  Example:  sc.parallelize([[10, 1], [20, 2], [10, 4]]).\n   groupByKey().collect().on('data', console.log);\n// [ 10, [ 1, 4 ] ]\n// [ 20, [ 2 ] ]", 
            "title": "ds.groupByKey()"
        }, 
        {
            "location": "/skale-API/#dsintersectionother", 
            "text": "Returns a dataset containing only elements found in source dataset and  other \ndataset.  Example:  var ds1 = sc.parallelize([1, 2, 3, 4, 5]);\nvar ds2 = sc.parallelize([3, 4, 5, 6, 7]);\nds1.intersection(ds2).collect().toArray().then(console.log);\n// [ 3, 4, 5 ]", 
            "title": "ds.intersection(other)"
        }, 
        {
            "location": "/skale-API/#dsjoinother", 
            "text": "When called on source dataset of type  [k,v]  and  other  dataset of type [k,w] , returns a dataset of type  [k, [v, w]]  pairs with all pairs\nof elements for each key.  Example:  var ds1 = sc.parallelize([[10, 1], [20, 2]]);\nvar ds2 = sc.parallelize([[10, 'world'], [30, 3]]);\nds1.join(ds2).collect().on('data', console.log);\n// [ 10, [ 1, 'world' ] ]", 
            "title": "ds.join(other)"
        }, 
        {
            "location": "/skale-API/#dskeys", 
            "text": "When called on source dataset of type  [k,v] , returns a dataset with just\nthe elements  k .  Example:  sc.parallelize([[10, 'world'], [30, 3]]).\n   keys.collect().on('data', console.log);\n// 10\n// 30", 
            "title": "ds.keys()"
        }, 
        {
            "location": "/skale-API/#dsleftouterjoinother", 
            "text": "When called on source dataset of type  [k,v]  and  other  dataset of type [k,w] , returns a dataset of type  [k, [v, w]]  pairs where the key\nmust be present in the  other  dataset.  Example:  var ds1 = sc.parallelize([[10, 1], [20, 2]]);\nvar ds2 = sc.parallelize([[10, 'world'], [30, 3]]);\nds1.leftOuterJoin(ds2).collect().on('data', console.log);\n// [ 10, [ 1, 'world' ] ]\n// [ 20, [ 2, null ] ]", 
            "title": "ds.leftOuterJoin(other)"
        }, 
        {
            "location": "/skale-API/#dslookupk", 
            "text": "When called on source dataset of type  [k,v] , returns a  readable stream \nof values  v  for key  k .  Example:  sc.parallelize([[10, 'world'], [20, 2], [10, 1], [30, 3]]).\n   lookup(10).on('data', console.log);\n// world\n// 1", 
            "title": "ds.lookup(k)"
        }, 
        {
            "location": "/skale-API/#dsmapmapperobj", 
            "text": "Applies the provided mapper function to each element of the source\ndataset and returns a new dataset.   mapper : a function of the form  callback(element[,obj[,wc]]) ,\n  returning an element and where:  element : the next element of the dataset on which  map()  operates  obj : the same parameter  obj  passed to  map()  wc : the worker context, a persistent object local to each\n  worker, where user can store and access worker local dependencies.    obj : user provided data. Data will be passed to carrying\n  serializable data from master to workers, obj is shared amongst\n  mapper executions over each element of the dataset   The following example program  sc.parallelize([1, 2, 3, 4]).\n   map((data, obj) =  data * obj.scaling, {scaling: 1.2}).\n   collect().toArray().then(console.log);\n// [ 1.2, 2.4, 3.6, 4.8 ]", 
            "title": "ds.map(mapper[,obj])"
        }, 
        {
            "location": "/skale-API/#dsmapvaluesmapperobj", 
            "text": "mapper : a function of the form  callback(element[,obj[,wc]]) ,\n  returning an element and where:  element : the value v of the next [k,v] element of the dataset on\n  which  mapValues()  operates  obj : the same parameter  obj  passed to  mapValues()  wc : the worker context, a persistent object local to each\n  worker, where user can store and access worker local dependencies    obj : user provided data. Data will be passed to carrying\n  serializable data from master to workers, obj is shared amongst\n  mapper executions over each element of the dataset   Applies the provided mapper function to the value of each  [k,v] \nelement of the source dataset and return a new dataset containing elements\ndefined as  [k, mapper(v)] , keeping the key unchanged for each\nsource element.  Example:  sc.parallelize([['hello', 1], ['world', 2]]).\n   mapValues((a, obj) =  a*obj.fact, {fact: 2}).\n   collect().on('data', console.log);\n// ['hello', 2]\n// ['world', 4]", 
            "title": "ds.mapValues(mapper[,obj])"
        }, 
        {
            "location": "/skale-API/#dspartitionbypartitioner", 
            "text": "Returns a dataset partitioned using the specified partitioner. The\npurpose of this transformation is not to change the dataset content,\nbut to increase processing speed by ensuring that the elements\naccessed by further transfomations reside in the same partition.  Example:  var skale = require('skale-engine');\nvar sc = skale.context();\n\nsc.parallelize([['hello', 1], ['world', 1], ['hello', 2], ['world', 2], ['cedric', 3]])\n  .partitionBy(new skale.HashPartitioner(3))\n  .collect.on('data', console.log)\n// ['world', 1], ['world', 2], ['hello', 1], ['hello', 2], ['cedric', 3]", 
            "title": "ds.partitionBy(partitioner)"
        }, 
        {
            "location": "/skale-API/#dspersist", 
            "text": "Returns the dataset, and persists the dataset content on disk (and\nin memory if available) in order to directly reuse content in further\ntasks.  Example:  var dataset = sc.range(100).map(a =  a * a);\n\n// First action: compute dataset\ndataset.collect().on('data', console.log)\n\n// Second action: reuse dataset, avoid map transform\ndataset.collect().on('data', console.log)", 
            "title": "ds.persist()"
        }, 
        {
            "location": "/skale-API/#dsreducereducer-initobj", 
            "text": "Returns a  readable stream  of the aggregated value of the elements\nof the dataset using a  reducer()  function.   reducer : a function of the form  function(acc,val[,obj[,wc]]) ,\n  which returns the next value of the accumulator (which must be\n  of the same type as  acc  and  val ) and with:  acc : the value of the accumulator, initially set to  init  val : the value of the next element of the dataset on which\n    reduce()  operates  obj : the same parameter  obj  passed to  reduce()  wc : the worker context, a persistent object local to each\n   worker, where user can store and access worker local dependencies.    init : the initial value of the accumulators that are used by\n   reducer() . It should be the identity element of the operation\n  (i.e. applying it through the function should not change result).  obj : user provided data. Data will be passed to carrying\n  serializable data from master to workers, obj is shared amongst\n  mapper executions over each element of the dataset   Example:  sc.parallelize([1, 2, 4, 8]).\n   reduce((a, b) =  a + b, 0).\n   on('data', console.log);\n// 15", 
            "title": "ds.reduce(reducer, init[,obj])"
        }, 
        {
            "location": "/skale-API/#dsreducebykeyreducer-init-obj", 
            "text": "reducer : a function of the form  callback(acc,val[,obj[,wc]]) ,\n  returning the next value of the accumulator (which must be of the\n  same type as  acc  and  val ) and where:  acc : the value of the accumulator, initially set to  init  val : the value  v  of the next  [k,v]  element of the dataset on\n  which  reduceByKey()  operates  obj : the same parameter  obj  passed to  reduceByKey()  wc : the worker context, a persistent object local to each\n  worker, where user can store and access worker local dependencies.    init : the initial value of accumulator for each key. Will be\n  passed to  reducer .  obj : user provided data. Data will be passed to carrying\n  serializable data from master to workers, obj is shared amongst\n  mapper executions over each element of the dataset   When called on a dataset of type  [k,v] , returns a dataset of type  [k,v] \nwhere the values of each key are aggregated using the  reducer \nfunction and the  init  initial value.  Example:  sc.parallelize([[10, 1], [10, 2], [10, 4]]).\n   reduceByKey((a,b) =  a+b, 0).\n   collect().on('data', console.log);\n// [10, 7]", 
            "title": "ds.reduceByKey(reducer, init[, obj])"
        }, 
        {
            "location": "/skale-API/#dsrightouterjoinother", 
            "text": "When called on source dataset of type  [k,v]  and  other  dataset of type [k,w] , returns a dataset of type  [k, [v, w]]  pairs where the key\nmust be present in the  source  dataset.  Example:  var ds1 = sc.parallelize([[10, 1], [20, 2]]);\nvar ds2 = sc.parallelize([[10, 'world'], [30, 3]]);\nds1.rightOuterJoin(ds2).collect().on('data', console.log);\n// [ 10, [ 1, 'world' ] ]\n// [ 30, [ null, 2 ] ]", 
            "title": "ds.rightOuterJoin(other)"
        }, 
        {
            "location": "/skale-API/#dssamplewithreplacement-frac-seed", 
            "text": "withReplacement :  Boolean  value,  true  if data must be sampled\n  with replacement  frac :  Number  value of the fraction of source dataset to return  seed :  Number  value of pseudo-random seed   Returns a dataset by sampling a fraction  frac  of source dataset, with or\nwithout replacement, using a given random generator  seed .  Example:  sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8]).\n   sample(true, 0.5, 0).\n   collect().toArray().then(console.log);\n// [ 1, 1, 3, 4, 4, 5, 7 ]", 
            "title": "ds.sample(withReplacement, frac, seed)"
        }, 
        {
            "location": "/skale-API/#dssortbykeyfunc-ascending", 
            "text": "Returns a dataset sorted by the given  keyfunc .   keyfunc : a function of the form  function(element)  which returns\n  a value used for comparison in the sort function and where  element \n  is the next element of the dataset on which  sortBy()  operates  ascending : a boolean to set the sort direction. Default: true   Example:  sc.parallelize([4, 6, 10, 5, 1, 2, 9, 7, 3, 0])\n  .sortBy(a =  a)\n  .collect().toArray().then(console.log)\n// [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]", 
            "title": "ds.sortBy(keyfunc[, ascending])"
        }, 
        {
            "location": "/skale-API/#dssortbykeyascending", 
            "text": "When called on a dataset of type  [k,v] , returns a dataset of type  [k,v] \nsorted on  k . The optional parameter  ascending  is a boolean which sets\nthe sort direction, true by default.  Example:  sc.parallelize([['world', 2], ['cedric', 3], ['hello', 1]])\n  .sortByKey()\n  .collect().toArray().then(console.log)\n// [['cedric', 3], ['hello', 1], ['world', 2]]", 
            "title": "ds.sortByKey(ascending)"
        }, 
        {
            "location": "/skale-API/#dssubtractother", 
            "text": "Returns a dataset containing only elements of source dataset which are not\nin  other  dataset.  Example:  var ds1 = sc.parallelize([1, 2, 3, 4, 5]);\nvar ds2 = sc.parallelize([3, 4, 5, 6, 7]);\nds1.subtract(ds2).collect().on('data', console.log);\n// 1 2", 
            "title": "ds.subtract(other)"
        }, 
        {
            "location": "/skale-API/#dstakenum", 
            "text": "Returns a  readable stream  of the  num  first elements of the source\ndataset.  Example:  sc.parallelize([1, 2, 3, 4]).\n   take(2).\n   toArray().then(console.log)\n// [1, 2]", 
            "title": "ds.take(num)"
        }, 
        {
            "location": "/skale-API/#dstopnum", 
            "text": "Returns a  readable stream  of the  num  top elements of the source\ndataset.  Example:  sc.parallelize([1, 2, 3, 4]).\n   top(2).\n   toArray().then(console.log)\n// [3, 4]", 
            "title": "ds.top(num)"
        }, 
        {
            "location": "/skale-API/#dsunionother", 
            "text": "Returns a dataset that contains the union of the elements in the source\ndataset and the  other  dataset.  Example:  var ds1 = sc.parallelize([1, 2, 3, 4, 5]);\nvar ds2 = sc.parallelize([3, 4, 5, 6, 7]);\nds1.union(ds2).collect().toArray().then(console.log);\n// [ 1, 2, 3, 4, 5, 3, 4, 5, 6, 7 ]", 
            "title": "ds.union(other)"
        }, 
        {
            "location": "/skale-API/#dsvalues", 
            "text": "When called on source dataset of type  [k,v] , returns a dataset with just\nthe elements  v .  Example:  sc.parallelize([[10, 'world'], [30, 3]]).\n   keys.collect().on('data', console.log);\n// 'world'\n// 3", 
            "title": "ds.values()"
        }, 
        {
            "location": "/skale-API/#partitioners", 
            "text": "A partitioner is an object passed to ds.partitionBy(partitioner)  which\nplaces data in partitions according to a strategy, for example hash\npartitioning, where data having the same key are placed in the same\npartition, or range partitioning, where data in the same range are\nin the same partition. This is useful to accelerate processing, as\nit limits data transfers between workers during jobs.  A partition object must provide the following properties:   numPartitions : a  Number  of partitions for the dataset  getPartitionIndex : a  Function  of type  function(element) \n  which returns the partition index (comprised between 0 and\n   numPartitions ) for the  element  of the dataset on which\n   partitionBy()  operates.", 
            "title": "Partitioners"
        }, 
        {
            "location": "/skale-API/#hashpartitionernumpartitions", 
            "text": "Returns a partitioner object which implements hash based partitioning\nusing a hash checksum of each element as a string.   numPartitions :  Number  of partitions for this dataset   Example:  var hp = new skale.HashPartitioner(3)\nvar dataset = sc.range(10).partitionBy(hp)", 
            "title": "HashPartitioner(numPartitions)"
        }, 
        {
            "location": "/skale-API/#rangepartitionernumpartitions-keyfunc-dataset", 
            "text": "Returns a partitioner object which first defines ranges by sampling\nthe dataset and then places elements by comparing them with ranges.   numPartitions :  Number  of partitions for this dataset  keyfunc : a function of the form  function(element)  which returns\n  a value used for comparison in the sort function and where  element \n  is the next element of the dataset on which  partitionBy()  operates  dataset : the dataset object on which  partitionBy()  operates   Example:  var dataset = sc.range(100)\nvar rp = new skale.RangePartitioner(3, a =  a, dataset)\nvar dataset = sc.range(10).partitionBy(rp)", 
            "title": "RangePartitioner(numPartitions, keyfunc, dataset)"
        }
    ]
}