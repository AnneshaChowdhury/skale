{
    "docs": [
        {
            "location": "/", 
            "text": "High performance distributed data processing and machine learning.\n\n\nSkale provides a high-level API in Javascript and an optimized\nparallel execution engine on top of NodeJS.\n\n\nFeatures\n\n\n\n\nPure javascript implementation of a Spark like engine\n\n\nMultiple data sources: filesystems, databases, cloud (S3, azure)\n\n\nMultiple data formats: CSV, JSON, Columnar (Parquet)...\n\n\n50 high level operators to build parallel apps\n\n\nMachine learning: scalable classification, regression, clusterization\n\n\nRun interactively in a nodeJS REPL shell\n\n\nDocker \nready\n, simple local mode or full distributed mode\n\n\nVery fast, see \nbenchmark\n\n\n\n\nQuickstart\n\n\nnpm install skale\n\n\n\n\n\nWord count example: \n\n\nvar\n \nsc\n \n=\n \nrequire\n(\nskale\n).\ncontext\n();\n\n\n\nsc\n.\ntextFile\n(\n/my/path/*.txt\n)\n\n  \n.\nflatMap\n(\nline\n \n=\n \nline\n.\nsplit\n(\n \n))\n\n  \n.\nmap\n(\nword\n \n=\n \n[\nword\n,\n \n1\n])\n\n  \n.\nreduceByKey\n((\na\n,\n \nb\n)\n \n=\n \na\n \n+\n \nb\n,\n \n0\n)\n\n  \n.\ncount\n(\nfunction\n \n(\nerr\n,\n \nresult\n)\n \n{\n\n    \nconsole\n.\nlog\n(\nresult\n);\n\n    \nsc\n.\nend\n();\n\n  \n});\n\n\n\n\n\n\nLocal mode\n\n\nIn local mode, worker processes are automatically forked and\ncommunicate with app through child process IPC channel. This is\nthe simplest way to operate, and it allows to use all machine\navailable cores.\n\n\nTo run in local mode, just execute your app script:\n\n\nnode my_app.js\n\n\n\n\n\nor with debug traces:\n\n\nSKALE_DEBUG\n=\n2\n node my_app.js\n\n\n\n\n\nDistributed mode\n\n\nIn distributed mode, a cluster server process and worker processes\nmust be started prior to start app. Processes communicate with each\nother via raw TCP or via websockets.\n\n\nTo run in distributed cluster mode, first start a cluster server\non \nserver_host\n:\n\n\n./bin/server.js\n\n\n\n\n\nOn each worker host, start a worker controller process which connects\nto server:\n\n\n./bin/worker.js -H server_host\n\n\n\n\n\nThen run your app, setting the cluster server host in environment:\n\n\nSKALE_HOST\n=\nserver_host node my_app.js\n\n\n\n\n\nThe same with debug traces:\n\n\nSKALE_HOST\n=\nserver_host \nSKALE_DEBUG\n=\n2\n node my_app.js\n\n\n\n\n\nResources\n\n\n\n\nContributing guide\n\n\nGitter\n for support and\n  discussion\n\n\nMailing list\n\n  for discussion about use and development\n\n\n\n\nAuthors\n\n\nThe original authors of skale are \nCedric Artigue\n and \nMarc Vertes\n.\n\n\nList of all\ncontributors\n\n\nLicense\n\n\nApache-2.0\n\n\nCredits\n\n\nLogo Icon made by \nSmashicons\n from \nwww.flaticon.com\n is licensed by \nCC 3.0 BY", 
            "title": "About Skale"
        }, 
        {
            "location": "/#features", 
            "text": "Pure javascript implementation of a Spark like engine  Multiple data sources: filesystems, databases, cloud (S3, azure)  Multiple data formats: CSV, JSON, Columnar (Parquet)...  50 high level operators to build parallel apps  Machine learning: scalable classification, regression, clusterization  Run interactively in a nodeJS REPL shell  Docker  ready , simple local mode or full distributed mode  Very fast, see  benchmark", 
            "title": "Features"
        }, 
        {
            "location": "/#quickstart", 
            "text": "npm install skale  Word count example:   var   sc   =   require ( skale ). context ();  sc . textFile ( /my/path/*.txt ) \n   . flatMap ( line   =   line . split (   )) \n   . map ( word   =   [ word ,   1 ]) \n   . reduceByKey (( a ,   b )   =   a   +   b ,   0 ) \n   . count ( function   ( err ,   result )   { \n     console . log ( result ); \n     sc . end (); \n   });", 
            "title": "Quickstart"
        }, 
        {
            "location": "/#local-mode", 
            "text": "In local mode, worker processes are automatically forked and\ncommunicate with app through child process IPC channel. This is\nthe simplest way to operate, and it allows to use all machine\navailable cores.  To run in local mode, just execute your app script:  node my_app.js  or with debug traces:  SKALE_DEBUG = 2  node my_app.js", 
            "title": "Local mode"
        }, 
        {
            "location": "/#distributed-mode", 
            "text": "In distributed mode, a cluster server process and worker processes\nmust be started prior to start app. Processes communicate with each\nother via raw TCP or via websockets.  To run in distributed cluster mode, first start a cluster server\non  server_host :  ./bin/server.js  On each worker host, start a worker controller process which connects\nto server:  ./bin/worker.js -H server_host  Then run your app, setting the cluster server host in environment:  SKALE_HOST = server_host node my_app.js  The same with debug traces:  SKALE_HOST = server_host  SKALE_DEBUG = 2  node my_app.js", 
            "title": "Distributed mode"
        }, 
        {
            "location": "/#resources", 
            "text": "Contributing guide  Gitter  for support and\n  discussion  Mailing list \n  for discussion about use and development", 
            "title": "Resources"
        }, 
        {
            "location": "/#authors", 
            "text": "The original authors of skale are  Cedric Artigue  and  Marc Vertes .  List of all\ncontributors", 
            "title": "Authors"
        }, 
        {
            "location": "/#license", 
            "text": "Apache-2.0", 
            "title": "License"
        }, 
        {
            "location": "/#credits", 
            "text": "Logo Icon made by  Smashicons  from  www.flaticon.com  is licensed by  CC 3.0 BY", 
            "title": "Credits"
        }, 
        {
            "location": "/concepts/", 
            "text": "Introduction\n\n\nSkale is a fast and general purpose distributed data processing\nsystem. It provides a high-level API in Javascript and an optimized\nparallel execution engine.\n\n\nA Skale application consists of a \nmaster\n program that runs the\nuser code and executes various \nparallel operations\n on a cluster\nof \nworkers\n.\n\n\nThe main abstraction Skale provides is a \ndataset\n which is similar\nto a Javascript \narray\n, but partitioned accross the workers that\ncan be operated in parallel.\n\n\nThere are several ways to create a dataset: \nparallelizing\n an existing\narray in the master program, or referencing a dataset in a distributed\nstorage system (such as HDFS), or \nstreaming\n the content of any\nsource that can be processed through Node.js \nStreams\n. We call\n\nsource\n a function which initializes a dataset.\n\n\nDatasets support two kinds of operations: \ntransformations\n, which create\na new dataset from an existing one, and \nactions\n, which\nreturn a value to the \nmaster\n program after running a computation\non the dataset.\n\n\nFor example, \nmap\n is a transformation that applies a function to\neach element of a dataset, returning a new dataset. On the other\nhand, \nreduce\n is an action that aggregates all elements of a dataset\nusing some function, and returns the final result to the master.\n\n\nSources\n and \ntransformations\n in Skale are \nlazy\n. They do not\nstart right away, but are triggered by \nactions\n, thus allowing\nefficient pipelined execution and optimized data transfers.\n\n\nA first example:\n\n\nvar\n \nsc\n \n=\n \nrequire\n(\nskale\n).\ncontext\n();\n        \n// create a new context\n\n\nsc\n.\nparallelize\n([\n1\n,\n \n2\n,\n \n3\n,\n \n4\n]).\n               \n// source\n\n   \nmap\n(\nfunction\n \n(\nx\n)\n \n{\nreturn\n \nx\n+\n1\n}).\n          \n// transform\n\n   \nreduce\n(\nfunction\n \n(\na\n,\n \nb\n)\n \n{\nreturn\n \na\n+\nb\n},\n \n0\n).\n \n// action\n\n   \nthen\n(\nconsole\n.\nlog\n);\n                       \n// process result: 14\n\n\n\n\n\n\nCore concepts\n\n\nAs stated above, a program can be considered as a workflow of steps,\neach step consisting of a transformation which inputs from one or\nmore datasets (parents), and outputs to a new dataset (child).\n\n\nPartitioning\n\n\nDatasets are divided into several partitions, so each partition can\nbe assigned to a separate worker, and processing can occur concurently\nin a distributed and parallel system.\n\n\nThe consequence of this partitioning is that two types of transformations\nexist:\n\n\n\n\n\n\nNarrow\n transformations, where each partition of the parent dataset\n  is used by at most one partition of the child dataset. This is the\n  case for example for \nmap()\n or \nfilter()\n, where each dataset entry\n  is processed independently from each other.\n  Partitions are decoupled, no synchronization\n  between workers is required, and narrow transformations can be\n  pipelined on each worker.\n\n\n\n\n\n\nWide\n transformations, where multiple child partitions may depend\n  on one parent partition. This is the case for example for \nsortBy()\n\n  or \ngroupByKey()\n. Data need to be exchanged between workers or\n  \nshuffled\n, in order to complete the transformation. This introduces\n  synchronization points which prevent pipelining.\n\n\n\n\n\n\nPipeline stages and shuffles\n\n\nInternally, each wide transformation consists of a pre-shuffle and\na post-shuffle part. All sequences of steps from source to pre-shuffle,\nor from post-shuffle to next pre-shuffle or action, are thus only\nnarrow transformations, or pipelined stages (the most efficient\npattern).  A skale program is therefore simply a sequence of stages\nand shuffles, shuffles being global serialization points.\n\n\nIt's important to grab this concept as it sets the limit to the\nlevel of parallelism which can be achieved by a given code.\n\n\nThe synoptic table of \ntransformations\n indicates\nfor each transformation if it is narrow or wide (shuffle).\n\n\nWorking with datasets\n\n\nSources\n\n\nAfter having initialized a cluster context using \nskale.context()\n,\none can create a dataset using the following sources:\n\n\n\n\n\n\n\n\nSource Name\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nlineStream(stream)\n\n\nCreate a dataset from a text stream\n\n\n\n\n\n\nobjectStream(stream)\n\n\nCreate a dataset from an object stream\n\n\n\n\n\n\nparallelize(array)\n\n\nCreate a dataset from an array\n\n\n\n\n\n\nrange(start,end,step)\n\n\nCreate a dataset containing integers from start to end\n\n\n\n\n\n\nsource(size,callback,args)\n\n\nCreate a dataset from a custom source function\n\n\n\n\n\n\ntextFile(path, options)\n\n\nCreate a dataset from text file\n\n\n\n\n\n\n\n\nTransformations\n\n\nTransformations operate on a dataset and return a new dataset. Note that some\ntransformation operate only on datasets where each element is in the form\nof 2 elements array of key and value (\n[k,v]\n dataset):\n\n\n[[Ki,Vi], ..., [Kj, Vj]]\n\n\n\n\n\nA special transformation \npersist()\n enables one to \npersist\n a dataset\nin memory, allowing efficient reuse accross parallel operations.\n\n\n\n\n\n\n\n\nTransformation Name\n\n\nDescription\n\n\nIn\n\n\nOut\n\n\nShuffle\n\n\n\n\n\n\n\n\n\n\naggregateByKey(func, func, init)\n\n\nReduce and combine by key using functions\n\n\n[k,v]\n\n\n[k,v]\n\n\nyes\n\n\n\n\n\n\ncartesian(other)\n\n\nPerform a cartesian product with the other dataset\n\n\nv w\n\n\n[v,w]\n\n\nyes\n\n\n\n\n\n\ncoGroup(other)\n\n\nGroup data from both datasets sharing the same key\n\n\n[k,v] [k,w]\n\n\n[k,[[v],[w]]]\n\n\nyes\n\n\n\n\n\n\ndistinct()\n\n\nReturn a dataset where duplicates are removed\n\n\nv\n\n\nw\n\n\nyes\n\n\n\n\n\n\nfilter(func)\n\n\nReturn a dataset of elements on which function returns true\n\n\nv\n\n\nw\n\n\nno\n\n\n\n\n\n\nflatMap(func)\n\n\nPass the dataset elements to a function which returns a sequence\n\n\nv\n\n\nw\n\n\nno\n\n\n\n\n\n\nflatMapValues(func)\n\n\nPass the dataset [k,v] elements to a function without changing the keys\n\n\n[k,v]\n\n\n[k,w]\n\n\nno\n\n\n\n\n\n\ngroupByKey()\n\n\nGroup values with the same key\n\n\n[k,v]\n\n\n[k,[v]]\n\n\nyes\n\n\n\n\n\n\nintersection(other)\n\n\nReturn a dataset containing only elements found in both datasets\n\n\nv w\n\n\nv\n\n\nyes\n\n\n\n\n\n\njoin(other)\n\n\nPerform an inner join between 2 datasets\n\n\n[k,v]\n\n\n[k,[v,w]]\n\n\nyes\n\n\n\n\n\n\nleftOuterJoin(other)\n\n\nJoin 2 datasets where the key must be present in the other\n\n\n[k,v]\n\n\n[k,[v,w]]\n\n\nyes\n\n\n\n\n\n\nrightOuterJoin(other)\n\n\nJoin 2 datasets where the key must be present in the first\n\n\n[k,v]\n\n\n[k,[v,w]]\n\n\nyes\n\n\n\n\n\n\nkeys()\n\n\nReturn a dataset of just the keys\n\n\n[k,v]\n\n\nk\n\n\nno\n\n\n\n\n\n\nmap(func)\n\n\nReturn a dataset where elements are passed through a function\n\n\nv\n\n\nw\n\n\nno\n\n\n\n\n\n\nmapValues(func)\n\n\nMap a function to the value field of key-value dataset\n\n\n[k,v]\n\n\n[k,w]\n\n\nno\n\n\n\n\n\n\nreduceByKey(func, init)\n\n\nCombine values with the same key\n\n\n[k,v]\n\n\n[k,w]\n\n\nyes\n\n\n\n\n\n\npartitionBy(partitioner)\n\n\nPartition using the partitioner\n\n\nv\n\n\nv\n\n\nyes\n\n\n\n\n\n\npersist()\n\n\nIdempotent, keep content of dataset in cache for further reuse\n\n\nv\n\n\nv\n\n\nno\n\n\n\n\n\n\nsample(rep, frac)\n\n\nSample a dataset, with or without replacement\n\n\nv\n\n\nw\n\n\nno\n\n\n\n\n\n\nsortBy(func)\n\n\nSort a dataset\n\n\nv\n\n\nv\n\n\nyes\n\n\n\n\n\n\nsortByKey()\n\n\nSort a [k,v] dataset\n\n\n[k,v]\n\n\n[k,v]\n\n\nyes\n\n\n\n\n\n\nsubtract(other)\n\n\nRemove the content of one dataset\n\n\nv w\n\n\nv\n\n\nyes\n\n\n\n\n\n\nunion(other)\n\n\nReturn a dataset containing elements from both datasets\n\n\nv\n\n\nv w\n\n\nno\n\n\n\n\n\n\nvalues()\n\n\nReturn a dataset of just the values\n\n\n[k,v]\n\n\nv\n\n\nno\n\n\n\n\n\n\n\n\nActions\n\n\nActions operate on a dataset and send back results to the \nmaster\n. Results\nare always produced asynchronously and send to an optional callback function,\nalternatively through a returned \nES6 promise\n.\n\n\n\n\n\n\n\n\nAction Name\n\n\nDescription\n\n\nout\n\n\n\n\n\n\n\n\n\n\naggregate(func, func, init)\n\n\nSimilar to reduce() but may return a different typei\n\n\nvalue\n\n\n\n\n\n\ncollect()\n\n\nReturn the content of dataset\n\n\narray of elements\n\n\n\n\n\n\ncount()\n\n\nReturn the number of elements from dataset\n\n\nnumber\n\n\n\n\n\n\ncountByKey()\n\n\nReturn the number of occurrences for each key in a \n[k,v]\n dataset\n\n\narray of [k,number]\n\n\n\n\n\n\ncountByValue()\n\n\nReturn the number of occurrences of elements from dataset\n\n\narray of [v,number]\n\n\n\n\n\n\nfirst()\n\n\nReturn the first element in dataset i\n\n\nvalue\n\n\n\n\n\n\nforEach(func)\n\n\nApply the provided function to each element of the dataset\n\n\nempty\n\n\n\n\n\n\nlookup(k)\n\n\nReturn the list of values \nv\n for key \nk\n in a \n[k,v]\n dataset\n\n\narray of v\n\n\n\n\n\n\nreduce(func, init)\n\n\nAggregates dataset elements using a function into one value\n\n\nvalue\n\n\n\n\n\n\nsave(url)\n\n\nSave the content of a dataset to an url\n\n\nempty\n\n\n\n\n\n\nstream()\n\n\nStream out a dataset\n\n\nstream\n\n\n\n\n\n\ntake(num)\n\n\nReturn the first \nnum\n elements of dataset\n\n\narray of value\n\n\n\n\n\n\ntakeSample(withReplacement, num)\n\n\nReturn a sample of \nnum\n elements of dataset\n\n\narray of value\n\n\n\n\n\n\ntop(num)\n\n\nReturn the top \nnum\n elements of dataset\n\n\narray of value", 
            "title": "Concepts"
        }, 
        {
            "location": "/concepts/#introduction", 
            "text": "Skale is a fast and general purpose distributed data processing\nsystem. It provides a high-level API in Javascript and an optimized\nparallel execution engine.  A Skale application consists of a  master  program that runs the\nuser code and executes various  parallel operations  on a cluster\nof  workers .  The main abstraction Skale provides is a  dataset  which is similar\nto a Javascript  array , but partitioned accross the workers that\ncan be operated in parallel.  There are several ways to create a dataset:  parallelizing  an existing\narray in the master program, or referencing a dataset in a distributed\nstorage system (such as HDFS), or  streaming  the content of any\nsource that can be processed through Node.js  Streams . We call source  a function which initializes a dataset.  Datasets support two kinds of operations:  transformations , which create\na new dataset from an existing one, and  actions , which\nreturn a value to the  master  program after running a computation\non the dataset.  For example,  map  is a transformation that applies a function to\neach element of a dataset, returning a new dataset. On the other\nhand,  reduce  is an action that aggregates all elements of a dataset\nusing some function, and returns the final result to the master.  Sources  and  transformations  in Skale are  lazy . They do not\nstart right away, but are triggered by  actions , thus allowing\nefficient pipelined execution and optimized data transfers.  A first example:  var   sc   =   require ( skale ). context ();          // create a new context  sc . parallelize ([ 1 ,   2 ,   3 ,   4 ]).                 // source \n    map ( function   ( x )   { return   x + 1 }).            // transform \n    reduce ( function   ( a ,   b )   { return   a + b },   0 ).   // action \n    then ( console . log );                         // process result: 14", 
            "title": "Introduction"
        }, 
        {
            "location": "/concepts/#core-concepts", 
            "text": "As stated above, a program can be considered as a workflow of steps,\neach step consisting of a transformation which inputs from one or\nmore datasets (parents), and outputs to a new dataset (child).", 
            "title": "Core concepts"
        }, 
        {
            "location": "/concepts/#partitioning", 
            "text": "Datasets are divided into several partitions, so each partition can\nbe assigned to a separate worker, and processing can occur concurently\nin a distributed and parallel system.  The consequence of this partitioning is that two types of transformations\nexist:    Narrow  transformations, where each partition of the parent dataset\n  is used by at most one partition of the child dataset. This is the\n  case for example for  map()  or  filter() , where each dataset entry\n  is processed independently from each other.\n  Partitions are decoupled, no synchronization\n  between workers is required, and narrow transformations can be\n  pipelined on each worker.    Wide  transformations, where multiple child partitions may depend\n  on one parent partition. This is the case for example for  sortBy() \n  or  groupByKey() . Data need to be exchanged between workers or\n   shuffled , in order to complete the transformation. This introduces\n  synchronization points which prevent pipelining.", 
            "title": "Partitioning"
        }, 
        {
            "location": "/concepts/#pipeline-stages-and-shuffles", 
            "text": "Internally, each wide transformation consists of a pre-shuffle and\na post-shuffle part. All sequences of steps from source to pre-shuffle,\nor from post-shuffle to next pre-shuffle or action, are thus only\nnarrow transformations, or pipelined stages (the most efficient\npattern).  A skale program is therefore simply a sequence of stages\nand shuffles, shuffles being global serialization points.  It's important to grab this concept as it sets the limit to the\nlevel of parallelism which can be achieved by a given code.  The synoptic table of  transformations  indicates\nfor each transformation if it is narrow or wide (shuffle).", 
            "title": "Pipeline stages and shuffles"
        }, 
        {
            "location": "/concepts/#working-with-datasets", 
            "text": "", 
            "title": "Working with datasets"
        }, 
        {
            "location": "/concepts/#sources", 
            "text": "After having initialized a cluster context using  skale.context() ,\none can create a dataset using the following sources:     Source Name  Description      lineStream(stream)  Create a dataset from a text stream    objectStream(stream)  Create a dataset from an object stream    parallelize(array)  Create a dataset from an array    range(start,end,step)  Create a dataset containing integers from start to end    source(size,callback,args)  Create a dataset from a custom source function    textFile(path, options)  Create a dataset from text file", 
            "title": "Sources"
        }, 
        {
            "location": "/concepts/#transformations", 
            "text": "Transformations operate on a dataset and return a new dataset. Note that some\ntransformation operate only on datasets where each element is in the form\nof 2 elements array of key and value ( [k,v]  dataset):  [[Ki,Vi], ..., [Kj, Vj]]  A special transformation  persist()  enables one to  persist  a dataset\nin memory, allowing efficient reuse accross parallel operations.     Transformation Name  Description  In  Out  Shuffle      aggregateByKey(func, func, init)  Reduce and combine by key using functions  [k,v]  [k,v]  yes    cartesian(other)  Perform a cartesian product with the other dataset  v w  [v,w]  yes    coGroup(other)  Group data from both datasets sharing the same key  [k,v] [k,w]  [k,[[v],[w]]]  yes    distinct()  Return a dataset where duplicates are removed  v  w  yes    filter(func)  Return a dataset of elements on which function returns true  v  w  no    flatMap(func)  Pass the dataset elements to a function which returns a sequence  v  w  no    flatMapValues(func)  Pass the dataset [k,v] elements to a function without changing the keys  [k,v]  [k,w]  no    groupByKey()  Group values with the same key  [k,v]  [k,[v]]  yes    intersection(other)  Return a dataset containing only elements found in both datasets  v w  v  yes    join(other)  Perform an inner join between 2 datasets  [k,v]  [k,[v,w]]  yes    leftOuterJoin(other)  Join 2 datasets where the key must be present in the other  [k,v]  [k,[v,w]]  yes    rightOuterJoin(other)  Join 2 datasets where the key must be present in the first  [k,v]  [k,[v,w]]  yes    keys()  Return a dataset of just the keys  [k,v]  k  no    map(func)  Return a dataset where elements are passed through a function  v  w  no    mapValues(func)  Map a function to the value field of key-value dataset  [k,v]  [k,w]  no    reduceByKey(func, init)  Combine values with the same key  [k,v]  [k,w]  yes    partitionBy(partitioner)  Partition using the partitioner  v  v  yes    persist()  Idempotent, keep content of dataset in cache for further reuse  v  v  no    sample(rep, frac)  Sample a dataset, with or without replacement  v  w  no    sortBy(func)  Sort a dataset  v  v  yes    sortByKey()  Sort a [k,v] dataset  [k,v]  [k,v]  yes    subtract(other)  Remove the content of one dataset  v w  v  yes    union(other)  Return a dataset containing elements from both datasets  v  v w  no    values()  Return a dataset of just the values  [k,v]  v  no", 
            "title": "Transformations"
        }, 
        {
            "location": "/concepts/#actions", 
            "text": "Actions operate on a dataset and send back results to the  master . Results\nare always produced asynchronously and send to an optional callback function,\nalternatively through a returned  ES6 promise .     Action Name  Description  out      aggregate(func, func, init)  Similar to reduce() but may return a different typei  value    collect()  Return the content of dataset  array of elements    count()  Return the number of elements from dataset  number    countByKey()  Return the number of occurrences for each key in a  [k,v]  dataset  array of [k,number]    countByValue()  Return the number of occurrences of elements from dataset  array of [v,number]    first()  Return the first element in dataset i  value    forEach(func)  Apply the provided function to each element of the dataset  empty    lookup(k)  Return the list of values  v  for key  k  in a  [k,v]  dataset  array of v    reduce(func, init)  Aggregates dataset elements using a function into one value  value    save(url)  Save the content of a dataset to an url  empty    stream()  Stream out a dataset  stream    take(num)  Return the first  num  elements of dataset  array of value    takeSample(withReplacement, num)  Return a sample of  num  elements of dataset  array of value    top(num)  Return the top  num  elements of dataset  array of value", 
            "title": "Actions"
        }, 
        {
            "location": "/skale-API/", 
            "text": "Skale API\n\n\nThe Skale module is the main entry point for Skale functionality.\nTo use it, one must \nrequire('skale')\n.\n\n\nskale.context([config])\n\n\nCreates and returns a new context which represents the connection\nto the Skale cluster, and which can be used to create datasets on that\ncluster. Config is an \nObject\n which defines the cluster server,\nwith the following defaults:\n\n\n{\n\n  \nhost\n:\n \nlocalhost\n,\n  \n// Cluster server host, settable also by SKALE_HOST env\n\n  \nport\n:\n \n12346\n       \n// Cluster server port, settable also by SKALE_PORT env\n\n\n}\n\n\n\n\n\n\nExample:\n\n\nvar\n \nskale\n \n=\n \nrequire\n(\nskale\n);\n\n\nvar\n \nsc\n \n=\n \nskale\n.\ncontext\n();\n\n\n\n\n\n\nsc.env\n\n\nThe \nsc.env\n property returns an object containing user environment variables\nto be set in workers.\n\n\nTo set and propagate an environment variable to all workers, assign \nsc.env\n object\nprior to invoking an action.\n\n\nExample:\n\n\nsc\n.\nenv\n.\nMY_VAR\n \n=\n \nmy_value\n;\n\n\n\n\n\n\nsc.end()\n\n\nCloses the connection to the cluster.\n\n\nsc.lineStream(input_stream)\n\n\nReturns a new dataset of lines of text read from input_stream\n\nObject\n, which is a \nreadable stream\n where dataset content is\nread from.\n\n\nThe following example computes the size of a file using streams:\n\n\nvar\n \nstream\n \n=\n \nfs\n.\ncreateReadStream\n(\ndata.txt\n,\n \nutf8\n);\n\n\nsc\n.\nlineStream\n(\nstream\n)\n\n  \n.\nmap\n(\ns\n \n=\n \ns\n.\nlength\n)\n\n  \n.\nreduce\n((\na\n,\n \nb\n)\n \n=\n \na\n \n+\n \nb\n,\n \n0\n)\n\n  \n.\nthen\n(\nconsole\n.\nlog\n);\n\n\n\n\n\n\nsc.objectStream(input_stream)\n\n\nReturns a new dataset of Javascript \nObjects\n read from input_stream\n\nObject\n, which is a \nreadable stream\n where dataset content is\nread from.\n\n\nThe following example counts the number of objects returned in an\nobject stream using the mongodb native Javascript driver:\n\n\nvar\n \ncursor\n \n=\n \ndb\n.\ncollection\n(\nclients\n).\nfind\n();\n\n\nsc\n.\nobjectStream\n(\ncursor\n).\ncount\n().\nthen\n(\nconsole\n.\nlog\n);\n\n\n\n\n\n\nsc.parallelize(array)\n\n\nReturns a new dataset containing elements from the \nArray\n array.\n\n\nExample:\n\n\nvar\n \na\n \n=\n \nsc\n.\nparallelize\n([\nHello\n,\n \nWorld\n]);\n\n\n\n\n\n\nsc.range(start[, end[, step]])\n\n\nReturns a new dataset of integers from \nstart\n to \nend\n (exclusive)\nincreased by \nstep\n (default 1) every element. If called with a\nsingle argument, the argument is interpreted as \nend\n, and \nstart\n\nis set to 0.\n\n\nsc\n.\nrange\n(\n5\n).\ncollect\n().\nthen\n(\nconsole\n.\nlog\n)\n\n\n// [ 0, 1, 2, 3, 4 ]\n\n\nsc\n.\nrange\n(\n2\n,\n \n4\n).\ncollect\n().\nthen\n(\nconsole\n.\nlog\n)\n\n\n// [ 2, 3 ]\n\n\nsc\n.\nrange\n(\n10\n,\n \n-\n5\n,\n \n-\n3\n).\ncollect\n().\nthen\n(\nconsole\n.\nlog\n)\n\n\n// [ 10, 7, 4, 1, -2 ]\n\n\n\n\n\n\nsc.require(modules)\n\n\nSets a list of dependency modules to be deployed in workers for use\nby callbacks, such as mappers or reducers.  Returns the context\nobject.\n\n\n\n\nmodules\n: an \nObject\n of the form \n{name1: 'path1', ...}\n\n  where \nname1\n is the name of the variable to which the module\n  is assigned to, and \npath1\n a path expression as in \nrequire.resolve(path)\n.\n\n\n\n\nUnder the hood, \nbrowserify\n is used on master side to build a\nbundle which is serialized and sent to workers, where\nIt is then evaluated in global context.\n\n\nExample:\n\n\n// deps.js contains:\n\n\n// module.export = function add3(a) {return a + 3;};\n\n\n\nsc\n.\nrequire\n({\nadd3\n:\n \n./deps.js\n})\n\n  \n.\nrange\n(\n4\n)\n\n  \n.\nmap\n(\na\n \n=\n \nadd3\n)\n\n  \n.\ncollect\n()\n\n  \n.\nthen\n(\nconsole\n.\nlog\n);\n\n\n// [ 3, 4, 5, 6 ]\n\n\n\n\n\n\nsc.source(size, callback[, args])\n\n\nReturns a new dataset of \nsize\n elements, where each element is\ngenerated by a custom function \ncallback\n executed on workers.\n\n\n\n\nsize\n: an integer \nNumber\n of elements in the dataset\n\n\ncallback\n: a function of the form \nfunction(index, args[, wc])\n\n  which returns the next element and with:\n\n\nindex\n: the index of the element in the dataset, comprised\n  between \n0\n and \nsize - 1\n\n\nargs\n: the same parameter \nargs\n passed to source\n\n\nwc\n: the worker context, a persistent object local to each\n  worker, where user can store and access worker local dependencies\n\n\n\n\n\n\nargs\n: user custom parameter, passed to \ncallback\n\n\n\n\nfunction\n \nrandArray\n(\nindex\n,\n \nlen\n)\n \n{\n\n  \nvar\n \narr\n \n=\n \n[];\n\n  \nfor\n \n(\nvar\n \ni\n \n=\n \n0\n;\n \ni\n \n \nlen\n;\n \ni\n++\n)\n\n    \narr\n.\npush\n(\nMath\n.\nfloor\n(\nMath\n.\nrandom\n()\n \n*\n \n100\n));\n\n  \nreturn\n \narr\n;\n\n\n}\n\n\n\nsc\n.\nsource\n(\n3\n,\n \nrandArray\n,\n \n2\n).\ncollect\n().\nthen\n(\nconsole\n.\nlog\n);\n\n\n// [ [ 31, 85 ], [ 93, 21 ], [ 99, 58 ] ]\n\n\n\n\n\n\nsc.textFile(path[, options])\n\n\nReturns a new dataset of lines in file specified by path \nString\n.\n\n\n\n\npath\n: a \nString\n of the general form \nprotocol://host/path\n or \n/path\n,\n  where protocol can be one of:\n\n\nfile\n: if path is on local filesystem\n\n\ns3\n: if path relates to a repository on AWS [S3] storage system\n\n\nwasb\n: if path relates to a repository on Azure blob storage system\n\n\noptions\n: an \nObject\n with the following fields:\n\n\nmaxFiles\n: a \nNumber\n of maximum files to process if the path refers\n    to a directory.\n\n\nparquet\n: a \nBoolean\n to indicate that all files are in the\n    [parquet] format. Default value is \nfalse\n.\n\n\n\n\nif \npath\n ends by a '/' (directory separator), then the dataset\nwill be composed of all the files in the directory. Sub-directories\nare not supported. Wildcard characters such as \n*\n, \n?\n, etc, as\nin the Unix Shell globbing patterns are supported.\n\n\nIf a file name ends by '.gz', then its content will be automatically\nuncompressed using GZIP.\n\n\nIf a file name ends by '.parquet', it will automatically be processed\nas a [parquet].\n\n\nNote: If using a path on the local filesystem, the file must also\nbe accessible at the same path on worker nodes. Either copy the\nfile to all workers or use a network-mounted shared file system.\n\n\nFor example, the following program prints the length of a text file:\n\n\nvar\n \nlines\n \n=\n \nsc\n.\ntextFile\n(\ndata.txt\n);\n\n\nlines\n.\nmap\n(\ns\n \n=\n \ns\n.\nlength\n).\nreduce\n((\na\n,\n \nb\n)\n \n=\n \na\n \n+\n \nb\n,\n \n0\n).\nthen\n(\nconsole\n.\nlog\n);\n\n\n\n\n\n\nDataset methods\n\n\nDataset objects, as created initially by above skale context source\nfunctions, have the following methods, allowing either to instantiate\na new dataset through a transformation, or to return results to the\nmaster program.\n\n\nds.aggregate(reducer, combiner, init[, obj][, done])\n\n\nThis \naction\n computes the aggregated value of the elements\nof the dataset using two functions \nreducer()\n and \ncombiner()\n,\nallowing to use an arbitrary accumulator type, different from element\ntype (as opposed to \nreduce()\n which imposes the same type for\naccumulator and element).\nThe result is passed to the \ndone()\n callback if provided, otherwise an\n\nES6 promise\n is returned.\n\n\n\n\nreducer\n: a function of the form \nfunction(acc, val[, obj[, wc]])\n,\n  which returns the next value of the accumulator (which must be\n  of the same type as \nacc\n) and with:\n\n\nacc\n: the value of the accumulator, initially set to \ninit\n\n\nval\n: the value of the next element of the dataset on which\n   \naggregate()\n operates\n\n\nobj\n: the same parameter \nobj\n passed to \naggregate()\n\n\nwc\n: the worker context, a persistent object local to each\n   worker, where user can store and access worker local dependencies.\n\n\n\n\n\n\ncombiner\n: a function of the form \nfunction(acc1, acc2[, obj])\n,\n  which returns the merged value of accumulators and with:\n\n\nacc1\n: the value of an accumulator, computed locally on a worker\n\n\nacc2\n: the value of an other accumulator, issued by another worker\n\n\nobj\n: the same parameter \nobj\n passed to \naggregate()\n\n\n\n\n\n\ninit\n: the initial value of the accumulators that are used by\n  \nreducer()\n and \ncombiner()\n. It should be the identity element\n  of the operation (a neutral zero value, i.e. applying it through the\n  function should not change result).\n\n\nobj\n: user provided data. Data will be passed to carrying\n  serializable data from master to workers, obj is shared amongst\n  mapper executions over each element of the dataset.\n\n\ndone\n: a callback of the form \nfunction(error, result)\n which is\n  called at completion. If \nundefined\n, \naggregate()\n returns an\n  \nES6 promise\n.\n\n\n\n\nThe following example computes the average of a dataset, avoiding a \nmap()\n:\n\n\nsc\n.\nparallelize\n([\n3\n,\n \n5\n,\n \n2\n,\n \n7\n,\n \n4\n,\n \n8\n])\n\n  \n.\naggregate\n((\na\n,\n \nv\n)\n \n=\n \n[\na\n[\n0\n]\n \n+\n \nv\n,\n \na\n[\n1\n]\n \n+\n \n1\n],\n\n    \n(\na1\n,\n \na2\n)\n \n=\n \n[\na1\n[\n0\n]\n \n+\n \na2\n[\n0\n],\n \na1\n[\n1\n]\n \n+\n \na2\n[\n1\n]],\n \n[\n0\n,\n \n0\n])\n\n  \n.\nthen\n(\nfunction\n(\ndata\n)\n \n{\n\n    \nconsole\n.\nlog\n(\ndata\n[\n0\n]\n \n/\n \ndata\n[\n1\n]);\n\n  \n})\n\n\n// 4.8333\n\n\n\n\n\n\nds.aggregateByKey(reducer, combiner, init,[ obj])\n\n\nWhen called on a dataset of type \n[k,v]\n, returns a dataset of type\n\n[k,v]\n where \nv\n is the aggregated value of all elements of same\nkey \nk\n. The aggregation is performed using two functions \nreducer()\n\nand \ncombiner()\n allowing to use an arbitrary accumulator type,\ndifferent from element type.\n\n\n\n\nreducer\n: a function of the form \nfunction(acc, val[, obj[, wc]])\n,\n  which returns the next value of the accumulator (which must be\n  of the same type as \nacc\n) and with:\n\n\nacc\n: the value of the accumulator, initially set to \ninit\n\n\nval\n: the value \nv\n of the next \n[k,v]\n element of the dataset\n   on which \naggregateByKey()\n operates\n\n\nobj\n: the same parameter \nobj\n passed to \naggregateByKey()\n\n\nwc\n: the worker context, a persistent object local to each\n   worker, where user can store and access worker local dependencies.\n\n\n\n\n\n\ncombiner\n: a function of the form \nfunction(acc1, acc2[, obj])\n,\n  which returns the merged value of accumulators and with:\n\n\nacc1\n: the value of an accumulator, computed locally on a worker\n\n\nacc2\n: the value of an other accumulator, issued by another worker\n\n\nobj\n: the same parameter \nobj\n passed to \naggregate()\n\n\n\n\n\n\ninit\n: the initial value of the accumulators that are used by\n  \nreducer()\n and \ncombiner()\n. It should be the identity element\n  of the operation (a neutral zero value, i.e. applying it through the\n  function should not change result).\n\n\nobj\n: user provided data. Data will be passed to carrying\n  serializable data from master to workers, obj is shared amongst\n  mapper executions over each element of the dataset.\n\n\n\n\nExample:\n\n\nsc\n.\nparallelize\n([[\nhello\n,\n \n1\n],\n \n[\nhello\n,\n \n1\n],\n \n[\nworld\n,\n \n1\n]])\n\n  \n.\naggregateByKey\n((\na\n,\n \nb\n)\n \n=\n \na\n \n+\n \nb\n,\n \n(\na\n,\n \nb\n)\n \n=\n \na\n \n+\n \nb\n,\n \n0\n)\n\n  \n.\ncollect\n()\n\n  \n.\nthen\n(\nconsole\n.\nlog\n);\n\n\n// [ [ \nhello\n, 2 ], [ \nworld\n, 1 ] ]\n\n\n\n\n\n\nds.cartesian(other)\n\n\nReturns a dataset wich contains all possible pairs \n[a, b]\n where \na\n\nis in the source dataset and \nb\n is in the \nother\n dataset.\n\n\nExample:\n\n\nvar\n \nds1\n \n=\n \nsc\n.\nparallelize\n([\n1\n,\n \n2\n]);\n\n\nvar\n \nds2\n \n=\n \nsc\n.\nparallelize\n([\na\n,\n \nb\n,\n \nc\n]);\n\n\nds1\n.\ncartesian\n(\nds2\n).\ncollect\n().\nthen\n(\nconsole\n.\nlog\n);\n\n\n// [ [ 1, \na\n ], [ 1, \nb\n ], [ 1, \nc\n ],\n\n\n//   [ 2, \na\n ], [ 2, \nb\n ], [ 2, \nc\n ] ]\n\n\n\n\n\n\nds.coGroup(other)\n\n\nWhen called on dataset of type \n[k,v]\n and \n[k,w]\n, returns a dataset of type\n\n[k, [[v], [w]]]\n, where data of both datasets share the same key.\n\n\nExample:\n\n\nvar\n \nds1\n \n=\n \nsc\n.\nparallelize\n([[\n10\n,\n \n1\n],\n \n[\n20\n,\n \n2\n]]);\n\n\nvar\n \nds2\n \n=\n \nsc\n.\nparallelize\n([[\n10\n,\n \nworld\n],\n \n[\n30\n,\n \n3\n]]);\n\n\nds1\n.\ncoGroup\n(\nds2\n).\ncollect\n().\nthen\n(\nconsole\n.\nlog\n);\n\n\n// [ [ 10, [ [ 1 ], [ \nworld\n ] ] ],\n\n\n//   [ 20, [ [ 2 ], [] ] ],\n\n\n//   [ 30, [ [], [ 3 ] ] ] ]\n\n\n\n\n\n\nds.collect([done])\n\n\nThis \naction\n returns the content of the dataset in form of an array.\nThe result is passed to the \ndone()\n callback if provided, otherwise an\n\nES6 promise\n is returned.\n\n\n\n\ndone\n: a callback of the form \nfunction(error, result)\n which is\n  called at completion.\n\n\n\n\nExample:\n\n\nsc\n.\nparallelize\n([\n1\n,\n \n2\n,\n \n3\n,\n \n4\n])\n\n  \n.\ncollect\n(\nfunction\n \n(\nerr\n,\n \nres\n)\n \n{\n\n     \nconsole\n.\nlog\n(\nres\n);\n\n   \n});\n\n\n// [ 1, 2, 3, 4 ]\n\n\n\n\n\n\nds.count([done])\n\n\nThis \naction\n computes the number of elements in the dataset. The\nresult is passed to the \ndone()\n callback if provided, otherwise\nan \nES6 promise\n is returned.\n\n\n\n\ndone\n: a callback of the form \nfunction(error, result)\n which is\n  called at completion.\n\n\n\n\nExample:\n\n\nsc\n.\nparallelize\n([\n10\n,\n \n20\n,\n \n30\n,\n \n40\n]).\ncount\n().\nthen\n(\nconsole\n.\nlog\n);\n\n\n// 4\n\n\n\n\n\n\nds.countByKey([done])\n\n\nWhen called on a dataset of type \n[k,v]\n, this \naction\n computes\nthe number of occurrences of elements for each key in a dataset of\ntype \n[k,v]\n. It produces an array of elements of type \n[k,w]\n where\n\nw\n is the result count.  The result is passed to the \ndone()\n\ncallback if provided, otherwise an \nES6 promise\n is returned.\n\n\n\n\ndone\n: a callback of the form \nfunction(error, result)\n which is\n  called at completion.\n\n\n\n\nExample:\n\n\nsc\n.\nparallelize\n([[\n10\n,\n \n1\n],\n \n[\n20\n,\n \n2\n],\n \n[\n10\n,\n \n4\n]])\n\n  \n.\ncountByKey\n().\nthen\n(\nconsole\n.\nlog\n);\n\n\n// [ [ 10, 2 ], [ 20, 1 ] ]\n\n\n\n\n\n\nds.countByValue([done])\n\n\nThis \naction\n computes the number of occurences of each element in\ndataset and returns an array of elements of type \n[v,n]\n where \nv\n\nis the element and \nn\n its number of occurrences.  The result is\npassed to the \ndone()\n callback if provided, otherwise an \nES6\npromise\n is returned.\n\n\n\n\ndone\n: a callback of the form \nfunction(error, result)\n which is\n  called at completion.\n\n\n\n\nExample:\n\n\nsc\n.\nparallelize\n([\n \n1\n,\n \n2\n,\n \n3\n,\n \n1\n,\n \n3\n,\n \n2\n,\n \n5\n \n])\n\n  \n.\ncountByValue\n().\nthen\n(\nconsole\n.\nlog\n);\n\n\n// [ [ 1, 2 ], [ 2, 2 ], [ 3, 2 ], [ 5, 1 ] ]\n\n\n\n\n\n\nds.distinct()\n\n\nReturns a dataset where duplicates are removed.\n\n\nExample:\n\n\nsc\n.\nparallelize\n([\n \n1\n,\n \n2\n,\n \n3\n,\n \n1\n,\n \n4\n,\n \n3\n,\n \n5\n \n])\n\n  \n.\ndistinct\n()\n\n  \n.\ncollect\n().\nthen\n(\nconsole\n.\nlog\n);\n\n\n// [ 1, 2, 3, 4, 5 ]\n\n\n\n\n\n\nds.filter(filter[, obj])\n\n\n\n\nfilter\n: a function of the form \ncallback(element[, obj[, wc]])\n,\n  returning a \nBoolean\n and where:\n\n\nelement\n: the next element of the dataset on which \nfilter()\n operates\n\n\nobj\n: the same parameter \nobj\n passed to \nfilter()\n\n\nwc\n: the worker context, a persistent object local to each\n  worker, where user can store and access worker local dependencies.\n\n\n\n\n\n\nobj\n: user provided data. Data will be passed to carrying\n  serializable data from master to workers, obj is shared amongst\n  mapper executions over each element of the dataset\n\n\n\n\nApplies the provided filter function to each element of the source\ndataset and returns a new dataset containing the elements that passed the\ntest.\n\n\nExample:\n\n\nfunction\n \nfilter\n(\ndata\n,\n \nobj\n)\n \n{\n \nreturn\n \ndata\n \n%\n \nobj\n.\nmodulo\n;\n \n}\n\n\n\nsc\n.\nparallelize\n([\n1\n,\n \n2\n,\n \n3\n,\n \n4\n])\n\n  \n.\nfilter\n(\nfilter\n,\n \n{\nmodulo\n:\n \n2\n})\n\n  \n.\ncollect\n().\nthen\n(\nconsole\n.\nlog\n);\n\n\n// [ 1, 3 ]\n\n\n\n\n\n\nds.first([done])\n\n\nThis \naction\n computes the first element in this dataset.\nThe result is passed to the \ndone()\n callback if provided, otherwise an\n\nES6 promise\n is returned.\n\n\n\n\ndone\n: a callback of the form \nfunction(error, result)\n which is\n  called at completion.\n\n\n\n\nsc\n.\nparallelize\n([\n1\n,\n \n2\n,\n \n3\n]).\nfirst\n().\nthen\n(\nconsole\n.\nlog\n);\n\n\n// 1\n\n\n\n\n\n\nds.flatMap(flatMapper[, obj])\n\n\nApplies the provided mapper function to each element of the source\ndataset and returns a new dataset.\n\n\n\n\nflatMapper\n: a function of the form \ncallback(element[, obj[, wc]])\n,\n  returning an \nArray\n and where:\n\n\nelement\n: the next element of the dataset on which \nflatMap()\n operates\n\n\nobj\n: the same parameter \nobj\n passed to \nflatMap()\n\n\nwc\n: the worker context, a persistent object local to each\n  worker, where user can store and access worker local dependencies.\n\n\n\n\n\n\nobj\n: user provided data. Data will be passed to carrying\n  serializable data from master to workers, obj is shared amongst\n  mapper executions over each element of the dataset\n\n\n\n\nExample:\n\n\nsc\n.\nrange\n(\n5\n).\nflatMap\n(\na\n \n=\n \n[\na\n,\n \na\n]).\ncollect\n().\nthen\n(\nconsole\n.\nlog\n);\n\n\n// [ 0, 0, 1, 1, 2, 2, 3, 3, 4, 4 ]\n\n\n\n\n\n\nds.flatMapValues(flatMapper[, obj])\n\n\nApplies the provided flatMapper function to the value of each [key,\nvalue] element of the source dataset and return a new dataset containing\nelements defined as [key, mapper(value)], keeping the key unchanged\nfor each source element.\n\n\n\n\nflatMapper\n: a function of the form \ncallback(element[, obj[, wc]])\n,\n  returning an \nArray\n and where:\n\n\nelement\n: the value v of the next [k,v] element of the dataset on\n  which \nflatMapValues()\n operates\n\n\nobj\n: the same parameter \nobj\n passed to \nflatMapValues()\n\n\nwc\n: the worker context, a persistent object local to each\n  worker, where user can store and access worker local dependencies.\n\n\n\n\n\n\nobj\n: user provided data. Data will be passed to carrying\n  serializable data from master to workers, obj is shared amongst\n  mapper executions over each element of the dataset\n\n\n\n\nExample:\n\n\nfunction\n \nvalueFlatMapper\n(\ndata\n,\n \nobj\n)\n \n{\n\n    \nvar\n \ntmp\n \n=\n \n[];\n\n    \nfor\n \n(\nvar\n \ni\n \n=\n \n0\n;\n \ni\n \n \nobj\n.\nN\n;\n \ni\n++\n)\n \ntmp\n.\npush\n(\ndata\n \n*\n \nobj\n.\nfact\n);\n\n    \nreturn\n \ntmp\n;\n\n\n}\n\n\n\nsc\n.\nparallelize\n([[\nhello\n,\n \n1\n],\n \n[\nworld\n,\n \n2\n]])\n\n  \n.\nflatMapValues\n(\nvalueFlatMapper\n,\n \n{\nN\n:\n \n2\n,\n \nfact\n:\n \n2\n})\n\n  \n.\ncollect\n().\nthen\n(\nconsole\n.\nlog\n);\n\n\n// [ [ \nhello\n, 2 ], [ \nhello\n, 2 ], [ \nworld\n, 4 ], [ \nworld\n, 4 ] ]\n\n\n\n\n\n\nds.forEach(callback[, obj][, done])\n\n\nThis \naction\n applies a \ncallback\n function on each element of the dataset.\nIf provided, the \ndone()\n callback is invoked at completion, otherwise an\n\nES6 promise\n is returned.\n\n\n\n\ncallback\n: a function of the form \nfunction(val[, obj[, wc]])\n,\n  which returns \nnull\n and with:\n\n\nval\n: the value of the next element of the dataset on which\n   \nforEach()\n operates\n\n\nobj\n: the same parameter \nobj\n passed to \nforEach()\n\n\nwc\n: the worker context, a persistent object local to each\n   worker, where user can store and access worker local dependencies.\n\n\n\n\n\n\nobj\n: user provided data. Data will be passed to carrying\n  serializable data from master to workers, obj is shared amongst\n  mapper executions over each element of the dataset\n\n\ndone\n: a callback of the form \nfunction(error, result)\n which is\n  called at completion.\n\n\n\n\nIn the following example, the \nconsole.log()\n callback provided\nto \nforEach()\n is executed on workers and may be not visible:\n\n\nsc\n.\nparallelize\n([\n1\n,\n \n2\n,\n \n3\n,\n \n4\n])\n\n  \n.\nforEach\n(\nconsole\n.\nlog\n).\nthen\n(\nconsole\n.\nlog\n(\nfinished\n));\n\n\n\n\n\n\nds.groupByKey()\n\n\nWhen called on a dataset of type \n[k,v]\n, returns a dataset of type \n[k, [v]]\n\nwhere values with the same key are grouped.\n\n\nExample:\n\n\nsc\n.\nparallelize\n([[\n10\n,\n \n1\n],\n \n[\n20\n,\n \n2\n],\n \n[\n10\n,\n \n4\n]])\n\n  \n.\ngroupByKey\n().\ncollect\n().\nthen\n(\nconsole\n.\nlog\n);\n\n\n// [ [ 10, [ 1, 4 ] ], [ 20, [ 2 ] ] ]\n\n\n\n\n\n\nds.intersection(other)\n\n\nReturns a dataset containing only elements found in source dataset and \nother\n\ndataset.\n\n\nExample:\n\n\nvar\n \nds1\n \n=\n \nsc\n.\nparallelize\n([\n1\n,\n \n2\n,\n \n3\n,\n \n4\n,\n \n5\n]);\n\n\nvar\n \nds2\n \n=\n \nsc\n.\nparallelize\n([\n3\n,\n \n4\n,\n \n5\n,\n \n6\n,\n \n7\n]);\n\n\nds1\n.\nintersection\n(\nds2\n).\ncollect\n().\nthen\n(\nconsole\n.\nlog\n);\n \n// [ 3, 4, 5 ]\n\n\n\n\n\n\nds.join(other)\n\n\nWhen called on source dataset of type \n[k,v]\n and \nother\n dataset of type\n\n[k,w]\n, returns a dataset of type \n[k, [v, w]]\n pairs with all pairs\nof elements for each key.\n\n\nExample:\n\n\nvar\n \nds1\n \n=\n \nsc\n.\nparallelize\n([[\n10\n,\n \n1\n],\n \n[\n20\n,\n \n2\n]]);\n\n\nvar\n \nds2\n \n=\n \nsc\n.\nparallelize\n([[\n10\n,\n \nworld\n],\n \n[\n30\n,\n \n3\n]]);\n\n\nds1\n.\njoin\n(\nds2\n).\ncollect\n().\nthen\n(\nconsole\n.\nlog\n);\n\n\n// [ [ 10, [ 1, \nworld\n ] ] ]\n\n\n\n\n\n\nds.keys()\n\n\nWhen called on source dataset of type \n[k,v]\n, returns a dataset with just\nthe elements \nk\n.\n\n\nExample:\n\n\nsc\n.\nparallelize\n([[\n10\n,\n \nworld\n],\n \n[\n30\n,\n \n3\n]])\n\n  \n.\nkeys\n.\ncollect\n().\nthen\n(\nconsole\n.\nlog\n);\n\n\n// [ 10, 30 ]\n\n\n\n\n\n\nds.leftOuterJoin(other)\n\n\nWhen called on source dataset of type \n[k,v]\n and \nother\n dataset of type\n\n[k,w]\n, returns a dataset of type \n[k, [v, w]]\n pairs where the key\nmust be present in the \nother\n dataset.\n\n\nExample:\n\n\nvar\n \nds1\n \n=\n \nsc\n.\nparallelize\n([[\n10\n,\n \n1\n],\n \n[\n20\n,\n \n2\n]]);\n\n\nvar\n \nds2\n \n=\n \nsc\n.\nparallelize\n([[\n10\n,\n \nworld\n],\n \n[\n30\n,\n \n3\n]]);\n\n\nds1\n.\nleftOuterJoin\n(\nds2\n).\ncollect\n().\nthen\n(\nconsole\n.\nlog\n);\n\n\n// [ [ 10, [ 1, \nworld\n ] ], [ 20, [ 2, null ] ] ]\n\n\n\n\n\n\nds.lookup(k[, done])\n\n\nWhen called on source dataset of type \n[k,v]\n, returns an array\nof values \nv\n for key \nk\n.\nThe result is passed to the \ndone()\n callback if provided, otherwise an\n\nES6 promise\n is returned.\n\n\n\n\ndone\n: a callback of the form \nfunction(error, result)\n which is\n  called at completion.\n\n\n\n\nExample:\n\n\nsc\n.\nparallelize\n([[\n10\n,\n \nworld\n],\n \n[\n20\n,\n \n2\n],\n \n[\n10\n,\n \n1\n],\n \n[\n30\n,\n \n3\n]])\n\n  \n.\nlookup\n(\n10\n).\nthen\n(\nconsole\n.\nlog\n);\n\n\n// [ world, 1 ]\n\n\n\n\n\n\nds.map(mapper[, obj])\n\n\nApplies the provided mapper function to each element of the source\ndataset and returns a new dataset.\n\n\n\n\nmapper\n: a function of the form \ncallback(element[, obj[, wc]])\n,\n  returning an element and where:\n\n\nelement\n: the next element of the dataset on which \nmap()\n operates\n\n\nobj\n: the same parameter \nobj\n passed to \nmap()\n\n\nwc\n: the worker context, a persistent object local to each\n  worker, where user can store and access worker local dependencies.\n\n\n\n\n\n\nobj\n: user provided data. Data will be passed to carrying\n  serializable data from master to workers, obj is shared amongst\n  mapper executions over each element of the dataset\n\n\n\n\nExample:\n\n\nsc\n.\nparallelize\n([\n1\n,\n \n2\n,\n \n3\n,\n \n4\n])\n\n  \n.\nmap\n((\ndata\n,\n \nobj\n)\n \n=\n \ndata\n \n*\n \nobj\n.\nscaling\n,\n \n{\nscaling\n:\n \n1.2\n})\n\n  \n.\ncollect\n().\nthen\n(\nconsole\n.\nlog\n);\n\n\n// [ 1.2, 2.4, 3.6, 4.8 ]\n\n\n\n\n\n\nds.mapValues(mapper[, obj])\n\n\n\n\nmapper\n: a function of the form \ncallback(element[, obj[, wc]])\n,\n  returning an element and where:\n\n\nelement\n: the value v of the next [k,v] element of the dataset on\n  which \nmapValues()\n operates\n\n\nobj\n: the same parameter \nobj\n passed to \nmapValues()\n\n\nwc\n: the worker context, a persistent object local to each\n  worker, where user can store and access worker local dependencies\n\n\n\n\n\n\nobj\n: user provided data. Data will be passed to carrying\n  serializable data from master to workers, obj is shared amongst\n  mapper executions over each element of the dataset\n\n\n\n\nApplies the provided mapper function to the value of each \n[k,v]\n\nelement of the source dataset and return a new dataset containing elements\ndefined as \n[k, mapper(v)]\n, keeping the key unchanged for each\nsource element.\n\n\nExample:\n\n\nsc\n.\nparallelize\n([[\nhello\n,\n \n1\n],\n \n[\nworld\n,\n \n2\n]])\n\n  \n.\nmapValues\n((\na\n,\n \nobj\n)\n \n=\n \na\n*\nobj\n.\nfact\n,\n \n{\nfact\n:\n \n2\n})\n\n  \n.\ncollect\n().\nthen\n(\nconsole\n.\nlog\n);\n\n\n// [ [\nhello\n, 2], [\nworld\n, 4] ]\n\n\n\n\n\n\nds.partitionBy(partitioner)\n\n\nReturns a dataset partitioned using the specified partitioner. The\npurpose of this transformation is not to change the dataset content,\nbut to increase processing speed by ensuring that the elements\naccessed by further transfomations reside in the same partition.\n\n\nExample:\n\n\nvar\n \nskale\n \n=\n \nrequire\n(\nskale\n);\n\n\nvar\n \nsc\n \n=\n \nskale\n.\ncontext\n();\n\n\n\nsc\n.\nparallelize\n([[\nhello\n,\n \n1\n],\n \n[\nworld\n,\n \n1\n],\n \n[\nhello\n,\n \n2\n],\n \n[\nworld\n,\n \n2\n],\n \n[\ncedric\n,\n \n3\n]])\n\n  \n.\npartitionBy\n(\nnew\n \nskale\n.\nHashPartitioner\n(\n3\n))\n\n  \n.\ncollect\n.\nthen\n(\nconsole\n.\nlog\n)\n\n\n// [ [\nworld\n, 1], [\nworld\n, 2], [\nhello\n, 1], [\nhello\n, 2], [\ncedric\n, 3] ]\n\n\n\n\n\n\nds.persist()\n\n\nReturns the dataset, and persists the dataset content on disk (and\nin memory if available) in order to directly reuse content in further\ntasks.\n\n\nExample:\n\n\nvar\n \ndataset\n \n=\n \nsc\n.\nrange\n(\n100\n).\nmap\n(\na\n \n=\n \na\n \n*\n \na\n);\n\n\n\n// First action: compute dataset\n\n\ndataset\n.\ncollect\n().\nthen\n(\nconsole\n.\nlog\n)\n\n\n\n// Second action: reuse dataset, avoid map transform\n\n\ndataset\n.\ncollect\n().\nthen\n(\nconsole\n.\nlog\n)\n\n\n\n\n\n\nds.reduce(reducer, init[, obj][, done])\n\n\nThis \naction\n returns the aggregated value of the elements\nof the dataset using a \nreducer()\n function.\nThe result is passed to the \ndone()\n callback if provided, otherwise an\n\nES6 promise\n is returned.\n\n\n\n\nreducer\n: a function of the form \nfunction(acc, val[, obj[, wc]])\n,\n  which returns the next value of the accumulator (which must be\n  of the same type as \nacc\n and \nval\n) and with:\n\n\nacc\n: the value of the accumulator, initially set to \ninit\n\n\nval\n: the value of the next element of the dataset on which\n   \nreduce()\n operates\n\n\nobj\n: the same parameter \nobj\n passed to \nreduce()\n\n\nwc\n: the worker context, a persistent object local to each\n   worker, where user can store and access worker local dependencies.\n\n\n\n\n\n\ninit\n: the initial value of the accumulators that are used by\n  \nreducer()\n. It should be the identity element of the operation\n  (i.e. applying it through the function should not change result).\n\n\nobj\n: user provided data. Data will be passed to carrying\n  serializable data from master to workers, obj is shared amongst\n  mapper executions over each element of the dataset\n\n\ndone\n: a callback of the form \nfunction(error, result)\n which is\n  called at completion.\n\n\n\n\nExample:\n\n\nsc\n.\nparallelize\n([\n1\n,\n \n2\n,\n \n4\n,\n \n8\n])\n\n  \n.\nreduce\n((\na\n,\n \nb\n)\n \n=\n \na\n \n+\n \nb\n,\n \n0\n)\n\n  \n.\nthen\n(\nconsole\n.\nlog\n);\n\n\n// 15\n\n\n\n\n\n\nds.reduceByKey(reducer, init[, obj])\n\n\n\n\nreducer\n: a function of the form \ncallback(acc,val[, obj[, wc]])\n,\n  returning the next value of the accumulator (which must be of the\n  same type as \nacc\n and \nval\n) and where:\n\n\nacc\n: the value of the accumulator, initially set to \ninit\n\n\nval\n: the value \nv\n of the next \n[k,v]\n element of the dataset on\n  which \nreduceByKey()\n operates\n\n\nobj\n: the same parameter \nobj\n passed to \nreduceByKey()\n\n\nwc\n: the worker context, a persistent object local to each\n  worker, where user can store and access worker local dependencies.\n\n\n\n\n\n\ninit\n: the initial value of accumulator for each key. Will be\n  passed to \nreducer\n.\n\n\nobj\n: user provided data. Data will be passed to carrying\n  serializable data from master to workers, obj is shared amongst\n  mapper executions over each element of the dataset\n\n\n\n\nWhen called on a dataset of type \n[k,v]\n, returns a dataset of type \n[k,v]\n\nwhere the values of each key are aggregated using the \nreducer\n\nfunction and the \ninit\n initial value.\n\n\nExample:\n\n\nsc\n.\nparallelize\n([[\n10\n,\n \n1\n],\n \n[\n10\n,\n \n2\n],\n \n[\n10\n,\n \n4\n]])\n\n  \n.\nreduceByKey\n((\na\n,\nb\n)\n \n=\n \na\n+\nb\n,\n \n0\n)\n\n  \n.\ncollect\n().\nthen\n(\nconsole\n.\nlog\n);\n\n\n// [ [10, 7] ]\n\n\n\n\n\n\nds.rightOuterJoin(other)\n\n\nWhen called on source dataset of type \n[k,v]\n and \nother\n dataset of type\n\n[k,w]\n, returns a dataset of type \n[k, [v, w]]\n pairs where the key\nmust be present in the \nsource\n dataset.\n\n\nExample:\n\n\nvar\n \nds1\n \n=\n \nsc\n.\nparallelize\n([[\n10\n,\n \n1\n],\n \n[\n20\n,\n \n2\n]]);\n\n\nvar\n \nds2\n \n=\n \nsc\n.\nparallelize\n([[\n10\n,\n \nworld\n],\n \n[\n30\n,\n \n3\n]]);\n\n\nds1\n.\nrightOuterJoin\n(\nds2\n).\ncollect\n().\nthen\n(\nconsole\n.\nlog\n);\n\n\n// [ [ 10, [ 1, \nworld\n ] ], [ 30, [ null, 2 ] ] ]\n\n\n\n\n\n\nds.sample(withReplacement, frac)\n\n\n\n\nwithReplacement\n: \nBoolean\n value, \ntrue\n if data must be sampled\n  with replacement\n\n\nfrac\n: \nNumber\n value of the fraction of source dataset to return\n\n\n\n\nReturns a dataset by sampling a fraction \nfrac\n of source dataset, with or\nwithout replacement.\n\n\nExample:\n\n\nsc\n.\nparallelize\n([\n1\n,\n \n2\n,\n \n3\n,\n \n4\n,\n \n5\n,\n \n6\n,\n \n7\n,\n \n8\n])\n\n  \n.\nsample\n(\ntrue\n,\n \n0.5\n,\n \n0\n)\n\n  \n.\ncollect\n().\nthen\n(\nconsole\n.\nlog\n);\n\n\n// [ 1, 1, 3, 4, 4, 5, 7 ]\n\n\n\n\n\n\nds.save(url[, options][, done])\n\n\nThis \naction\n saves the content of the dataset to the destination URL. The\ndestination is a flat directory which will contain as many files as partitions\nin the dataset. Files are named from partition numbers, starting at 0.\nThe file format is a stream of JSON strings (one per dataset\nelement) separated by newlines.\n\n\n\n\nurl\n: a \nString\n of the general form \nprotocol://host/path\n or \n/path\n. See\n  below for supported protocols\n\n\noptions\n: an \nObject\n with the following fields:\n\n\ngzip\n: \nBoolean\n (default false) to enable gzip compression. If compression\n  is enabled, files are suffixed with \n.gz\n\n\n\n\n\n\ndone\n: an optional callback function of the form \nfunction(error, result)\n\n  called at completion. If not provided, an \nES6 promise\n is returned.\n\n\n\n\nFile protocol\n\n\nThe URL form is \nfile://path\n or simply \npath\n where \npath\n is an absolute\npathname in the master host local file system.\n\n\nExample:\n\n\nsc\n.\nrange\n(\n300\n).\nsave\n(\n/tmp/results/\n).\nthen\n(\nsc\n.\nend\n());\n\n\n// will produce /tmp/results/0, /tmp/results/1\n\n\n\n\n\n\nAWS S3 protocol\n\n\nThe URL form is \ns3://bucket/key\n. AWS credentials must be provided by environment\nvariables i.e \nAWS_SECRET_ACCESS_KEY\n, \nAWS_ACCESS_KEY_ID\n.\n\n\nExample:\n\n\nsc\n.\nrange\n(\n300\n).\nsave\n(\ns3://myproject/mydataset\n,\n \n{\ngzip\n:\n \ntrue\n}).\nthen\n(\nsc\n.\nend\n());\n\n\n// will produce https://myproject.s3.amazonaws.com/mydataset/0.gz\n\n\n\n\n\n\nAzure blob storage protocol\n\n\nThe URL form is \nwasb://container@user.blob.core.windows.net/blob_name\n. Azure\ncredentials must be provided by environment variables i.e\n\nAZURE_STORAGE_ACCOUNT\n and \nAZURE_STORAGE_ACCESS_KEY\n.\n\n\nds.sortBy(keyfunc[, ascending])\n\n\nReturns a dataset sorted by the given \nkeyfunc\n.\n\n\n\n\nkeyfunc\n: a function of the form \nfunction(element)\n which returns\n  a value used for comparison in the sort function and where \nelement\n\n  is the next element of the dataset on which \nsortBy()\n operates\n\n\nascending\n: a boolean to set the sort direction. Default: true\n\n\n\n\nExample:\n\n\nsc\n.\nparallelize\n([\n4\n,\n \n6\n,\n \n10\n,\n \n5\n,\n \n1\n,\n \n2\n,\n \n9\n,\n \n7\n,\n \n3\n,\n \n0\n])\n\n  \n.\nsortBy\n(\na\n \n=\n \na\n)\n\n  \n.\ncollect\n().\nthen\n(\nconsole\n.\nlog\n)\n\n\n// [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n\n\n\n\n\nds.sortByKey(ascending)\n\n\nWhen called on a dataset of type \n[k,v]\n, returns a dataset of type \n[k,v]\n\nsorted on \nk\n. The optional parameter \nascending\n is a boolean which sets\nthe sort direction, true by default.\n\n\nExample:\n\n\nsc\n.\nparallelize\n([[\nworld\n,\n \n2\n],\n \n[\ncedric\n,\n \n3\n],\n \n[\nhello\n,\n \n1\n]])\n\n  \n.\nsortByKey\n()\n\n  \n.\ncollect\n().\nthen\n(\nconsole\n.\nlog\n)\n\n\n// [[\ncedric\n, 3], [\nhello\n, 1], [\nworld\n, 2]]\n\n\n\n\n\n\nds.stream([opt])\n\n\nThis \naction\n returns a \nreadable stream\n of dataset content. The order\nof data and partitions is maintained.\n\n\n\n\nopt\n: an object with the following fields:\n\n\nend\n: \nBoolean\n, when true, call \nsc.end()\n on stream \nend\n event. Default value: false.\n\n\ngzip\n: \nBoolean\n, when true, enable gzip compression. Default value: false.\n\n\n\n\nExample:\n\n\nvar\n \ns\n \n=\n \nsc\n.\nrange\n(\n4\n).\nstream\n();\n\n\ns\n.\npipe\n(\nprocess\n.\nstdout\n);\n\n\n// 0\n\n\n// 1\n\n\n// 2\n\n\n// 3\n\n\n\n\n\n\nds.subtract(other)\n\n\nReturns a dataset containing only elements of source dataset which\nare not in \nother\n dataset.\n\n\nExample:\n\n\nvar\n \nds1\n \n=\n \nsc\n.\nparallelize\n([\n1\n,\n \n2\n,\n \n3\n,\n \n4\n,\n \n5\n]);\n\n\nvar\n \nds2\n \n=\n \nsc\n.\nparallelize\n([\n3\n,\n \n4\n,\n \n5\n,\n \n6\n,\n \n7\n]);\n\n\nds1\n.\nsubtract\n(\nds2\n).\ncollect\n().\nthen\n(\nconsole\n.\nlog\n);\n\n\n// [ 1, 2 ]\n\n\n\n\n\n\nds.take(num[, done])\n\n\nThis \naction\n returns an array of the \nnum\n first elements of the\nsource dataset.  The result is passed to the \ndone()\n callback if\nprovided, otherwise an \nES6 promise\n is returned.\n\n\n\n\nnum\n: positive integer \nNumber\n of elements\n\n\ndone\n: a callback of the form \nfunction(error, result)\n which is\n  called at completion.\n\n\n\n\nExample:\n\n\nsc\n.\nrange\n(\n5\n).\ntake\n(\n2\n).\nthen\n(\nconsole\n.\nlog\n);\n\n\n// [1, 2]\n\n\n\n\n\n\nds.takeSample(withReplacement, num[, done])\n\n\nThis \naction\n returns an array with a random sample of \nnum\n elements\nof the dataset, with or without replacement. The result is passed to\nthe \ndone()\n callback if provided, otherwise an \nES6 promise\n is returned.\n\n\n\n\nwithReplacement\n: \nBoolean\n value, \ntrue\n if data must be sampled\n  with replacement\n\n\nnum\n: positive integer \nNumber\n of elements\n\n\ndone\n: a callback of the form \nfunction(error, result)\n which is\n  called at completion.\n\n\n\n\nExample:\n\n\nsc\n.\nrange\n(\n100\n).\ntakeSample\n(\n4\n).\nthen\n(\nconsole\n.\nlog\n);\n\n\n// [ 18, 75, 4, 57 ]\n\n\n\n\n\n\nds.top(num[, done])\n\n\nThis \naction\n returns an array of the \nnum\n top elements of the\nsource dataset.  The result is passed to the \ndone()\n callback if\nprovided, otherwise an \nES6 promise\n is returned.\n\n\n\n\nnum\n: positive integer \nNumber\n of elements\n\n\ndone\n: a callback of the form \nfunction(error, result)\n which is\n  called at completion.\n\n\n\n\nExample:\n\n\nsc\n.\nrange\n(\n5\n).\ntop\n(\n2\n).\nthen\n(\nconsole\n.\nlog\n);\n\n\n// [3, 4]\n\n\n\n\n\n\nds.union(other)\n\n\nReturns a dataset that contains the union of the elements in the source\ndataset and the \nother\n dataset.\n\n\nExample:\n\n\nvar\n \nds1\n \n=\n \nsc\n.\nparallelize\n([\n1\n,\n \n2\n,\n \n3\n,\n \n4\n,\n \n5\n]);\n\n\nvar\n \nds2\n \n=\n \nsc\n.\nparallelize\n([\n3\n,\n \n4\n,\n \n5\n,\n \n6\n,\n \n7\n]);\n\n\nds1\n.\nunion\n(\nds2\n).\ncollect\n().\nthen\n(\nconsole\n.\nlog\n);\n\n\n// [ 1, 2, 3, 4, 5, 3, 4, 5, 6, 7 ]\n\n\n\n\n\n\nds.values()\n\n\nWhen called on source dataset of type \n[k,v]\n, returns a dataset with just\nthe elements \nv\n.\n\n\nExample:\n\n\nsc\n.\nparallelize\n([[\n10\n,\n \nworld\n],\n \n[\n30\n,\n \n3\n]])\n\n  \n.\nkeys\n.\ncollect\n().\nthen\n(\nconsole\n.\nlog\n);\n\n\n// [ \nworld\n, 3 ]\n\n\n\n\n\n\nPartitioners\n\n\nA partitioner is an object passed to\n\nds.partitionBy(partitioner)\n which\nplaces data in partitions according to a strategy, for example hash\npartitioning, where data having the same key are placed in the same\npartition, or range partitioning, where data in the same range are\nin the same partition. This is useful to accelerate processing, as\nit limits data transfers between workers during jobs.\n\n\nA partition object must provide the following properties:\n\n\n\n\nnumPartitions\n: a \nNumber\n of partitions for the dataset\n\n\ngetPartitionIndex\n: a \nFunction\n of type \nfunction(element)\n\n  which returns the partition index (comprised between 0 and\n  \nnumPartitions\n) for the \nelement\n of the dataset on which\n  \npartitionBy()\n operates.\n\n\n\n\nHashPartitioner(numPartitions)\n\n\nReturns a partitioner object which implements hash based partitioning\nusing a hash checksum of each element as a string.\n\n\n\n\nnumPartitions\n: \nNumber\n of partitions for this dataset\n\n\n\n\nExample:\n\n\nvar\n \nhp\n \n=\n \nnew\n \nskale\n.\nHashPartitioner\n(\n3\n)\n\n\nvar\n \ndataset\n \n=\n \nsc\n.\nrange\n(\n10\n).\npartitionBy\n(\nhp\n)\n\n\n\n\n\n\nRangePartitioner(numPartitions, keyfunc, dataset)\n\n\nReturns a partitioner object which first defines ranges by sampling\nthe dataset and then places elements by comparing them with ranges.\n\n\n\n\nnumPartitions\n: \nNumber\n of partitions for this dataset\n\n\nkeyfunc\n: a function of the form \nfunction(element)\n which returns\n  a value used for comparison in the sort function and where \nelement\n\n  is the next element of the dataset on which \npartitionBy()\n operates\n\n\ndataset\n: the dataset object on which \npartitionBy()\n operates\n\n\n\n\nExample:\n\n\nvar\n \ndataset\n \n=\n \nsc\n.\nrange\n(\n100\n)\n\n\nvar\n \nrp\n \n=\n \nnew\n \nskale\n.\nRangePartitioner\n(\n3\n,\n \na\n \n=\n \na\n,\n \ndataset\n)\n\n\nvar\n \ndataset\n \n=\n \nsc\n.\nrange\n(\n10\n).\npartitionBy\n(\nrp\n)\n\n\n\n\n\n\nEnvironment variables\n\n\n\n\nSKALE_HOST\n: The hostname of the skale-server process in distributed mode. If unset, the master runs in standalone mode.\n\n\nSKALE_PORT\n: The port of the skale-server process in distributed mode. Default value: \"12346\"\n\n\nSKALE_KEY\n: An authentication token which may be required by the skale-server process\n\n\nSKALE_DEBUG\n: set the debug trace level to the following values:\n\n\n0\n: or unset: no traces\n\n\n1\n: debug traces from master side\n\n\n2\n: above traces plus worker traces\n\n\n3\n: above traces plus network protocol traces (if running in distributed mode)", 
            "title": "Skale API"
        }, 
        {
            "location": "/skale-API/#skale-api", 
            "text": "The Skale module is the main entry point for Skale functionality.\nTo use it, one must  require('skale') .", 
            "title": "Skale API"
        }, 
        {
            "location": "/skale-API/#skalecontextconfig", 
            "text": "Creates and returns a new context which represents the connection\nto the Skale cluster, and which can be used to create datasets on that\ncluster. Config is an  Object  which defines the cluster server,\nwith the following defaults:  { \n   host :   localhost ,    // Cluster server host, settable also by SKALE_HOST env \n   port :   12346         // Cluster server port, settable also by SKALE_PORT env  }   Example:  var   skale   =   require ( skale );  var   sc   =   skale . context ();", 
            "title": "skale.context([config])"
        }, 
        {
            "location": "/skale-API/#scenv", 
            "text": "The  sc.env  property returns an object containing user environment variables\nto be set in workers.  To set and propagate an environment variable to all workers, assign  sc.env  object\nprior to invoking an action.  Example:  sc . env . MY_VAR   =   my_value ;", 
            "title": "sc.env"
        }, 
        {
            "location": "/skale-API/#scend", 
            "text": "Closes the connection to the cluster.", 
            "title": "sc.end()"
        }, 
        {
            "location": "/skale-API/#sclinestreaminput_stream", 
            "text": "Returns a new dataset of lines of text read from input_stream Object , which is a  readable stream  where dataset content is\nread from.  The following example computes the size of a file using streams:  var   stream   =   fs . createReadStream ( data.txt ,   utf8 );  sc . lineStream ( stream ) \n   . map ( s   =   s . length ) \n   . reduce (( a ,   b )   =   a   +   b ,   0 ) \n   . then ( console . log );", 
            "title": "sc.lineStream(input_stream)"
        }, 
        {
            "location": "/skale-API/#scobjectstreaminput_stream", 
            "text": "Returns a new dataset of Javascript  Objects  read from input_stream Object , which is a  readable stream  where dataset content is\nread from.  The following example counts the number of objects returned in an\nobject stream using the mongodb native Javascript driver:  var   cursor   =   db . collection ( clients ). find ();  sc . objectStream ( cursor ). count (). then ( console . log );", 
            "title": "sc.objectStream(input_stream)"
        }, 
        {
            "location": "/skale-API/#scparallelizearray", 
            "text": "Returns a new dataset containing elements from the  Array  array.  Example:  var   a   =   sc . parallelize ([ Hello ,   World ]);", 
            "title": "sc.parallelize(array)"
        }, 
        {
            "location": "/skale-API/#scrangestart-end-step", 
            "text": "Returns a new dataset of integers from  start  to  end  (exclusive)\nincreased by  step  (default 1) every element. If called with a\nsingle argument, the argument is interpreted as  end , and  start \nis set to 0.  sc . range ( 5 ). collect (). then ( console . log )  // [ 0, 1, 2, 3, 4 ]  sc . range ( 2 ,   4 ). collect (). then ( console . log )  // [ 2, 3 ]  sc . range ( 10 ,   - 5 ,   - 3 ). collect (). then ( console . log )  // [ 10, 7, 4, 1, -2 ]", 
            "title": "sc.range(start[, end[, step]])"
        }, 
        {
            "location": "/skale-API/#screquiremodules", 
            "text": "Sets a list of dependency modules to be deployed in workers for use\nby callbacks, such as mappers or reducers.  Returns the context\nobject.   modules : an  Object  of the form  {name1: 'path1', ...} \n  where  name1  is the name of the variable to which the module\n  is assigned to, and  path1  a path expression as in  require.resolve(path) .   Under the hood,  browserify  is used on master side to build a\nbundle which is serialized and sent to workers, where\nIt is then evaluated in global context.  Example:  // deps.js contains:  // module.export = function add3(a) {return a + 3;};  sc . require ({ add3 :   ./deps.js }) \n   . range ( 4 ) \n   . map ( a   =   add3 ) \n   . collect () \n   . then ( console . log );  // [ 3, 4, 5, 6 ]", 
            "title": "sc.require(modules)"
        }, 
        {
            "location": "/skale-API/#scsourcesize-callback-args", 
            "text": "Returns a new dataset of  size  elements, where each element is\ngenerated by a custom function  callback  executed on workers.   size : an integer  Number  of elements in the dataset  callback : a function of the form  function(index, args[, wc]) \n  which returns the next element and with:  index : the index of the element in the dataset, comprised\n  between  0  and  size - 1  args : the same parameter  args  passed to source  wc : the worker context, a persistent object local to each\n  worker, where user can store and access worker local dependencies    args : user custom parameter, passed to  callback   function   randArray ( index ,   len )   { \n   var   arr   =   []; \n   for   ( var   i   =   0 ;   i     len ;   i ++ ) \n     arr . push ( Math . floor ( Math . random ()   *   100 )); \n   return   arr ;  }  sc . source ( 3 ,   randArray ,   2 ). collect (). then ( console . log );  // [ [ 31, 85 ], [ 93, 21 ], [ 99, 58 ] ]", 
            "title": "sc.source(size, callback[, args])"
        }, 
        {
            "location": "/skale-API/#sctextfilepath-options", 
            "text": "Returns a new dataset of lines in file specified by path  String .   path : a  String  of the general form  protocol://host/path  or  /path ,\n  where protocol can be one of:  file : if path is on local filesystem  s3 : if path relates to a repository on AWS [S3] storage system  wasb : if path relates to a repository on Azure blob storage system  options : an  Object  with the following fields:  maxFiles : a  Number  of maximum files to process if the path refers\n    to a directory.  parquet : a  Boolean  to indicate that all files are in the\n    [parquet] format. Default value is  false .   if  path  ends by a '/' (directory separator), then the dataset\nwill be composed of all the files in the directory. Sub-directories\nare not supported. Wildcard characters such as  * ,  ? , etc, as\nin the Unix Shell globbing patterns are supported.  If a file name ends by '.gz', then its content will be automatically\nuncompressed using GZIP.  If a file name ends by '.parquet', it will automatically be processed\nas a [parquet].  Note: If using a path on the local filesystem, the file must also\nbe accessible at the same path on worker nodes. Either copy the\nfile to all workers or use a network-mounted shared file system.  For example, the following program prints the length of a text file:  var   lines   =   sc . textFile ( data.txt );  lines . map ( s   =   s . length ). reduce (( a ,   b )   =   a   +   b ,   0 ). then ( console . log );", 
            "title": "sc.textFile(path[, options])"
        }, 
        {
            "location": "/skale-API/#dataset-methods", 
            "text": "Dataset objects, as created initially by above skale context source\nfunctions, have the following methods, allowing either to instantiate\na new dataset through a transformation, or to return results to the\nmaster program.", 
            "title": "Dataset methods"
        }, 
        {
            "location": "/skale-API/#dsaggregatereducer-combiner-init-obj-done", 
            "text": "This  action  computes the aggregated value of the elements\nof the dataset using two functions  reducer()  and  combiner() ,\nallowing to use an arbitrary accumulator type, different from element\ntype (as opposed to  reduce()  which imposes the same type for\naccumulator and element).\nThe result is passed to the  done()  callback if provided, otherwise an ES6 promise  is returned.   reducer : a function of the form  function(acc, val[, obj[, wc]]) ,\n  which returns the next value of the accumulator (which must be\n  of the same type as  acc ) and with:  acc : the value of the accumulator, initially set to  init  val : the value of the next element of the dataset on which\n    aggregate()  operates  obj : the same parameter  obj  passed to  aggregate()  wc : the worker context, a persistent object local to each\n   worker, where user can store and access worker local dependencies.    combiner : a function of the form  function(acc1, acc2[, obj]) ,\n  which returns the merged value of accumulators and with:  acc1 : the value of an accumulator, computed locally on a worker  acc2 : the value of an other accumulator, issued by another worker  obj : the same parameter  obj  passed to  aggregate()    init : the initial value of the accumulators that are used by\n   reducer()  and  combiner() . It should be the identity element\n  of the operation (a neutral zero value, i.e. applying it through the\n  function should not change result).  obj : user provided data. Data will be passed to carrying\n  serializable data from master to workers, obj is shared amongst\n  mapper executions over each element of the dataset.  done : a callback of the form  function(error, result)  which is\n  called at completion. If  undefined ,  aggregate()  returns an\n   ES6 promise .   The following example computes the average of a dataset, avoiding a  map() :  sc . parallelize ([ 3 ,   5 ,   2 ,   7 ,   4 ,   8 ]) \n   . aggregate (( a ,   v )   =   [ a [ 0 ]   +   v ,   a [ 1 ]   +   1 ], \n     ( a1 ,   a2 )   =   [ a1 [ 0 ]   +   a2 [ 0 ],   a1 [ 1 ]   +   a2 [ 1 ]],   [ 0 ,   0 ]) \n   . then ( function ( data )   { \n     console . log ( data [ 0 ]   /   data [ 1 ]); \n   })  // 4.8333", 
            "title": "ds.aggregate(reducer, combiner, init[, obj][, done])"
        }, 
        {
            "location": "/skale-API/#dsaggregatebykeyreducer-combiner-init-obj", 
            "text": "When called on a dataset of type  [k,v] , returns a dataset of type [k,v]  where  v  is the aggregated value of all elements of same\nkey  k . The aggregation is performed using two functions  reducer() \nand  combiner()  allowing to use an arbitrary accumulator type,\ndifferent from element type.   reducer : a function of the form  function(acc, val[, obj[, wc]]) ,\n  which returns the next value of the accumulator (which must be\n  of the same type as  acc ) and with:  acc : the value of the accumulator, initially set to  init  val : the value  v  of the next  [k,v]  element of the dataset\n   on which  aggregateByKey()  operates  obj : the same parameter  obj  passed to  aggregateByKey()  wc : the worker context, a persistent object local to each\n   worker, where user can store and access worker local dependencies.    combiner : a function of the form  function(acc1, acc2[, obj]) ,\n  which returns the merged value of accumulators and with:  acc1 : the value of an accumulator, computed locally on a worker  acc2 : the value of an other accumulator, issued by another worker  obj : the same parameter  obj  passed to  aggregate()    init : the initial value of the accumulators that are used by\n   reducer()  and  combiner() . It should be the identity element\n  of the operation (a neutral zero value, i.e. applying it through the\n  function should not change result).  obj : user provided data. Data will be passed to carrying\n  serializable data from master to workers, obj is shared amongst\n  mapper executions over each element of the dataset.   Example:  sc . parallelize ([[ hello ,   1 ],   [ hello ,   1 ],   [ world ,   1 ]]) \n   . aggregateByKey (( a ,   b )   =   a   +   b ,   ( a ,   b )   =   a   +   b ,   0 ) \n   . collect () \n   . then ( console . log );  // [ [  hello , 2 ], [  world , 1 ] ]", 
            "title": "ds.aggregateByKey(reducer, combiner, init,[ obj])"
        }, 
        {
            "location": "/skale-API/#dscartesianother", 
            "text": "Returns a dataset wich contains all possible pairs  [a, b]  where  a \nis in the source dataset and  b  is in the  other  dataset.  Example:  var   ds1   =   sc . parallelize ([ 1 ,   2 ]);  var   ds2   =   sc . parallelize ([ a ,   b ,   c ]);  ds1 . cartesian ( ds2 ). collect (). then ( console . log );  // [ [ 1,  a  ], [ 1,  b  ], [ 1,  c  ],  //   [ 2,  a  ], [ 2,  b  ], [ 2,  c  ] ]", 
            "title": "ds.cartesian(other)"
        }, 
        {
            "location": "/skale-API/#dscogroupother", 
            "text": "When called on dataset of type  [k,v]  and  [k,w] , returns a dataset of type [k, [[v], [w]]] , where data of both datasets share the same key.  Example:  var   ds1   =   sc . parallelize ([[ 10 ,   1 ],   [ 20 ,   2 ]]);  var   ds2   =   sc . parallelize ([[ 10 ,   world ],   [ 30 ,   3 ]]);  ds1 . coGroup ( ds2 ). collect (). then ( console . log );  // [ [ 10, [ [ 1 ], [  world  ] ] ],  //   [ 20, [ [ 2 ], [] ] ],  //   [ 30, [ [], [ 3 ] ] ] ]", 
            "title": "ds.coGroup(other)"
        }, 
        {
            "location": "/skale-API/#dscollectdone", 
            "text": "This  action  returns the content of the dataset in form of an array.\nThe result is passed to the  done()  callback if provided, otherwise an ES6 promise  is returned.   done : a callback of the form  function(error, result)  which is\n  called at completion.   Example:  sc . parallelize ([ 1 ,   2 ,   3 ,   4 ]) \n   . collect ( function   ( err ,   res )   { \n      console . log ( res ); \n    });  // [ 1, 2, 3, 4 ]", 
            "title": "ds.collect([done])"
        }, 
        {
            "location": "/skale-API/#dscountdone", 
            "text": "This  action  computes the number of elements in the dataset. The\nresult is passed to the  done()  callback if provided, otherwise\nan  ES6 promise  is returned.   done : a callback of the form  function(error, result)  which is\n  called at completion.   Example:  sc . parallelize ([ 10 ,   20 ,   30 ,   40 ]). count (). then ( console . log );  // 4", 
            "title": "ds.count([done])"
        }, 
        {
            "location": "/skale-API/#dscountbykeydone", 
            "text": "When called on a dataset of type  [k,v] , this  action  computes\nthe number of occurrences of elements for each key in a dataset of\ntype  [k,v] . It produces an array of elements of type  [k,w]  where w  is the result count.  The result is passed to the  done() \ncallback if provided, otherwise an  ES6 promise  is returned.   done : a callback of the form  function(error, result)  which is\n  called at completion.   Example:  sc . parallelize ([[ 10 ,   1 ],   [ 20 ,   2 ],   [ 10 ,   4 ]]) \n   . countByKey (). then ( console . log );  // [ [ 10, 2 ], [ 20, 1 ] ]", 
            "title": "ds.countByKey([done])"
        }, 
        {
            "location": "/skale-API/#dscountbyvaluedone", 
            "text": "This  action  computes the number of occurences of each element in\ndataset and returns an array of elements of type  [v,n]  where  v \nis the element and  n  its number of occurrences.  The result is\npassed to the  done()  callback if provided, otherwise an  ES6\npromise  is returned.   done : a callback of the form  function(error, result)  which is\n  called at completion.   Example:  sc . parallelize ([   1 ,   2 ,   3 ,   1 ,   3 ,   2 ,   5   ]) \n   . countByValue (). then ( console . log );  // [ [ 1, 2 ], [ 2, 2 ], [ 3, 2 ], [ 5, 1 ] ]", 
            "title": "ds.countByValue([done])"
        }, 
        {
            "location": "/skale-API/#dsdistinct", 
            "text": "Returns a dataset where duplicates are removed.  Example:  sc . parallelize ([   1 ,   2 ,   3 ,   1 ,   4 ,   3 ,   5   ]) \n   . distinct () \n   . collect (). then ( console . log );  // [ 1, 2, 3, 4, 5 ]", 
            "title": "ds.distinct()"
        }, 
        {
            "location": "/skale-API/#dsfilterfilter-obj", 
            "text": "filter : a function of the form  callback(element[, obj[, wc]]) ,\n  returning a  Boolean  and where:  element : the next element of the dataset on which  filter()  operates  obj : the same parameter  obj  passed to  filter()  wc : the worker context, a persistent object local to each\n  worker, where user can store and access worker local dependencies.    obj : user provided data. Data will be passed to carrying\n  serializable data from master to workers, obj is shared amongst\n  mapper executions over each element of the dataset   Applies the provided filter function to each element of the source\ndataset and returns a new dataset containing the elements that passed the\ntest.  Example:  function   filter ( data ,   obj )   {   return   data   %   obj . modulo ;   }  sc . parallelize ([ 1 ,   2 ,   3 ,   4 ]) \n   . filter ( filter ,   { modulo :   2 }) \n   . collect (). then ( console . log );  // [ 1, 3 ]", 
            "title": "ds.filter(filter[, obj])"
        }, 
        {
            "location": "/skale-API/#dsfirstdone", 
            "text": "This  action  computes the first element in this dataset.\nThe result is passed to the  done()  callback if provided, otherwise an ES6 promise  is returned.   done : a callback of the form  function(error, result)  which is\n  called at completion.   sc . parallelize ([ 1 ,   2 ,   3 ]). first (). then ( console . log );  // 1", 
            "title": "ds.first([done])"
        }, 
        {
            "location": "/skale-API/#dsflatmapflatmapper-obj", 
            "text": "Applies the provided mapper function to each element of the source\ndataset and returns a new dataset.   flatMapper : a function of the form  callback(element[, obj[, wc]]) ,\n  returning an  Array  and where:  element : the next element of the dataset on which  flatMap()  operates  obj : the same parameter  obj  passed to  flatMap()  wc : the worker context, a persistent object local to each\n  worker, where user can store and access worker local dependencies.    obj : user provided data. Data will be passed to carrying\n  serializable data from master to workers, obj is shared amongst\n  mapper executions over each element of the dataset   Example:  sc . range ( 5 ). flatMap ( a   =   [ a ,   a ]). collect (). then ( console . log );  // [ 0, 0, 1, 1, 2, 2, 3, 3, 4, 4 ]", 
            "title": "ds.flatMap(flatMapper[, obj])"
        }, 
        {
            "location": "/skale-API/#dsflatmapvaluesflatmapper-obj", 
            "text": "Applies the provided flatMapper function to the value of each [key,\nvalue] element of the source dataset and return a new dataset containing\nelements defined as [key, mapper(value)], keeping the key unchanged\nfor each source element.   flatMapper : a function of the form  callback(element[, obj[, wc]]) ,\n  returning an  Array  and where:  element : the value v of the next [k,v] element of the dataset on\n  which  flatMapValues()  operates  obj : the same parameter  obj  passed to  flatMapValues()  wc : the worker context, a persistent object local to each\n  worker, where user can store and access worker local dependencies.    obj : user provided data. Data will be passed to carrying\n  serializable data from master to workers, obj is shared amongst\n  mapper executions over each element of the dataset   Example:  function   valueFlatMapper ( data ,   obj )   { \n     var   tmp   =   []; \n     for   ( var   i   =   0 ;   i     obj . N ;   i ++ )   tmp . push ( data   *   obj . fact ); \n     return   tmp ;  }  sc . parallelize ([[ hello ,   1 ],   [ world ,   2 ]]) \n   . flatMapValues ( valueFlatMapper ,   { N :   2 ,   fact :   2 }) \n   . collect (). then ( console . log );  // [ [  hello , 2 ], [  hello , 2 ], [  world , 4 ], [  world , 4 ] ]", 
            "title": "ds.flatMapValues(flatMapper[, obj])"
        }, 
        {
            "location": "/skale-API/#dsforeachcallback-obj-done", 
            "text": "This  action  applies a  callback  function on each element of the dataset.\nIf provided, the  done()  callback is invoked at completion, otherwise an ES6 promise  is returned.   callback : a function of the form  function(val[, obj[, wc]]) ,\n  which returns  null  and with:  val : the value of the next element of the dataset on which\n    forEach()  operates  obj : the same parameter  obj  passed to  forEach()  wc : the worker context, a persistent object local to each\n   worker, where user can store and access worker local dependencies.    obj : user provided data. Data will be passed to carrying\n  serializable data from master to workers, obj is shared amongst\n  mapper executions over each element of the dataset  done : a callback of the form  function(error, result)  which is\n  called at completion.   In the following example, the  console.log()  callback provided\nto  forEach()  is executed on workers and may be not visible:  sc . parallelize ([ 1 ,   2 ,   3 ,   4 ]) \n   . forEach ( console . log ). then ( console . log ( finished ));", 
            "title": "ds.forEach(callback[, obj][, done])"
        }, 
        {
            "location": "/skale-API/#dsgroupbykey", 
            "text": "When called on a dataset of type  [k,v] , returns a dataset of type  [k, [v]] \nwhere values with the same key are grouped.  Example:  sc . parallelize ([[ 10 ,   1 ],   [ 20 ,   2 ],   [ 10 ,   4 ]]) \n   . groupByKey (). collect (). then ( console . log );  // [ [ 10, [ 1, 4 ] ], [ 20, [ 2 ] ] ]", 
            "title": "ds.groupByKey()"
        }, 
        {
            "location": "/skale-API/#dsintersectionother", 
            "text": "Returns a dataset containing only elements found in source dataset and  other \ndataset.  Example:  var   ds1   =   sc . parallelize ([ 1 ,   2 ,   3 ,   4 ,   5 ]);  var   ds2   =   sc . parallelize ([ 3 ,   4 ,   5 ,   6 ,   7 ]);  ds1 . intersection ( ds2 ). collect (). then ( console . log );   // [ 3, 4, 5 ]", 
            "title": "ds.intersection(other)"
        }, 
        {
            "location": "/skale-API/#dsjoinother", 
            "text": "When called on source dataset of type  [k,v]  and  other  dataset of type [k,w] , returns a dataset of type  [k, [v, w]]  pairs with all pairs\nof elements for each key.  Example:  var   ds1   =   sc . parallelize ([[ 10 ,   1 ],   [ 20 ,   2 ]]);  var   ds2   =   sc . parallelize ([[ 10 ,   world ],   [ 30 ,   3 ]]);  ds1 . join ( ds2 ). collect (). then ( console . log );  // [ [ 10, [ 1,  world  ] ] ]", 
            "title": "ds.join(other)"
        }, 
        {
            "location": "/skale-API/#dskeys", 
            "text": "When called on source dataset of type  [k,v] , returns a dataset with just\nthe elements  k .  Example:  sc . parallelize ([[ 10 ,   world ],   [ 30 ,   3 ]]) \n   . keys . collect (). then ( console . log );  // [ 10, 30 ]", 
            "title": "ds.keys()"
        }, 
        {
            "location": "/skale-API/#dsleftouterjoinother", 
            "text": "When called on source dataset of type  [k,v]  and  other  dataset of type [k,w] , returns a dataset of type  [k, [v, w]]  pairs where the key\nmust be present in the  other  dataset.  Example:  var   ds1   =   sc . parallelize ([[ 10 ,   1 ],   [ 20 ,   2 ]]);  var   ds2   =   sc . parallelize ([[ 10 ,   world ],   [ 30 ,   3 ]]);  ds1 . leftOuterJoin ( ds2 ). collect (). then ( console . log );  // [ [ 10, [ 1,  world  ] ], [ 20, [ 2, null ] ] ]", 
            "title": "ds.leftOuterJoin(other)"
        }, 
        {
            "location": "/skale-API/#dslookupk-done", 
            "text": "When called on source dataset of type  [k,v] , returns an array\nof values  v  for key  k .\nThe result is passed to the  done()  callback if provided, otherwise an ES6 promise  is returned.   done : a callback of the form  function(error, result)  which is\n  called at completion.   Example:  sc . parallelize ([[ 10 ,   world ],   [ 20 ,   2 ],   [ 10 ,   1 ],   [ 30 ,   3 ]]) \n   . lookup ( 10 ). then ( console . log );  // [ world, 1 ]", 
            "title": "ds.lookup(k[, done])"
        }, 
        {
            "location": "/skale-API/#dsmapmapper-obj", 
            "text": "Applies the provided mapper function to each element of the source\ndataset and returns a new dataset.   mapper : a function of the form  callback(element[, obj[, wc]]) ,\n  returning an element and where:  element : the next element of the dataset on which  map()  operates  obj : the same parameter  obj  passed to  map()  wc : the worker context, a persistent object local to each\n  worker, where user can store and access worker local dependencies.    obj : user provided data. Data will be passed to carrying\n  serializable data from master to workers, obj is shared amongst\n  mapper executions over each element of the dataset   Example:  sc . parallelize ([ 1 ,   2 ,   3 ,   4 ]) \n   . map (( data ,   obj )   =   data   *   obj . scaling ,   { scaling :   1.2 }) \n   . collect (). then ( console . log );  // [ 1.2, 2.4, 3.6, 4.8 ]", 
            "title": "ds.map(mapper[, obj])"
        }, 
        {
            "location": "/skale-API/#dsmapvaluesmapper-obj", 
            "text": "mapper : a function of the form  callback(element[, obj[, wc]]) ,\n  returning an element and where:  element : the value v of the next [k,v] element of the dataset on\n  which  mapValues()  operates  obj : the same parameter  obj  passed to  mapValues()  wc : the worker context, a persistent object local to each\n  worker, where user can store and access worker local dependencies    obj : user provided data. Data will be passed to carrying\n  serializable data from master to workers, obj is shared amongst\n  mapper executions over each element of the dataset   Applies the provided mapper function to the value of each  [k,v] \nelement of the source dataset and return a new dataset containing elements\ndefined as  [k, mapper(v)] , keeping the key unchanged for each\nsource element.  Example:  sc . parallelize ([[ hello ,   1 ],   [ world ,   2 ]]) \n   . mapValues (( a ,   obj )   =   a * obj . fact ,   { fact :   2 }) \n   . collect (). then ( console . log );  // [ [ hello , 2], [ world , 4] ]", 
            "title": "ds.mapValues(mapper[, obj])"
        }, 
        {
            "location": "/skale-API/#dspartitionbypartitioner", 
            "text": "Returns a dataset partitioned using the specified partitioner. The\npurpose of this transformation is not to change the dataset content,\nbut to increase processing speed by ensuring that the elements\naccessed by further transfomations reside in the same partition.  Example:  var   skale   =   require ( skale );  var   sc   =   skale . context ();  sc . parallelize ([[ hello ,   1 ],   [ world ,   1 ],   [ hello ,   2 ],   [ world ,   2 ],   [ cedric ,   3 ]]) \n   . partitionBy ( new   skale . HashPartitioner ( 3 )) \n   . collect . then ( console . log )  // [ [ world , 1], [ world , 2], [ hello , 1], [ hello , 2], [ cedric , 3] ]", 
            "title": "ds.partitionBy(partitioner)"
        }, 
        {
            "location": "/skale-API/#dspersist", 
            "text": "Returns the dataset, and persists the dataset content on disk (and\nin memory if available) in order to directly reuse content in further\ntasks.  Example:  var   dataset   =   sc . range ( 100 ). map ( a   =   a   *   a );  // First action: compute dataset  dataset . collect (). then ( console . log )  // Second action: reuse dataset, avoid map transform  dataset . collect (). then ( console . log )", 
            "title": "ds.persist()"
        }, 
        {
            "location": "/skale-API/#dsreducereducer-init-obj-done", 
            "text": "This  action  returns the aggregated value of the elements\nof the dataset using a  reducer()  function.\nThe result is passed to the  done()  callback if provided, otherwise an ES6 promise  is returned.   reducer : a function of the form  function(acc, val[, obj[, wc]]) ,\n  which returns the next value of the accumulator (which must be\n  of the same type as  acc  and  val ) and with:  acc : the value of the accumulator, initially set to  init  val : the value of the next element of the dataset on which\n    reduce()  operates  obj : the same parameter  obj  passed to  reduce()  wc : the worker context, a persistent object local to each\n   worker, where user can store and access worker local dependencies.    init : the initial value of the accumulators that are used by\n   reducer() . It should be the identity element of the operation\n  (i.e. applying it through the function should not change result).  obj : user provided data. Data will be passed to carrying\n  serializable data from master to workers, obj is shared amongst\n  mapper executions over each element of the dataset  done : a callback of the form  function(error, result)  which is\n  called at completion.   Example:  sc . parallelize ([ 1 ,   2 ,   4 ,   8 ]) \n   . reduce (( a ,   b )   =   a   +   b ,   0 ) \n   . then ( console . log );  // 15", 
            "title": "ds.reduce(reducer, init[, obj][, done])"
        }, 
        {
            "location": "/skale-API/#dsreducebykeyreducer-init-obj", 
            "text": "reducer : a function of the form  callback(acc,val[, obj[, wc]]) ,\n  returning the next value of the accumulator (which must be of the\n  same type as  acc  and  val ) and where:  acc : the value of the accumulator, initially set to  init  val : the value  v  of the next  [k,v]  element of the dataset on\n  which  reduceByKey()  operates  obj : the same parameter  obj  passed to  reduceByKey()  wc : the worker context, a persistent object local to each\n  worker, where user can store and access worker local dependencies.    init : the initial value of accumulator for each key. Will be\n  passed to  reducer .  obj : user provided data. Data will be passed to carrying\n  serializable data from master to workers, obj is shared amongst\n  mapper executions over each element of the dataset   When called on a dataset of type  [k,v] , returns a dataset of type  [k,v] \nwhere the values of each key are aggregated using the  reducer \nfunction and the  init  initial value.  Example:  sc . parallelize ([[ 10 ,   1 ],   [ 10 ,   2 ],   [ 10 ,   4 ]]) \n   . reduceByKey (( a , b )   =   a + b ,   0 ) \n   . collect (). then ( console . log );  // [ [10, 7] ]", 
            "title": "ds.reduceByKey(reducer, init[, obj])"
        }, 
        {
            "location": "/skale-API/#dsrightouterjoinother", 
            "text": "When called on source dataset of type  [k,v]  and  other  dataset of type [k,w] , returns a dataset of type  [k, [v, w]]  pairs where the key\nmust be present in the  source  dataset.  Example:  var   ds1   =   sc . parallelize ([[ 10 ,   1 ],   [ 20 ,   2 ]]);  var   ds2   =   sc . parallelize ([[ 10 ,   world ],   [ 30 ,   3 ]]);  ds1 . rightOuterJoin ( ds2 ). collect (). then ( console . log );  // [ [ 10, [ 1,  world  ] ], [ 30, [ null, 2 ] ] ]", 
            "title": "ds.rightOuterJoin(other)"
        }, 
        {
            "location": "/skale-API/#dssamplewithreplacement-frac", 
            "text": "withReplacement :  Boolean  value,  true  if data must be sampled\n  with replacement  frac :  Number  value of the fraction of source dataset to return   Returns a dataset by sampling a fraction  frac  of source dataset, with or\nwithout replacement.  Example:  sc . parallelize ([ 1 ,   2 ,   3 ,   4 ,   5 ,   6 ,   7 ,   8 ]) \n   . sample ( true ,   0.5 ,   0 ) \n   . collect (). then ( console . log );  // [ 1, 1, 3, 4, 4, 5, 7 ]", 
            "title": "ds.sample(withReplacement, frac)"
        }, 
        {
            "location": "/skale-API/#dssaveurl-options-done", 
            "text": "This  action  saves the content of the dataset to the destination URL. The\ndestination is a flat directory which will contain as many files as partitions\nin the dataset. Files are named from partition numbers, starting at 0.\nThe file format is a stream of JSON strings (one per dataset\nelement) separated by newlines.   url : a  String  of the general form  protocol://host/path  or  /path . See\n  below for supported protocols  options : an  Object  with the following fields:  gzip :  Boolean  (default false) to enable gzip compression. If compression\n  is enabled, files are suffixed with  .gz    done : an optional callback function of the form  function(error, result) \n  called at completion. If not provided, an  ES6 promise  is returned.", 
            "title": "ds.save(url[, options][, done])"
        }, 
        {
            "location": "/skale-API/#file-protocol", 
            "text": "The URL form is  file://path  or simply  path  where  path  is an absolute\npathname in the master host local file system.  Example:  sc . range ( 300 ). save ( /tmp/results/ ). then ( sc . end ());  // will produce /tmp/results/0, /tmp/results/1", 
            "title": "File protocol"
        }, 
        {
            "location": "/skale-API/#aws-s3-protocol", 
            "text": "The URL form is  s3://bucket/key . AWS credentials must be provided by environment\nvariables i.e  AWS_SECRET_ACCESS_KEY ,  AWS_ACCESS_KEY_ID .  Example:  sc . range ( 300 ). save ( s3://myproject/mydataset ,   { gzip :   true }). then ( sc . end ());  // will produce https://myproject.s3.amazonaws.com/mydataset/0.gz", 
            "title": "AWS S3 protocol"
        }, 
        {
            "location": "/skale-API/#azure-blob-storage-protocol", 
            "text": "The URL form is  wasb://container@user.blob.core.windows.net/blob_name . Azure\ncredentials must be provided by environment variables i.e AZURE_STORAGE_ACCOUNT  and  AZURE_STORAGE_ACCESS_KEY .", 
            "title": "Azure blob storage protocol"
        }, 
        {
            "location": "/skale-API/#dssortbykeyfunc-ascending", 
            "text": "Returns a dataset sorted by the given  keyfunc .   keyfunc : a function of the form  function(element)  which returns\n  a value used for comparison in the sort function and where  element \n  is the next element of the dataset on which  sortBy()  operates  ascending : a boolean to set the sort direction. Default: true   Example:  sc . parallelize ([ 4 ,   6 ,   10 ,   5 ,   1 ,   2 ,   9 ,   7 ,   3 ,   0 ]) \n   . sortBy ( a   =   a ) \n   . collect (). then ( console . log )  // [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]", 
            "title": "ds.sortBy(keyfunc[, ascending])"
        }, 
        {
            "location": "/skale-API/#dssortbykeyascending", 
            "text": "When called on a dataset of type  [k,v] , returns a dataset of type  [k,v] \nsorted on  k . The optional parameter  ascending  is a boolean which sets\nthe sort direction, true by default.  Example:  sc . parallelize ([[ world ,   2 ],   [ cedric ,   3 ],   [ hello ,   1 ]]) \n   . sortByKey () \n   . collect (). then ( console . log )  // [[ cedric , 3], [ hello , 1], [ world , 2]]", 
            "title": "ds.sortByKey(ascending)"
        }, 
        {
            "location": "/skale-API/#dsstreamopt", 
            "text": "This  action  returns a  readable stream  of dataset content. The order\nof data and partitions is maintained.   opt : an object with the following fields:  end :  Boolean , when true, call  sc.end()  on stream  end  event. Default value: false.  gzip :  Boolean , when true, enable gzip compression. Default value: false.   Example:  var   s   =   sc . range ( 4 ). stream ();  s . pipe ( process . stdout );  // 0  // 1  // 2  // 3", 
            "title": "ds.stream([opt])"
        }, 
        {
            "location": "/skale-API/#dssubtractother", 
            "text": "Returns a dataset containing only elements of source dataset which\nare not in  other  dataset.  Example:  var   ds1   =   sc . parallelize ([ 1 ,   2 ,   3 ,   4 ,   5 ]);  var   ds2   =   sc . parallelize ([ 3 ,   4 ,   5 ,   6 ,   7 ]);  ds1 . subtract ( ds2 ). collect (). then ( console . log );  // [ 1, 2 ]", 
            "title": "ds.subtract(other)"
        }, 
        {
            "location": "/skale-API/#dstakenum-done", 
            "text": "This  action  returns an array of the  num  first elements of the\nsource dataset.  The result is passed to the  done()  callback if\nprovided, otherwise an  ES6 promise  is returned.   num : positive integer  Number  of elements  done : a callback of the form  function(error, result)  which is\n  called at completion.   Example:  sc . range ( 5 ). take ( 2 ). then ( console . log );  // [1, 2]", 
            "title": "ds.take(num[, done])"
        }, 
        {
            "location": "/skale-API/#dstakesamplewithreplacement-num-done", 
            "text": "This  action  returns an array with a random sample of  num  elements\nof the dataset, with or without replacement. The result is passed to\nthe  done()  callback if provided, otherwise an  ES6 promise  is returned.   withReplacement :  Boolean  value,  true  if data must be sampled\n  with replacement  num : positive integer  Number  of elements  done : a callback of the form  function(error, result)  which is\n  called at completion.   Example:  sc . range ( 100 ). takeSample ( 4 ). then ( console . log );  // [ 18, 75, 4, 57 ]", 
            "title": "ds.takeSample(withReplacement, num[, done])"
        }, 
        {
            "location": "/skale-API/#dstopnum-done", 
            "text": "This  action  returns an array of the  num  top elements of the\nsource dataset.  The result is passed to the  done()  callback if\nprovided, otherwise an  ES6 promise  is returned.   num : positive integer  Number  of elements  done : a callback of the form  function(error, result)  which is\n  called at completion.   Example:  sc . range ( 5 ). top ( 2 ). then ( console . log );  // [3, 4]", 
            "title": "ds.top(num[, done])"
        }, 
        {
            "location": "/skale-API/#dsunionother", 
            "text": "Returns a dataset that contains the union of the elements in the source\ndataset and the  other  dataset.  Example:  var   ds1   =   sc . parallelize ([ 1 ,   2 ,   3 ,   4 ,   5 ]);  var   ds2   =   sc . parallelize ([ 3 ,   4 ,   5 ,   6 ,   7 ]);  ds1 . union ( ds2 ). collect (). then ( console . log );  // [ 1, 2, 3, 4, 5, 3, 4, 5, 6, 7 ]", 
            "title": "ds.union(other)"
        }, 
        {
            "location": "/skale-API/#dsvalues", 
            "text": "When called on source dataset of type  [k,v] , returns a dataset with just\nthe elements  v .  Example:  sc . parallelize ([[ 10 ,   world ],   [ 30 ,   3 ]]) \n   . keys . collect (). then ( console . log );  // [  world , 3 ]", 
            "title": "ds.values()"
        }, 
        {
            "location": "/skale-API/#partitioners", 
            "text": "A partitioner is an object passed to ds.partitionBy(partitioner)  which\nplaces data in partitions according to a strategy, for example hash\npartitioning, where data having the same key are placed in the same\npartition, or range partitioning, where data in the same range are\nin the same partition. This is useful to accelerate processing, as\nit limits data transfers between workers during jobs.  A partition object must provide the following properties:   numPartitions : a  Number  of partitions for the dataset  getPartitionIndex : a  Function  of type  function(element) \n  which returns the partition index (comprised between 0 and\n   numPartitions ) for the  element  of the dataset on which\n   partitionBy()  operates.", 
            "title": "Partitioners"
        }, 
        {
            "location": "/skale-API/#hashpartitionernumpartitions", 
            "text": "Returns a partitioner object which implements hash based partitioning\nusing a hash checksum of each element as a string.   numPartitions :  Number  of partitions for this dataset   Example:  var   hp   =   new   skale . HashPartitioner ( 3 )  var   dataset   =   sc . range ( 10 ). partitionBy ( hp )", 
            "title": "HashPartitioner(numPartitions)"
        }, 
        {
            "location": "/skale-API/#rangepartitionernumpartitions-keyfunc-dataset", 
            "text": "Returns a partitioner object which first defines ranges by sampling\nthe dataset and then places elements by comparing them with ranges.   numPartitions :  Number  of partitions for this dataset  keyfunc : a function of the form  function(element)  which returns\n  a value used for comparison in the sort function and where  element \n  is the next element of the dataset on which  partitionBy()  operates  dataset : the dataset object on which  partitionBy()  operates   Example:  var   dataset   =   sc . range ( 100 )  var   rp   =   new   skale . RangePartitioner ( 3 ,   a   =   a ,   dataset )  var   dataset   =   sc . range ( 10 ). partitionBy ( rp )", 
            "title": "RangePartitioner(numPartitions, keyfunc, dataset)"
        }, 
        {
            "location": "/skale-API/#environment-variables", 
            "text": "SKALE_HOST : The hostname of the skale-server process in distributed mode. If unset, the master runs in standalone mode.  SKALE_PORT : The port of the skale-server process in distributed mode. Default value: \"12346\"  SKALE_KEY : An authentication token which may be required by the skale-server process  SKALE_DEBUG : set the debug trace level to the following values:  0 : or unset: no traces  1 : debug traces from master side  2 : above traces plus worker traces  3 : above traces plus network protocol traces (if running in distributed mode)", 
            "title": "Environment variables"
        }, 
        {
            "location": "/machine-learning/", 
            "text": "Machine Learning module\n\n\nThe Machine Learning (ML) module provides scalable functions for\nsupervised (classification, regression) and unsupervised (clustering)\nstatistical learning on top of skale datasets and distributed\nmap-reduce engine.\n\n\nThe module can be loaded using:\n\n\nvar\n \nml\n \n=\n \nrequire\n(\nskale/ml\n)\n\n\n\n\n\n\nclassificationMetrics(measures[, options][, done])\n\n\nThis \naction\n computes various metrics to measure classification performance.\n\n\n\n\nmeasures\n: a dataset where entries are in the form\n  \n[prediction, label]\n with \nprediction\n and \nlabel\n being numbers where\n  only value sign is used: true if positive or false if negative.\n\n\noptions\n: an optional \nObject\n with the following fields:\n\n\nsteps\n: integer \nNumber\n defining the number of points in the Receiver\n  Operation Charateristics (ROC) curve. Defaults: 10\n\n\n\n\n\n\ndone\n:  an optional callback of the form \nfunction(error, result)\n\n  which is called at completion. \nresult\n is an object with the following fields:\n\n\nrates\n: an array of \nsteps\n of confusion matrix raw values\n\n\nauROC\n: area under ROC curve, using the trapezoidal rule\n\n\nauPR\n: area under Precision Recall curve, using the trapezoidal rule\n\n\n\n\n\n\n\n\nExample:\n\n\nvar\n \nmodel\n \n=\n \nnew\n \nml\n.\nSGDLinearModel\n();\n\n\nawait\n \nmodel\n.\nfit\n(\ntrainingSet\n);\n\n\nvar\n \npredictionAndLabels\n \n=\n \ntestSet\n.\nmap\n((\np\n,\n \nmodel\n)\n \n=\n \n[\nmodel\n.\npredict\n(\np\n[\n1\n]),\n \np\n[\n0\n]],\n \nmodel\n);\n\n\nvar\n \nmetrics\n \n=\n \nawait\n \nml\n.\nclassificationMetrics\n(\npredictionAndLabels\n)\n\n\nconsole\n.\nlog\n(\nROC AUC:\n,\n \nmetrics\n.\nauROC\n);\n\n\n// 0.869\n\n\n\n\n\n\nKMeans(nbClusters[, options])\n\n\nCreates a clusterization model fitted via \nK-Means\n algorithm.\n\n\n\n\nnbClusters\n: \nNumber\n, specifying the number of clusters in the model\n\n\nOptions\n: an optional \nObject\n with the following fields:\n\n\nmaxMse\n: \nNumber\n defining the maximum mean square error between cluster\n  centers since previous iteration. Used to stop iterations. Default to 1e-7.\n\n\nmaxIterations\n: \nNumber\n defining the maximum number of iterations. Default: 100.\n\n\nmeans\n: an initial array of vectors (arrays) of numbers, default undefined.\n\n\n\n\n\n\n\n\nExample:\n\n\nconst\n \ndataset\n \n=\n \nsc\n.\nparallelize\n([\n\n  \n[\n1\n,\n \n2\n],\n \n[\n1\n,\n \n4\n],\n \n[\n1\n,\n \n0\n],\n\n  \n[\n4\n,\n \n2\n],\n \n[\n4\n,\n \n4\n],\n \n[\n4\n,\n \n0\n]\n\n\n]);\n\n\nconst\n \nkmeans\n \n=\n \nml\n.\nKMeans\n(\n2\n);\n\n\nawait\n \nkmeans\n.\nfit\n(\ndataset\n);\n\n\nkmeans\n.\nmeans\n\n\n// [ [ 2.5, 1 ], [ 2.5, 4 ] ]\n\n\nkmeans\n.\npredict\n([\n0\n,\n \n0\n])\n\n\n// 0\n\n\nkmeans\n.\npredict\n([\n4\n,\n \n4\n]\n\n\n// 1\n\n\n\n\n\n\nkmeans.fit(trainingSet[, done])\n\n\nThis \naction\n updates \nkmeans\n model by fitting it to the input\ndataset \ntrainingSet\n. The \ndone()\n callback is called at completion\nif provided, otherwise an \nES6 promise\n is returned.\n\n\n\n\ntrainingSet\n: a dataset where entries are in the following format:\n  \n[feature0, feature1, ...]\n with \nfeatureN\n being a float number.\n\n\ndone\n: an optional callback of the form \nfunction(error)\n\n  which is called at completion.\n\n\n\n\nkmeans.predict(sample)\n\n\nReturns the closest cluster index for the \nsample\n.\n\n\n\n\nsample\n: an \nArray\n with the format \n[feature0, feature 1, ...]\n\n  with \nfeatureN\n being a float number.\n\n\n\n\nSGDLinearModel([options])\n\n\nCreates a regularized linear model fitted via \nstochastic\ngradient descent\n learning. Such model can be used either for \nregression or classification, as training method is identical,\nonly prediction changes. SGD is sensitive to the scaling\nof the features. For best results, the data should have zero mean and\nunit variance, which can be achieved with \nml.StandardScaler\n.\n\n\nThe model it fits can be controlled with the \nloss\n option; by default,\nit fits a linear \nsupport vector machine\n (SVM). A regularization term\ncan be added to the loss, by default the squared euclidean norm L2.\n\n\n\n\noptions\n: an \nObject\n with the following fields:\n\n\nfitIntercept\n: \nBoolean\n indicating whether to include an intercept. Default: \ntrue\n\n\nloss\n: \nString\n specifying the \nloss function\n to be used. Possible values are:\n\n\nhinge\n: (default), gives a linear SVM\n\n\nlog\n: gives logistic loss, a probabilistic classifier\n\n\nsquare\n: gives square loss fit\n\n\n\n\n\n\npenalty\n: \nString\n  specifying the \nregularization\n term. Possible values are:\n\n\nl2\n: (default) squared euclidean norm L2, standard regularizer for linear SVM models\n\n\nl1\n: absolute norm L1, might bring sparsity to the model, not achievable with \nl2\n\n\nnone\n: zero penalty\n\n\n\n\n\n\nproba\n: \nBoolean\n (default \nfalse\n). If \ntrue\n predict returns a probability rather than a raw number. Only applicable when logisitic loss is selected.\n\n\nregParam\n: \nNumber\n  \n= 0, defaults to 0.001, defines the trade-off between the\n    two goals of minimizing the loss (i.e. training error) and minimizing model complexity\n    (i.e. to avoid overfitting)\n\n\nstepSize\n: \nNumber\n \n= 0, defaults to 1, defines the initial step size of the gradient\n    descent\n\n\n\n\nExample:\n\n\nconst\n \ntrainingSet\n \n=\n \nsc\n.\nparallelize\n([\n\n \n[\n1\n,\n \n[\n0.5\n,\n \n-\n0.7\n]],\n\n \n[\n-\n1\n,\n \n[\n-\n0.5\n,\n \n0.7\n]]\n\n\n]);\n\n\nconst\n \nsgd\n \n=\n \nnew\n \nml\n.\nSGDLinearModel\n()\n\n\nawait\n \nsgd\n.\nfit\n(\ntrainingSet\n,\n \n2\n)\n\n\nsgd\n.\nweights\n\n\n// [ 0.8531998372026804, -1.1944797720837526 ]\n\n\nsgd\n.\npredict\n([\n2\n,\n \n-\n2\n])\n\n\n// 0.9836229103782058\n\n\n\n\n\n\nsgd.fit(trainingSet, iterations[, done])\n\n\nThis \naction\n updates \nsgdClassifier\n model by fitting it to the\ninput dataset \ntrainingSet\n. The \ndone()\n callback is called at\ncompletion if provided, otherwise an \nES6 promise\n is returned.\n\n\n\n\ntrainingSet\n: a dataset where entries are in the following format:\n  \n[label, [feature0, feature1, ...]]\n with \nlabel\n being either 1 or -1,\n  and \nfeatureN\n being a float number, preferentially with a zero mean and\n  unit variance (in range [-1, 1]). Sparse vectors with undefined features\n  are supported.\n\n\ndone\n: an optional callback of the form \nfunction(error)\n\n  which is called at completion.\n\n\n\n\nsgd.predict(sample)\n\n\nPredict a label for a given \nsample\n and returns a numerical value\nwhich can be converted to a label -1 if negative, or 1 if positive.\n\n\nIf selected loss is \nlog\n, the returned value can be interpreted as\na probability of the corresponding label.\n\n\nStandardScaler()\n\n\nCreates a standard scaler which standardizes features by removing\nthe mean and scaling to unit variance.\n\n\nCentering and scaling happen independently on each feature by\ncomputing the relevant statistics on the samples in the training\nset. \n\n\nStandardization of datasets is a common requirement for many machine\nlearning estimators. They might behave badly if the individual\nfeatures do not more or less look like standard normally distributed\ndata: Gaussian with zero mean and unit variance.\n\n\nExample:\n\n\nvar\n \ndata\n \n=\n \nsc\n.\nparallelize\n([[\n0\n,\n \n0\n],\n \n[\n0\n,\n \n0\n],\n \n[\n1\n,\n \n1\n],\n \n[\n1\n,\n \n1\n]]);\n\n\nvar\n \nscaler\n \n=\n \nnew\n \nml\n.\nStandardScaler\n();\n\n\nawait\n \nscaler\n.\nfit\n(\ndata\n);\n\n\nscaler\n\n\n//StandardScaler {\n\n\n//  transform: [Function],\n\n\n//  count: 4,\n\n\n//  mean: [ 0.5, 0.5 ],\n\n\n//  std: [ 0.5, 0.5 ] }\n\n\nvar\n \nscaled\n \n=\n \ndata\n.\nmap\n((\np\n,\n \nscaler\n)\n \n=\n \nscaler\n.\ntransform\n(\np\n),\n \nscaler\n)\n\n\nconsole\n.\nlog\n(\nawait\n \nscaled\n.\ncollect\n());\n\n\n// [ [ -1, -1 ], [ -1, -1 ], [ 1, 1 ], [ 1, 1 ] ]\n\n\nscaler\n.\ntransform\n([\n2\n,\n \n2\n])\n\n\n// [ 3, 3 ]\n\n\n\n\n\n\nscaler.fit(trainingSet[, done])\n\n\nThis \naction\n updates \nscaler\n by computing the mean and std of\n\ntrainingSet\n to be used for later scaling. The \ndone()\n callback\nis called at completion if provided, otherwise an \nES6 promise\n is\nreturned.\n\n\n\n\ntrainingSet\n: a dataset where entries are in the format\n  \n[feature0, feature1, ...]\n with \nfeatureN\n being a \nNumber\n\n\ndone\n: an optional callback of the form \nfunction (error)\n which\n  is called at completion.\n\n\n\n\nscaler.transform(sample)\n\n\nReturns the standardized scaled value of \nsample\n.\n\n\n\n\nsample\n: an \nArray\n with the format \n[feature0, feature 1, ...]\n\n  with \nfeatureN\n being a float number.", 
            "title": "Machine Learning module"
        }, 
        {
            "location": "/machine-learning/#machine-learning-module", 
            "text": "The Machine Learning (ML) module provides scalable functions for\nsupervised (classification, regression) and unsupervised (clustering)\nstatistical learning on top of skale datasets and distributed\nmap-reduce engine.  The module can be loaded using:  var   ml   =   require ( skale/ml )", 
            "title": "Machine Learning module"
        }, 
        {
            "location": "/machine-learning/#classificationmetricsmeasures-options-done", 
            "text": "This  action  computes various metrics to measure classification performance.   measures : a dataset where entries are in the form\n   [prediction, label]  with  prediction  and  label  being numbers where\n  only value sign is used: true if positive or false if negative.  options : an optional  Object  with the following fields:  steps : integer  Number  defining the number of points in the Receiver\n  Operation Charateristics (ROC) curve. Defaults: 10    done :  an optional callback of the form  function(error, result) \n  which is called at completion.  result  is an object with the following fields:  rates : an array of  steps  of confusion matrix raw values  auROC : area under ROC curve, using the trapezoidal rule  auPR : area under Precision Recall curve, using the trapezoidal rule     Example:  var   model   =   new   ml . SGDLinearModel ();  await   model . fit ( trainingSet );  var   predictionAndLabels   =   testSet . map (( p ,   model )   =   [ model . predict ( p [ 1 ]),   p [ 0 ]],   model );  var   metrics   =   await   ml . classificationMetrics ( predictionAndLabels )  console . log ( ROC AUC: ,   metrics . auROC );  // 0.869", 
            "title": "classificationMetrics(measures[, options][, done])"
        }, 
        {
            "location": "/machine-learning/#kmeansnbclusters-options", 
            "text": "Creates a clusterization model fitted via  K-Means  algorithm.   nbClusters :  Number , specifying the number of clusters in the model  Options : an optional  Object  with the following fields:  maxMse :  Number  defining the maximum mean square error between cluster\n  centers since previous iteration. Used to stop iterations. Default to 1e-7.  maxIterations :  Number  defining the maximum number of iterations. Default: 100.  means : an initial array of vectors (arrays) of numbers, default undefined.     Example:  const   dataset   =   sc . parallelize ([ \n   [ 1 ,   2 ],   [ 1 ,   4 ],   [ 1 ,   0 ], \n   [ 4 ,   2 ],   [ 4 ,   4 ],   [ 4 ,   0 ]  ]);  const   kmeans   =   ml . KMeans ( 2 );  await   kmeans . fit ( dataset );  kmeans . means  // [ [ 2.5, 1 ], [ 2.5, 4 ] ]  kmeans . predict ([ 0 ,   0 ])  // 0  kmeans . predict ([ 4 ,   4 ]  // 1", 
            "title": "KMeans(nbClusters[, options])"
        }, 
        {
            "location": "/machine-learning/#kmeansfittrainingset-done", 
            "text": "This  action  updates  kmeans  model by fitting it to the input\ndataset  trainingSet . The  done()  callback is called at completion\nif provided, otherwise an  ES6 promise  is returned.   trainingSet : a dataset where entries are in the following format:\n   [feature0, feature1, ...]  with  featureN  being a float number.  done : an optional callback of the form  function(error) \n  which is called at completion.", 
            "title": "kmeans.fit(trainingSet[, done])"
        }, 
        {
            "location": "/machine-learning/#kmeanspredictsample", 
            "text": "Returns the closest cluster index for the  sample .   sample : an  Array  with the format  [feature0, feature 1, ...] \n  with  featureN  being a float number.", 
            "title": "kmeans.predict(sample)"
        }, 
        {
            "location": "/machine-learning/#sgdlinearmodeloptions", 
            "text": "Creates a regularized linear model fitted via  stochastic\ngradient descent  learning. Such model can be used either for \nregression or classification, as training method is identical,\nonly prediction changes. SGD is sensitive to the scaling\nof the features. For best results, the data should have zero mean and\nunit variance, which can be achieved with  ml.StandardScaler .  The model it fits can be controlled with the  loss  option; by default,\nit fits a linear  support vector machine  (SVM). A regularization term\ncan be added to the loss, by default the squared euclidean norm L2.   options : an  Object  with the following fields:  fitIntercept :  Boolean  indicating whether to include an intercept. Default:  true  loss :  String  specifying the  loss function  to be used. Possible values are:  hinge : (default), gives a linear SVM  log : gives logistic loss, a probabilistic classifier  square : gives square loss fit    penalty :  String   specifying the  regularization  term. Possible values are:  l2 : (default) squared euclidean norm L2, standard regularizer for linear SVM models  l1 : absolute norm L1, might bring sparsity to the model, not achievable with  l2  none : zero penalty    proba :  Boolean  (default  false ). If  true  predict returns a probability rather than a raw number. Only applicable when logisitic loss is selected.  regParam :  Number    = 0, defaults to 0.001, defines the trade-off between the\n    two goals of minimizing the loss (i.e. training error) and minimizing model complexity\n    (i.e. to avoid overfitting)  stepSize :  Number   = 0, defaults to 1, defines the initial step size of the gradient\n    descent   Example:  const   trainingSet   =   sc . parallelize ([ \n  [ 1 ,   [ 0.5 ,   - 0.7 ]], \n  [ - 1 ,   [ - 0.5 ,   0.7 ]]  ]);  const   sgd   =   new   ml . SGDLinearModel ()  await   sgd . fit ( trainingSet ,   2 )  sgd . weights  // [ 0.8531998372026804, -1.1944797720837526 ]  sgd . predict ([ 2 ,   - 2 ])  // 0.9836229103782058", 
            "title": "SGDLinearModel([options])"
        }, 
        {
            "location": "/machine-learning/#sgdfittrainingset-iterations-done", 
            "text": "This  action  updates  sgdClassifier  model by fitting it to the\ninput dataset  trainingSet . The  done()  callback is called at\ncompletion if provided, otherwise an  ES6 promise  is returned.   trainingSet : a dataset where entries are in the following format:\n   [label, [feature0, feature1, ...]]  with  label  being either 1 or -1,\n  and  featureN  being a float number, preferentially with a zero mean and\n  unit variance (in range [-1, 1]). Sparse vectors with undefined features\n  are supported.  done : an optional callback of the form  function(error) \n  which is called at completion.", 
            "title": "sgd.fit(trainingSet, iterations[, done])"
        }, 
        {
            "location": "/machine-learning/#sgdpredictsample", 
            "text": "Predict a label for a given  sample  and returns a numerical value\nwhich can be converted to a label -1 if negative, or 1 if positive.  If selected loss is  log , the returned value can be interpreted as\na probability of the corresponding label.", 
            "title": "sgd.predict(sample)"
        }, 
        {
            "location": "/machine-learning/#standardscaler", 
            "text": "Creates a standard scaler which standardizes features by removing\nthe mean and scaling to unit variance.  Centering and scaling happen independently on each feature by\ncomputing the relevant statistics on the samples in the training\nset.   Standardization of datasets is a common requirement for many machine\nlearning estimators. They might behave badly if the individual\nfeatures do not more or less look like standard normally distributed\ndata: Gaussian with zero mean and unit variance.  Example:  var   data   =   sc . parallelize ([[ 0 ,   0 ],   [ 0 ,   0 ],   [ 1 ,   1 ],   [ 1 ,   1 ]]);  var   scaler   =   new   ml . StandardScaler ();  await   scaler . fit ( data );  scaler  //StandardScaler {  //  transform: [Function],  //  count: 4,  //  mean: [ 0.5, 0.5 ],  //  std: [ 0.5, 0.5 ] }  var   scaled   =   data . map (( p ,   scaler )   =   scaler . transform ( p ),   scaler )  console . log ( await   scaled . collect ());  // [ [ -1, -1 ], [ -1, -1 ], [ 1, 1 ], [ 1, 1 ] ]  scaler . transform ([ 2 ,   2 ])  // [ 3, 3 ]", 
            "title": "StandardScaler()"
        }, 
        {
            "location": "/machine-learning/#scalerfittrainingset-done", 
            "text": "This  action  updates  scaler  by computing the mean and std of trainingSet  to be used for later scaling. The  done()  callback\nis called at completion if provided, otherwise an  ES6 promise  is\nreturned.   trainingSet : a dataset where entries are in the format\n   [feature0, feature1, ...]  with  featureN  being a  Number  done : an optional callback of the form  function (error)  which\n  is called at completion.", 
            "title": "scaler.fit(trainingSet[, done])"
        }, 
        {
            "location": "/machine-learning/#scalertransformsample", 
            "text": "Returns the standardized scaled value of  sample .   sample : an  Array  with the format  [feature0, feature 1, ...] \n  with  featureN  being a float number.", 
            "title": "scaler.transform(sample)"
        }, 
        {
            "location": "/skale-hackers-guide/", 
            "text": "Skale Hacker's Guide\n\n\nIntroduction\n\n\nSkale is a fast and general purpose distributed data processing system. It provides a high-level API in Javascript and an optimized parallel execution engine.\n\n\nThis document gives an overview of its design and architecture, then some details on internals and code organisation, and finally presents how to extends various parts of the engine.\n\n\nIt is assumed that the reader is already familiar with using skale and with the \nreference guide\n, at least the \ncore concepts\n.\n\n\nArchitecture\n\n\nThis section describes the core architecture of skale. At high level, a skale application consists of a \nmaster\n program which launches various parallel tasks on \nworker\n nodes. The tasks read and write \ndatasets\n, i.e. arrays of data of abritrary size, split in \npartitions\n distributed on workers.\n\n\nMaster\n\n\nThe corresponding code is in \ncontext-local.js\n for the standalone mode, or \ncontext.js\n for the distributed mode, the only difference between the 2 being the way workers are created and connected to the master.\n\n\nIn a nutshell, the master performs the following:\n\n\n\n\nCreates a new skale \ncontext\n object to hold the state of cluster, datasets and tasks, then in this context:\n\n\nAllocates a new cluster, i.e. and array of \nworkers\n: connected slave processes on each worker host (1 process per CPU).\n\n\nCompiles then runs\n an execution graph derived from the user code, the \njob\n, consisting of a sequence of \nstages\n. This compilation is only triggered when an \naction\n is met, thus in \nlazy\n mode.\n\n\nFor each stage, \nruns the next task\n: serialize and send stage code and metadata about input dataset partitions to the next free worker, trigger execution, wait for result, repeat until all stage's tasks are completed.\n\n\n\n\nStage explanation here\n\n\nWorker\n\n\nThe corresponding code is in \nworker-local.js\n for the standalone mode and \nworker.js\n for the distributed mode. The common part is implemented in \ntask.js\n.\n\n\nA worker performs the following:\n\n\n\n\nConnects to the master and wait for the next task to execute, then for each task:\n\n\nSelect input partition(s), possible cases are:\n\n\nin memory local partition computed from a previous stage, already loaded\n\n\non-disk local partition computed from a previous stage, spilled to disk\n\n\nremote partition stored on a separate worker (post-shuffle)\n\n\nexternal data source, through a source connector\n\n\nIterate on partition(s), applying for each record a \npipeline\n of functions as defined by the user for the current stage (for example a filter function, followed by a mapper function, followed by a reducer function)\n\n\nThe last function of the pipeline is either an \naction\n (function returning data to master), or a pre-shuffle function (saving data on disk for remote access at start of next stage, i.e post-shuffle)\n\n\nAt end of task, a result is sent to master, usually metadata for output files, used for next stage or for final combiner action\n\n\n\n\nExplain communication model here, for data transfers and remote procedure calls\n\n\nDatasets\n\n\nThe main abstraction provided by skale is a \ndataset\n which is similar to a Javascript array, but partitioned accross the workers that can be operated in parallel.\n\n\nA dataset object is always created first on the master side, either by a \nsource\n function which returns a dataset from an external input or from scratch, or by a \ntransformation\n function, which takes a dataset in input and outputs a new dataset.\n\n\nThe same code, in \ndataset.js\n is loaded both in master and workers. A dataset object instantiated on master will be replicated on each worker through task \nserialization\n and \ndeserialization\n process.\n\n\nFrom an object oriented perspective, all \nsources\n and \ntransformations\n, as dataset contructors, are classes which derive and inherit from the \nDataset\n class, whereas \nactions\n, which operate on a dataset object, are simply methods of the \nDataset\n class.\n\n\nDataset objects have methods that can be run either on master side or on worker side (never on both), the following table provides a summary of these:\n\n\n\n\n\n\n\n\nDataset method\n\n\non master\n\n\non worker\n\n\ntype\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\ngetPartitions\n\n\n\u2713\n\n\n\n\nsource, post-shuffle transform\n\n\nAllocate output dataset partitions\n\n\n\n\n\n\ngetPreferedLocation\n\n\n\u2713\n\n\n\n\nsource\n\n\nreturn prefered worker for a given partition\n\n\n\n\n\n\niterate\n\n\n\n\n\u2713\n\n\nsource, post-shuffle transform\n\n\niterate stage pipeline on partition entries\n\n\n\n\n\n\ntransform\n\n\n\n\n\u2713\n\n\ntransform\n\n\nApply a custom operation on each input dataset entry, pre-shuffle\n\n\n\n\n\n\nspillToDisk\n\n\n\n\n\u2713\n\n\npre-shuffle transform\n\n\ndump partition data to disk during pre-shuffle, for next stage\n\n\n\n\n\n\n\n\nLocal standalone mode\n\n\nThe standalone mode is the default operating mode. All the processes, master and workers are running on the local host, using the \ncluster\n core NodeJS module. This mode is the simplest to operate: no dependency, and no server nor cluster setup and management required.  It is used as any standard NodeJS package: simply \nrequire('skale')\n, and that's it.\n\n\nThis mode is perfect for development, fast prototyping and tests on a single machine (i.e. a laptop). For unlimited scalibity, see distributed mode below.\n\n\nDistributed mode\n\n\nThe distributed mode allows to run the exact same code as in standalone over a network of multiple machines, thus achieving horizontal scalability.\n\n\nThe distributed mode involves two executables, which must be running prior to launch application programs:\n\n\n\n\na \nskale-server\n process, which is the access point where the \nmaster\n (user application) and \nworkers\n (running slaves) connect to, either by direct TCP connections, or by websockets.\n\n\nA \nskale-worker\n process, which is a worker controller, running on each machine of the computing cluster, and connecting to the \nskale-server\n. The worker controller will spawn worker processes on demand (typically one per CPU), each time a new job is submitted.\n\n\n\n\nTo run in distributed mode, the environment variable \nSKALE_HOST\n must be set to the \nskale-server\n hostname or IP address. If unset, the application will run in standalone mode. Multiple applications, each with its own set of workers and master processes can run simultaneously using the same server and worker controllers.\n\n\nAlthough not mandatory, running an external HTTP server on worker hosts, exposing skale temporary files, allows efficient peer-to-peer shuffle data transfer between workers. If not available, this traffic will go through the centralized \nskale-server\n. Any external HTTP server such as nginx, apache or busybox httpd, or even NodeJS (although not the most efficient for static file serving) will do.\n\n\nFor further details, see command line help for \nskale-worker\n and \nskale-server\n.\n\n\nAdding a new source\n\n\nA source returns a dataset from an external input or from scratch. For example, to be able to process data from kafka in a parallel manner, i.e. one topic partition per worker, one has to implement a kafka source in skale.\n\n\nAdding a new source is a matter of:\n\n\n\n\nDeriving a new class from the Dataset class, see as for example \nTextLocal\n, which implements a textFile source from local filesystem\n\n\nProviding a \ngetPartition\n method prototype, which allocates a fixed number of partitions, see \nTextLocal.getPartitions\n as an example of allocating one partition per file. This method will be run on the master, when triggered by the action, and prior to dispatch tasks to workers\n\n\nOptionally providing a \ngetPreferedLocation\n method prototype, to select a given worker according to your source semantics. If not provided, the master will dispatch the partition by default to the next free worker at execution time.\n\n\nProviding an \niterate\n method prototype, which operates this time on the worker to execute the stage pipeline on each partition entry. See for example \nTextLocal.iterate\n and \niterateStream\n which processes each line of a \nreadable stream\n. If the partition can be mapped to a readable stream, as it is the case for many NodeJS connectors, one can just reuse \niterateStream\n as is.\n\n\nExposing the source in the API, either by extending \ntextFile\n to process a new URL protocol, or adding a new source method in the context, see for example \nparallelize\n.\n\n\n\n\nAdding a new transform\n\n\nA new transform can be implemented either by deriving a new class from the Dataset class then providing dataset methods as in the previous table of dataset methods, or by composing existing tranform methods to issue a new one, see for example \ndistinct\n.\n\n\nHere give details on narrow vs wide transforms and impact on implementation\n\n\nAdding a new action", 
            "title": "Skale Hacker's Guide"
        }, 
        {
            "location": "/skale-hackers-guide/#skale-hackers-guide", 
            "text": "", 
            "title": "Skale Hacker's Guide"
        }, 
        {
            "location": "/skale-hackers-guide/#introduction", 
            "text": "Skale is a fast and general purpose distributed data processing system. It provides a high-level API in Javascript and an optimized parallel execution engine.  This document gives an overview of its design and architecture, then some details on internals and code organisation, and finally presents how to extends various parts of the engine.  It is assumed that the reader is already familiar with using skale and with the  reference guide , at least the  core concepts .", 
            "title": "Introduction"
        }, 
        {
            "location": "/skale-hackers-guide/#architecture", 
            "text": "This section describes the core architecture of skale. At high level, a skale application consists of a  master  program which launches various parallel tasks on  worker  nodes. The tasks read and write  datasets , i.e. arrays of data of abritrary size, split in  partitions  distributed on workers.", 
            "title": "Architecture"
        }, 
        {
            "location": "/skale-hackers-guide/#master", 
            "text": "The corresponding code is in  context-local.js  for the standalone mode, or  context.js  for the distributed mode, the only difference between the 2 being the way workers are created and connected to the master.  In a nutshell, the master performs the following:   Creates a new skale  context  object to hold the state of cluster, datasets and tasks, then in this context:  Allocates a new cluster, i.e. and array of  workers : connected slave processes on each worker host (1 process per CPU).  Compiles then runs  an execution graph derived from the user code, the  job , consisting of a sequence of  stages . This compilation is only triggered when an  action  is met, thus in  lazy  mode.  For each stage,  runs the next task : serialize and send stage code and metadata about input dataset partitions to the next free worker, trigger execution, wait for result, repeat until all stage's tasks are completed.   Stage explanation here", 
            "title": "Master"
        }, 
        {
            "location": "/skale-hackers-guide/#worker", 
            "text": "The corresponding code is in  worker-local.js  for the standalone mode and  worker.js  for the distributed mode. The common part is implemented in  task.js .  A worker performs the following:   Connects to the master and wait for the next task to execute, then for each task:  Select input partition(s), possible cases are:  in memory local partition computed from a previous stage, already loaded  on-disk local partition computed from a previous stage, spilled to disk  remote partition stored on a separate worker (post-shuffle)  external data source, through a source connector  Iterate on partition(s), applying for each record a  pipeline  of functions as defined by the user for the current stage (for example a filter function, followed by a mapper function, followed by a reducer function)  The last function of the pipeline is either an  action  (function returning data to master), or a pre-shuffle function (saving data on disk for remote access at start of next stage, i.e post-shuffle)  At end of task, a result is sent to master, usually metadata for output files, used for next stage or for final combiner action   Explain communication model here, for data transfers and remote procedure calls", 
            "title": "Worker"
        }, 
        {
            "location": "/skale-hackers-guide/#datasets", 
            "text": "The main abstraction provided by skale is a  dataset  which is similar to a Javascript array, but partitioned accross the workers that can be operated in parallel.  A dataset object is always created first on the master side, either by a  source  function which returns a dataset from an external input or from scratch, or by a  transformation  function, which takes a dataset in input and outputs a new dataset.  The same code, in  dataset.js  is loaded both in master and workers. A dataset object instantiated on master will be replicated on each worker through task  serialization  and  deserialization  process.  From an object oriented perspective, all  sources  and  transformations , as dataset contructors, are classes which derive and inherit from the  Dataset  class, whereas  actions , which operate on a dataset object, are simply methods of the  Dataset  class.  Dataset objects have methods that can be run either on master side or on worker side (never on both), the following table provides a summary of these:     Dataset method  on master  on worker  type  description      getPartitions  \u2713   source, post-shuffle transform  Allocate output dataset partitions    getPreferedLocation  \u2713   source  return prefered worker for a given partition    iterate   \u2713  source, post-shuffle transform  iterate stage pipeline on partition entries    transform   \u2713  transform  Apply a custom operation on each input dataset entry, pre-shuffle    spillToDisk   \u2713  pre-shuffle transform  dump partition data to disk during pre-shuffle, for next stage", 
            "title": "Datasets"
        }, 
        {
            "location": "/skale-hackers-guide/#local-standalone-mode", 
            "text": "The standalone mode is the default operating mode. All the processes, master and workers are running on the local host, using the  cluster  core NodeJS module. This mode is the simplest to operate: no dependency, and no server nor cluster setup and management required.  It is used as any standard NodeJS package: simply  require('skale') , and that's it.  This mode is perfect for development, fast prototyping and tests on a single machine (i.e. a laptop). For unlimited scalibity, see distributed mode below.", 
            "title": "Local standalone mode"
        }, 
        {
            "location": "/skale-hackers-guide/#distributed-mode", 
            "text": "The distributed mode allows to run the exact same code as in standalone over a network of multiple machines, thus achieving horizontal scalability.  The distributed mode involves two executables, which must be running prior to launch application programs:   a  skale-server  process, which is the access point where the  master  (user application) and  workers  (running slaves) connect to, either by direct TCP connections, or by websockets.  A  skale-worker  process, which is a worker controller, running on each machine of the computing cluster, and connecting to the  skale-server . The worker controller will spawn worker processes on demand (typically one per CPU), each time a new job is submitted.   To run in distributed mode, the environment variable  SKALE_HOST  must be set to the  skale-server  hostname or IP address. If unset, the application will run in standalone mode. Multiple applications, each with its own set of workers and master processes can run simultaneously using the same server and worker controllers.  Although not mandatory, running an external HTTP server on worker hosts, exposing skale temporary files, allows efficient peer-to-peer shuffle data transfer between workers. If not available, this traffic will go through the centralized  skale-server . Any external HTTP server such as nginx, apache or busybox httpd, or even NodeJS (although not the most efficient for static file serving) will do.  For further details, see command line help for  skale-worker  and  skale-server .", 
            "title": "Distributed mode"
        }, 
        {
            "location": "/skale-hackers-guide/#adding-a-new-source", 
            "text": "A source returns a dataset from an external input or from scratch. For example, to be able to process data from kafka in a parallel manner, i.e. one topic partition per worker, one has to implement a kafka source in skale.  Adding a new source is a matter of:   Deriving a new class from the Dataset class, see as for example  TextLocal , which implements a textFile source from local filesystem  Providing a  getPartition  method prototype, which allocates a fixed number of partitions, see  TextLocal.getPartitions  as an example of allocating one partition per file. This method will be run on the master, when triggered by the action, and prior to dispatch tasks to workers  Optionally providing a  getPreferedLocation  method prototype, to select a given worker according to your source semantics. If not provided, the master will dispatch the partition by default to the next free worker at execution time.  Providing an  iterate  method prototype, which operates this time on the worker to execute the stage pipeline on each partition entry. See for example  TextLocal.iterate  and  iterateStream  which processes each line of a  readable stream . If the partition can be mapped to a readable stream, as it is the case for many NodeJS connectors, one can just reuse  iterateStream  as is.  Exposing the source in the API, either by extending  textFile  to process a new URL protocol, or adding a new source method in the context, see for example  parallelize .", 
            "title": "Adding a new source"
        }, 
        {
            "location": "/skale-hackers-guide/#adding-a-new-transform", 
            "text": "A new transform can be implemented either by deriving a new class from the Dataset class then providing dataset methods as in the previous table of dataset methods, or by composing existing tranform methods to issue a new one, see for example  distinct .  Here give details on narrow vs wide transforms and impact on implementation", 
            "title": "Adding a new transform"
        }, 
        {
            "location": "/skale-hackers-guide/#adding-a-new-action", 
            "text": "", 
            "title": "Adding a new action"
        }
    ]
}